{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultimate TTT Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 10:04:01.946831: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-18 10:04:02.899324: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-07-18 10:04:04.145988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-18 10:04:04.268508: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-18 10:04:04.268707: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import scipy.signal\n",
    "\n",
    "# used for fps logging\n",
    "from datetime import datetime\n",
    "\n",
    "# sanity check\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train.ini']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"train.ini\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env Constants\n",
    "MAX_TIMESTEPS = config[\"ENV\"].getint(\"MAX_TIMESTEPS\")\n",
    "\n",
    "# board constants\n",
    "ROWS = config[\"ENV\"].getint(\"ROWS\")\n",
    "COLS = config[\"ENV\"].getint(\"COLS\")\n",
    "CELLS = config[\"ENV\"].getint(\"CELLS\")\n",
    "\n",
    "# socket constants\n",
    "S_PORT = config[\"ENV\"].getint(\"S_PORT\")\n",
    "A_PORT = config[\"ENV\"].getint(\"A_PORT\")\n",
    "R_PORT = config[\"ENV\"].getint(\"R_PORT\")\n",
    "MAX_MSG_SIZE = config[\"ENV\"].getint(\"MAX_MSG_SIZE\")\n",
    "\n",
    "# reward parameters\n",
    "WIN_REWARD = config[\"REWARD\"].getfloat(\"WIN_REWARD\")\n",
    "CELL_REWARD = config[\"REWARD\"].getfloat(\"CELL_REWARD\")\n",
    "VALID_REWARD = config[\"REWARD\"].getfloat(\"VALID_REWARD\")\n",
    "INVALID_PENALTY = config[\"REWARD\"].getfloat(\"INVALID_PENALTY\")\n",
    "LOSS_PENALTY = config[\"REWARD\"].getfloat(\"LOSS_PENALTY\")\n",
    "\n",
    "# misc\n",
    "SLEEP_TIME = config[\"ENV\"].getfloat(\"SLEEP_TIME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opponents\n",
    "\n",
    "# math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# opponents\n",
    "class Opponent:\n",
    "    def get_action(self, obs) -> int:\n",
    "        pass\n",
    "\n",
    "\n",
    "class RandomOpponent(Opponent):\n",
    "    \"\"\"\n",
    "    Makes completely random moves\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_actions) -> None:\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def get_action(self, obs) -> int:\n",
    "        return np.random.randint(0, self.n_actions)\n",
    "\n",
    "\n",
    "class ValidCurCellRandomOpponent(RandomOpponent):\n",
    "    \"\"\"\n",
    "    Makes valid moves when there is a cur cell\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_actions) -> None:\n",
    "        super().__init__(n_actions)\n",
    "\n",
    "    def get_cur_cell(self, obs: np.ndarray) -> tuple[bool, int]:\n",
    "        # determine if there is a current cell\n",
    "        cur_cell_exists = False\n",
    "        cur_cell = -1\n",
    "        for outer in range(obs.shape[0]):\n",
    "            if obs[outer, 0, 2] == 1:\n",
    "                cur_cell_exists = True\n",
    "                cur_cell = outer\n",
    "                break\n",
    "        return cur_cell_exists, cur_cell\n",
    "\n",
    "    def get_action(self, obs: np.ndarray) -> int:\n",
    "        cur_cell_exists, cur_cell = self.get_cur_cell(obs)\n",
    "        # make a valid move if there is a current cell\n",
    "        if cur_cell_exists:\n",
    "            indexes = []\n",
    "            for space in range(obs.shape[1]):\n",
    "                if obs[cur_cell, space, 0] == 0:  # if it's an empty space\n",
    "                    indexes.append(space)\n",
    "            return cur_cell * CELLS + np.random.choice(indexes)\n",
    "\n",
    "        return super().get_action(obs)\n",
    "\n",
    "\n",
    "class WinningRandomOpponent(ValidCurCellRandomOpponent):\n",
    "    def __init__(self, n_actions) -> None:\n",
    "        super().__init__(n_actions)\n",
    "\n",
    "    def get_winning_cells(self, cell: list[int], turn: int) -> list[bool]:\n",
    "        cell = np.array(cell).reshape((ROWS, COLS)).tolist()\n",
    "        ret = [[False for _ in range(COLS)] for x in range(ROWS)]\n",
    "\n",
    "        # check horizontals\n",
    "        for row in range(ROWS):\n",
    "            if cell[row].count(turn) == 2 and cell[row].count(0) == 1:\n",
    "                # this row is winnable\n",
    "                winning_cell = cell[row].index(0)\n",
    "                ret[row][winning_cell] = True\n",
    "\n",
    "        # check verticals\n",
    "        for c in range(COLS):\n",
    "            col = [cell[i][c] for i in range(ROWS)]\n",
    "            if col.count(turn) == 2 and col.count(0) == 1:\n",
    "                # this col is winnable\n",
    "                winning_row = col.index(0)\n",
    "                ret[winning_row][c] = True\n",
    "\n",
    "        # check diagonal left\n",
    "        left_diagonal = [cell[i][i] for i in range(ROWS)]\n",
    "        if left_diagonal.count(turn) == 2 and left_diagonal.count(0) == 1:\n",
    "            winning_space = left_diagonal.index(0)\n",
    "            ret[winning_space][winning_space] = True\n",
    "\n",
    "        # check diagonal right\n",
    "        right_diagonal = [cell[i][ROWS - 1 - i] for i in range(ROWS)]\n",
    "        if right_diagonal.count(turn) == 2 and right_diagonal.count(0) == 1:\n",
    "            winning_space = right_diagonal.index(0)\n",
    "            ret[winning_space][ROWS - 1 - winning_space] = True\n",
    "\n",
    "        return np.array(ret).flatten().tolist()\n",
    "\n",
    "    def get_winning_action(self, obs: np.ndarray) -> tuple[bool, int]:\n",
    "        # whose turn it is\n",
    "        turn = obs[0, 0, 3]\n",
    "\n",
    "        cur_cell_exists, cur_cell = self.get_cur_cell(obs)\n",
    "\n",
    "        # go through all the cells and see which are claimed\n",
    "        owners: list[int] = []\n",
    "        for outer in range(obs.shape[0]):\n",
    "            owners.append(obs[outer, 0, 1])\n",
    "\n",
    "        # see which cells are winning cells\n",
    "        winning_cells = self.get_winning_cells(owners, turn)\n",
    "        if True in winning_cells:\n",
    "            if cur_cell_exists:\n",
    "                # if the cur cell is a possible winning cell\n",
    "                if winning_cells[cur_cell]:\n",
    "                    # get the space owners\n",
    "                    space_owners = [obs[cur_cell, i, 0] for i in range(obs.shape[1])]\n",
    "                    winning_spaces = self.get_winning_cells(space_owners, turn)\n",
    "                    # it is possible\n",
    "                    if True in winning_spaces:\n",
    "                        return True, cur_cell * CELLS + winning_spaces.index(True)\n",
    "            else:\n",
    "                # we can go anywhere\n",
    "                for potential_winning_cell_idx in range(CELLS):\n",
    "                    if winning_cells[potential_winning_cell_idx]:\n",
    "                        # get the space owners\n",
    "                        space_owners = [\n",
    "                            obs[potential_winning_cell_idx, i, 0]\n",
    "                            for i in range(obs.shape[1])\n",
    "                        ]\n",
    "                        winning_spaces = self.get_winning_cells(space_owners, turn)\n",
    "                        # it is possible\n",
    "                        if True in winning_spaces:\n",
    "                            return (\n",
    "                                True,\n",
    "                                potential_winning_cell_idx * CELLS\n",
    "                                + winning_spaces.index(True),\n",
    "                            )\n",
    "\n",
    "        # winning isn't possible currently\n",
    "        return False, -1\n",
    "\n",
    "    def get_action(self, obs: np.ndarray) -> int:\n",
    "        winnable, action = self.get_winning_action(obs)\n",
    "        if winnable:\n",
    "            return action\n",
    "        return super().get_action(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to send data\n",
    "import os\n",
    "import time\n",
    "import socket\n",
    "\n",
    "# proto definitions\n",
    "import py.board_pb2 as pb\n",
    "\n",
    "# misc\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "# env\n",
    "class UltimateTicTacToeEnv:\n",
    "    obs_dim = (9, 9, 4)\n",
    "    n_actions = CELLS * CELLS\n",
    "\n",
    "    def __init__(\n",
    "        self, opponent: Opponent = RandomOpponent(n_actions), max_timesteps: int = 81\n",
    "    ) -> None:\n",
    "        self.s_conn, self.a_conn, self.r_conn = None, None, None\n",
    "        self.opponent = opponent\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.reset()\n",
    "\n",
    "    def _receive(self, conn: socket.socket, tp: type):\n",
    "        ret = tp()\n",
    "        b = conn.recv(MAX_MSG_SIZE)\n",
    "        ret.ParseFromString(b)\n",
    "        return ret\n",
    "\n",
    "    def _get_return(self) -> pb.ReturnMessage:\n",
    "        return self._receive(self.r_conn, pb.ReturnMessage)\n",
    "\n",
    "    def _get_state(self) -> pb.StateMessage:\n",
    "        return self._receive(self.s_conn, pb.StateMessage)\n",
    "\n",
    "    def _make_coord(self, idx) -> pb.Coord:\n",
    "        return pb.Coord(row=idx // COLS, col=idx % COLS)\n",
    "\n",
    "    def _send_action(self, move) -> None:\n",
    "        action = pb.ActionMessage(move=move)\n",
    "        self.a_conn.send(action.SerializeToString())\n",
    "\n",
    "    def _to_idx(self, coord: pb.Coord) -> int:\n",
    "        return coord.row * COLS + coord.col\n",
    "\n",
    "    def _to_multi_idx(self, move: pb.Move) -> int:\n",
    "        return self._to_idx(move.large) * CELLS + self._to_idx(move.small)\n",
    "\n",
    "    def _process_state(self, state: pb.StateMessage) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        The structure of the state:\n",
    "        (9, 9, 4)\n",
    "        Outer 9 represent board cells\n",
    "        inner 9 represent the cell spaces\n",
    "        each space has 3 objects:\n",
    "            space owner (0, 1, 2) representing if the space is claimed or not\n",
    "            cell owner (0, 1, 2) representing if the cell the space belongs to is claimed or not\n",
    "            curcellornot (0, 1); 1 if the space belongs to the current cell, 0 if not\n",
    "            turn (1, 2) 1 if the current turn is player1, 2 if the current turn is player2\n",
    "        \"\"\"\n",
    "        board_state = np.zeros(self.obs_dim)\n",
    "        for cell_idx in range(len(state.board.cells)):\n",
    "            for space_idx in range(len(state.board.cells[cell_idx].spaces)):\n",
    "                board_state[cell_idx, space_idx, 0] = (\n",
    "                    state.board.cells[cell_idx].spaces[space_idx].val\n",
    "                )\n",
    "                board_state[cell_idx, space_idx, 1] = state.cellowners[cell_idx]\n",
    "                board_state[cell_idx, space_idx, 2] = (\n",
    "                    1 if self._to_idx(state.board.curCell) == cell_idx else 0\n",
    "                )\n",
    "                board_state[cell_idx, space_idx, 3] = state.turn\n",
    "\n",
    "        return board_state\n",
    "\n",
    "    def _get_exploration_reward(self, action: int, msg: pb.ReturnMessage) -> float:\n",
    "        if msg.valid:\n",
    "            return VALID_REWARD\n",
    "        return INVALID_PENALTY\n",
    "\n",
    "    def _get_win_reward(self, msg: pb.ReturnMessage) -> float:\n",
    "        \"\"\"\n",
    "        Get's the reward for winning if the game was won\n",
    "        \"\"\"\n",
    "        # the turn sent in the return message should still be the caller's turn\n",
    "        if msg.state.winner == msg.state.turn:\n",
    "            if self.player_turn:\n",
    "                self.won = True\n",
    "                return WIN_REWARD\n",
    "            else:\n",
    "                self.lost = True\n",
    "                return LOSS_PENALTY\n",
    "        return 0\n",
    "\n",
    "    def _get_cell_reward(self, msg: pb.ReturnMessage) -> float:\n",
    "        \"\"\"\n",
    "        Get's the reward for claiming a cell if a cell was claimed\n",
    "        \"\"\"\n",
    "        if self.prev_cellowners == msg.state.cellowners:\n",
    "            return 0\n",
    "        elif list(msg.state.cellowners).count(\n",
    "            msg.state.turn\n",
    "        ) > self.prev_cellowners.count(msg.state.turn):\n",
    "            self.prev_cellowners = list(msg.state.cellowners)\n",
    "            return CELL_REWARD\n",
    "        return 0\n",
    "\n",
    "    def _get_reward(self, action: pb.Move, msg: pb.ReturnMessage) -> float:\n",
    "        return (\n",
    "            self._get_exploration_reward(action, msg)\n",
    "            + self._get_cell_reward(msg)\n",
    "            + self._get_win_reward(msg)\n",
    "        )\n",
    "\n",
    "    def _step(self, action: int) -> Tuple[np.ndarray, float, bool, bool]:\n",
    "        \"\"\"\n",
    "        Updates self.done\n",
    "        \"\"\"\n",
    "        # send action and get response\n",
    "        self._send_action(self.to_move(action))\n",
    "        ret_message = self._get_return()\n",
    "\n",
    "        # return information\n",
    "        reward = self._get_reward(action, ret_message)\n",
    "        self.done = ret_message.state.done\n",
    "        if not self.done:\n",
    "            return self.observe(), reward, self.done, ret_message.valid\n",
    "        else:\n",
    "            return (\n",
    "                self._process_state(ret_message.state),\n",
    "                reward,\n",
    "                self.done,\n",
    "                ret_message.valid,\n",
    "            )\n",
    "\n",
    "    def _take_opponent_turn(self) -> Tuple[np.ndarray, float, bool, bool]:\n",
    "        valid = False\n",
    "        while not valid:\n",
    "            obs, reward, done, valid = self._step(\n",
    "                self.opponent.get_action(self.cur_state)\n",
    "            )\n",
    "        return obs, reward, done, valid\n",
    "\n",
    "    def _reset_vars(self):\n",
    "        self.prev_cellowners = [pb.NONE] * 9\n",
    "        self.cur_state = None  # the current state; used for debugging\n",
    "        self.won = False  # whether or not the player won\n",
    "        self.done = False  # if the game is over\n",
    "        self.lost = False  # whether or not the player lost\n",
    "        self.player_turn = True  # whether or not it is the player's turn\n",
    "        self.cur_timestep = 0\n",
    "\n",
    "        self.s_conn = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.a_conn = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.r_conn = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "        self.s_conn.connect((\"\", 8000))\n",
    "        self.a_conn.connect((\"\", 8001))\n",
    "        self.r_conn.connect((\"\", 8002))\n",
    "\n",
    "    # public section\n",
    "    def observe(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Updates self.cur_state and self._turn\n",
    "        \"\"\"\n",
    "        state = self._get_state()\n",
    "        self._turn = state.turn\n",
    "        self.cur_state = self._process_state(state)\n",
    "        return self.cur_state\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool]:\n",
    "        \"\"\"\n",
    "        Updates current timestep\n",
    "\n",
    "        Returns:\n",
    "            - next state\n",
    "            - reward for the action\n",
    "            - done / not done\n",
    "            - valid / invalid\n",
    "        \"\"\"\n",
    "        self.player_turn = True\n",
    "        obs, reward, done, valid = self._step(action)\n",
    "        self.cur_timestep += 1\n",
    "        done = done or self.cur_timestep > self.max_timesteps\n",
    "\n",
    "        # take opponents turn\n",
    "        if valid and not done:\n",
    "            self.player_turn = False\n",
    "            obs, reward2, done, _ = self._take_opponent_turn()\n",
    "\n",
    "            # add the \"lost\" penalty\n",
    "            if self.done and self.lost:\n",
    "                return obs, reward + reward2, done, valid\n",
    "            # penalize losing cells\n",
    "            elif reward2 == CELL_REWARD + VALID_REWARD:\n",
    "                return obs, reward - reward2, done, valid\n",
    "            # nothing special, just return the reward\n",
    "            return obs, reward, done, valid\n",
    "        return obs, reward, done, valid\n",
    "\n",
    "    def turn(self):\n",
    "        return self._turn\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        while 1:\n",
    "            try:\n",
    "                self.cleanup()\n",
    "                # self.pid = os.spawnl(os.P_NOWAIT, \"uttt\", \"uttt\", \"aivai\")\n",
    "                ret = os.system(\"./uttt aivai &\")\n",
    "                time.sleep(SLEEP_TIME)\n",
    "                self._reset_vars()\n",
    "                break\n",
    "            except ConnectionRefusedError:\n",
    "                pass\n",
    "        return self.observe()\n",
    "\n",
    "    def cleanup(self):\n",
    "        os.system(\"killall -q uttt\")\n",
    "        if self.s_conn is not None:\n",
    "            self.s_conn.close()\n",
    "            self.r_conn.close()\n",
    "            self.a_conn.close()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.cleanup()\n",
    "\n",
    "    def to_move(self, idx: int) -> pb.Move:\n",
    "        outer_idx = idx // CELLS\n",
    "        inner_idx = idx % CELLS\n",
    "\n",
    "        return pb.Move(\n",
    "            large=self._make_coord(outer_idx), small=self._make_coord(inner_idx)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UltimateTicTacToeEnv(\n",
    "    max_timesteps=MAX_TIMESTEPS,\n",
    "    opponent=WinningRandomOpponent(UltimateTicTacToeEnv.n_actions),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer related hyperparameters\n",
    "gamma = config[\"BUFFER\"].getfloat(\"GAMMA\")\n",
    "lam = config[\"BUFFER\"].getfloat(\"LAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "def special_discounted_cumulative_sums(x, discount, invalid_locats):\n",
    "    \"\"\"\n",
    "    invalid_locats: np.ndarray[bool], True if invalid, False if valid\n",
    "    \"\"\"\n",
    "    # discounts, but doesn't stack INVALID_PENALTY\n",
    "    # where it was invalid, use 0, else use the positive x\n",
    "    zeros = np.where(invalid_locats, 0, x)\n",
    "    # filter this\n",
    "    filtered = scipy.signal.lfilter([1], [1, float(-discount)], zeros[::-1], axis=0)[\n",
    "        ::-1\n",
    "    ]\n",
    "    # replace so that you have the invalid penalty where it was invalid\n",
    "    # and the gamma'd reward on valid moves\n",
    "    return np.where(invalid_locats, x, filtered)\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    # Buffer for storing trajectories\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        # Buffer initialization\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, *observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, action, reward, value, logprobability):\n",
    "        # Append one step of agent-environment interaction\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.logprobability_buffer[self.pointer] = logprobability\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0):\n",
    "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        self.advantage_buffer[path_slice] = special_discounted_cumulative_sums(\n",
    "            deltas, self.gamma * self.lam, rewards[:-1] == INVALID_PENALTY\n",
    "        )\n",
    "        self.return_buffer[path_slice] = special_discounted_cumulative_sums(\n",
    "            rewards, self.gamma, rewards == INVALID_PENALTY\n",
    "        )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        # Get all data of the buffer and normalize the advantages\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.logprobability_buffer,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss related hyperparameters\n",
    "clip_ratio = config[\"LOSS\"].getfloat(\"CLIP_RATIO\")\n",
    "target_kl = config[\"LOSS\"].getfloat(\"TARGET_KL\")\n",
    "v_coef = config[\"LOSS\"].getfloat(\"VALUE_COEFFICIENT\")\n",
    "entropy_coef = config[\"LOSS\"].getfloat(\"ENTROPY_COEFFICIENT\")\n",
    "reg_coef = config[\"LOSS\"].getfloat(\"REGULARIZER_COEFFICIENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def logprobabilities(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    logprobabilities_all = tf.nn.log_softmax(logits)\n",
    "    logprobability = tf.reduce_sum(\n",
    "        tf.one_hot(a, UltimateTicTacToeEnv.n_actions) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    return logprobability\n",
    "\n",
    "\n",
    "# Sample action from actor\n",
    "@tf.function\n",
    "def sample_action_value(observation, model: keras.Model):\n",
    "    logits, value = model(observation)\n",
    "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "    return logits, action, value\n",
    "\n",
    "\n",
    "# Train the policy by maxizing the PPO-Clip objective\n",
    "@tf.function\n",
    "def train_mod(\n",
    "    observation_buffer,\n",
    "    action_buffer,\n",
    "    logprobability_buffer,\n",
    "    advantage_buffer,\n",
    "    return_buffer,\n",
    "    model: keras.Model,\n",
    "):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, values = model(observation_buffer)\n",
    "\n",
    "        new_probs = logprobabilities(logits, action_buffer)\n",
    "        # ratio = E(new_probs / old_probs)\n",
    "        # this subtraction method is a way to do this\n",
    "        ratio = tf.exp(new_probs - logprobability_buffer)\n",
    "\n",
    "        # L_clip = E_t * ( min( r_t*A_t, clip(r_t, 1-e, 1+e)*A_t ) )\n",
    "        clip_loss = -tf.reduce_mean(\n",
    "            tf.minimum(\n",
    "                ratio * advantage_buffer,\n",
    "                tf.clip_by_value(ratio, 1 - clip_ratio, 1 + clip_ratio)\n",
    "                * advantage_buffer,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        regularizer_loss = tf.reduce_sum(model.losses)\n",
    "\n",
    "        # critic loss = MSE\n",
    "        critic_loss = tf.keras.losses.mse(return_buffer, tf.squeeze(values))\n",
    "\n",
    "        # entropy loss to encourage exploration\n",
    "        entropy_loss = -tf.reduce_mean(-new_probs)\n",
    "\n",
    "        # full loss\n",
    "        loss = (\n",
    "            clip_loss\n",
    "            + critic_loss * v_coef\n",
    "            + entropy_loss * entropy_coef\n",
    "            + regularizer_loss * reg_coef\n",
    "        )\n",
    "\n",
    "    policy_grads = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(policy_grads, model.trainable_variables))\n",
    "\n",
    "    logits, _ = model(observation_buffer)\n",
    "    kl = tf.reduce_mean(logprobability_buffer - logprobabilities(logits, action_buffer))\n",
    "    kl = tf.reduce_sum(kl)\n",
    "\n",
    "    return kl, clip_loss, critic_loss, entropy_loss, regularizer_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ppo15\"\n",
    "load_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 10:04:06.926157: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-18 10:04:06.926398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-18 10:04:06.926534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-18 10:04:07.001311: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-18 10:04:07.001504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-18 10:04:07.001636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-18 10:04:07.001729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1172 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-07-18 10:04:08.128007: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 16777216 exceeds 10% of free system memory.\n",
      "2023-07-18 10:04:08.325834: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 16777216 exceeds 10% of free system memory.\n",
      "2023-07-18 10:04:09.594975: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 16777216 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "def create_shared_layers(input_layer):\n",
    "    # conv layers\n",
    "    x = layers.Conv1D(128, 9, padding=\"valid\", activation=\"selu\")(input_layer)\n",
    "    x = layers.Conv2D(\n",
    "        2048, (9, 1), padding=\"valid\", activation=\"selu\", activity_regularizer=\"l2\"\n",
    "    )(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "\n",
    "    # dense layers\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(2048, activation=\"selu\", activity_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(2048, activation=\"selu\", activity_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(1024, activation=\"selu\", activity_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(1024, activation=\"selu\", activity_regularizer=\"l2\")(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    # model inputs\n",
    "    inputs = tf.keras.Input(shape=UltimateTicTacToeEnv.obs_dim)\n",
    "    x = create_shared_layers(inputs)\n",
    "    logits = tf.keras.layers.Dense(UltimateTicTacToeEnv.n_actions)(x)\n",
    "    values = tf.keras.layers.Dense(1)(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=(logits, values))\n",
    "\n",
    "\n",
    "if load_model and os.path.exists(f\"models/{model_name}.keras\"):\n",
    "    print(\"loading model...\")\n",
    "    model = tf.keras.models.load_model(f\"models/{model_name}.keras\")\n",
    "else:\n",
    "    print(\"creating model...\")\n",
    "    model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 9, 9, 4)]            0         []                            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 9, 1, 128)            4736      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 1, 1, 2048)           2361344   ['conv1d[0][0]']              \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 1, 1, 2048)           0         ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 2048)                 0         ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 2048)                 4196352   ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 2048)                 0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 2048)                 8192      ['dropout_1[0][0]']           \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 2048)                 4196352   ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 2048)                 0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 2048)                 8192      ['dropout_2[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 1024)                 2098176   ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 1024)                 0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 1024)                 1049600   ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 81)                   83025     ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 1)                    1025      ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14006994 (53.43 MB)\n",
      "Trainable params: 13998802 (53.40 MB)\n",
      "Non-trainable params: 8192 (32.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate hyperparams\n",
    "learning_rate = config[\"OPTIMIZER\"].getfloat(\"LEARNING_RATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model and os.path.exists(f\"models/{model_name}.keras\"):\n",
    "    optim: tf.keras.optimizers.Adam = model.optimizer\n",
    "    optim.learning_rate = learning_rate\n",
    "else:\n",
    "    optim = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=3e-06>\n"
     ]
    }
   ],
   "source": [
    "print(optim.learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training time hyperparameters\n",
    "train_iterations = config[\"TRAIN\"].getint(\"TRAIN_ITERATIONS\")\n",
    "epochs = config[\"TRAIN\"].getint(\"EPOCHS\")\n",
    "minibatch_size = config[\"TRAIN\"].getint(\"MINIBATCH_SIZE\")\n",
    "steps_per_epoch = config[\"TRAIN\"].getint(\"STEPS_PER_EPOCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = config[\"TRAIN\"].getint(\"EPOCH\")\n",
    "summary_writer = tf.summary.create_file_writer(f\"./logs/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = Buffer(UltimateTicTacToeEnv.obs_dim, steps_per_epoch, gamma=gamma, lam=lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "def get_shuffled_values(*buffers):\n",
    "    # Get values from the buffer\n",
    "    # 0 - observation_buffer,\n",
    "    # 1 - action_buffer,\n",
    "    # 2 - advantage_buffer,\n",
    "    # 3 - return_buffer,\n",
    "    # 4 - logprobability_buffer\n",
    "    temp_values = list(zip(buffers[0], buffers[1], buffers[2], buffers[3], buffers[4]))\n",
    "    np.random.shuffle(temp_values)\n",
    "    return list(zip(*temp_values))\n",
    "\n",
    "\n",
    "# main training function\n",
    "def train_model(\n",
    "    model: keras.Model,\n",
    "    epoch_start: int,\n",
    "    use_kl: bool = True,\n",
    "    use_adaptive_kl: bool = True,\n",
    "    shuffle: bool = True,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Train the model for `epochs` epochs\n",
    "    Arguments:\n",
    "        * model: keras.Model - the model to train\n",
    "        * epoch_start: int - the epoch to start logging at\n",
    "        * use_kl: bool - whether or not to stop early if kl divergence > target_kl\n",
    "        * use_adaptive_kl: bool - whether or not to adapt kl (increase it if divergence > target_kl, decrease it otherwise)\n",
    "        * shuffle: bool - whether or not to shuffle the observations\n",
    "    \"\"\"\n",
    "    global target_kl\n",
    "    # Iterate over the number of epochs\n",
    "    for epoch in range(epoch_start, epoch_start + epochs):\n",
    "        # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "        sum_return = 0\n",
    "        num_episodes = 0\n",
    "        episode_return = 0\n",
    "        episode_length = 0\n",
    "        lengths = []\n",
    "\n",
    "        # logging variables\n",
    "        num_valid = 0\n",
    "        num_wins = 0\n",
    "        num_losses = 0\n",
    "\n",
    "        # Iterate over the steps of each epoch\n",
    "        observation = env.reset()\n",
    "        start_time = datetime.now()\n",
    "        for t in range(steps_per_epoch):\n",
    "            # Get the logits, action, and take one step in the environment\n",
    "            observation = observation.reshape(1, *env.obs_dim)\n",
    "            logits, action, value_t = sample_action_value(observation, model)\n",
    "            observation_new, reward, done, valid = env.step(action[0].numpy())\n",
    "            episode_return += reward\n",
    "            episode_length += 1\n",
    "\n",
    "            # logging variables\n",
    "            num_valid += 1 if valid else 0\n",
    "            num_wins += 1 if env.won else 0\n",
    "            num_losses += 1 if env.lost else 0\n",
    "\n",
    "            # Get the value and log-probability of the action\n",
    "            logprobability_t = logprobabilities(logits, action)\n",
    "\n",
    "            # Store obs, act, rew, v_t, logp_pi_t\n",
    "            buffer.store(observation, action, reward, value_t, logprobability_t)\n",
    "\n",
    "            # Update the observation\n",
    "            observation = observation_new\n",
    "\n",
    "            # Finish trajectory if reached to a terminal state\n",
    "            terminal = done\n",
    "            if terminal or t == steps_per_epoch - 1:\n",
    "                last_value = (\n",
    "                    0 if done else model(observation.reshape(1, *env.obs_dim))[1]\n",
    "                )\n",
    "                buffer.finish_trajectory(last_value)\n",
    "                sum_return += episode_return\n",
    "                lengths.append(episode_length)\n",
    "                num_episodes += 1\n",
    "                observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "            # log\n",
    "            print(\n",
    "                f\"Step {t+1} / {steps_per_epoch};\\t\"\n",
    "                + f\"% valid = {num_valid / (t+1):.4f};\\t\"\n",
    "                + f\"fps: {(t+1)/(datetime.now()-start_time).total_seconds():.2f};\\t\"\n",
    "                + f\"win rate: {(num_wins/num_episodes if num_episodes > 0 else 0):.2f}\\t\"\n",
    "                + f\"loss rate: {(num_losses/num_episodes if num_episodes > 0 else 0):.2f}\",\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "        print()\n",
    "\n",
    "        (\n",
    "            observation_buffer,\n",
    "            action_buffer,\n",
    "            advantage_buffer,\n",
    "            return_buffer,\n",
    "            logprobability_buffer,\n",
    "        ) = buffer.get()\n",
    "\n",
    "        # Update the policy and implement early stopping using KL divergence\n",
    "        kl = 0\n",
    "        clip_loss = 0\n",
    "        critic_loss = 0\n",
    "        entropy_loss = 0\n",
    "        regularizer_loss = 0\n",
    "        for it in range(train_iterations):\n",
    "            if shuffle:\n",
    "                (\n",
    "                    _observation_buffer,\n",
    "                    _action_buffer,\n",
    "                    _advantage_buffer,\n",
    "                    _return_buffer,\n",
    "                    _logprobability_buffer,\n",
    "                ) = get_shuffled_values(\n",
    "                    observation_buffer,\n",
    "                    action_buffer,\n",
    "                    advantage_buffer,\n",
    "                    return_buffer,\n",
    "                    logprobability_buffer,\n",
    "                )\n",
    "            else:\n",
    "                (\n",
    "                    _observation_buffer,\n",
    "                    _action_buffer,\n",
    "                    _advantage_buffer,\n",
    "                    _return_buffer,\n",
    "                    _logprobability_buffer,\n",
    "                ) = (\n",
    "                    observation_buffer,\n",
    "                    action_buffer,\n",
    "                    advantage_buffer,\n",
    "                    return_buffer,\n",
    "                    logprobability_buffer,\n",
    "                )\n",
    "            temp_clip_loss = 0\n",
    "            temp_regularizer_loss = 0\n",
    "            temp_critic_loss = 0\n",
    "            temp_entropy_loss = 0\n",
    "            stopped_early = False\n",
    "            # do minibatches\n",
    "            for i in range(steps_per_epoch // minibatch_size):\n",
    "                # get the parts of the kl and losses\n",
    "                (\n",
    "                    kl,\n",
    "                    clip_loss_part,\n",
    "                    critic_loss_part,\n",
    "                    entropy_loss_part,\n",
    "                    regularizer_loss_part,\n",
    "                ) = train_mod(\n",
    "                    tf.constant(\n",
    "                        _observation_buffer[\n",
    "                            i * minibatch_size : (i + 1) * minibatch_size\n",
    "                        ]\n",
    "                    ),  # obs\n",
    "                    tf.constant(\n",
    "                        _action_buffer[i * minibatch_size : (i + 1) * minibatch_size]\n",
    "                    ),  # act\n",
    "                    tf.constant(\n",
    "                        _logprobability_buffer[\n",
    "                            i * minibatch_size : (i + 1) * minibatch_size\n",
    "                        ]\n",
    "                    ),  # logprobs\n",
    "                    tf.constant(\n",
    "                        _advantage_buffer[i * minibatch_size : (i + 1) * minibatch_size]\n",
    "                    ),  # advantages\n",
    "                    tf.constant(\n",
    "                        _return_buffer[i * minibatch_size : (i + 1) * minibatch_size]\n",
    "                    ),  # returns\n",
    "                    model,\n",
    "                )\n",
    "                # update the temps\n",
    "                temp_clip_loss += clip_loss_part\n",
    "                temp_critic_loss += critic_loss_part\n",
    "                temp_entropy_loss += entropy_loss_part\n",
    "                temp_regularizer_loss += regularizer_loss_part\n",
    "                if use_kl and kl > 1.5 * target_kl:\n",
    "                    stopped_early = True\n",
    "                    if use_adaptive_kl:\n",
    "                        target_kl *= 1.5\n",
    "                    break\n",
    "\n",
    "            # average the temps to get the amount per that pass\n",
    "            temp_clip_loss /= i + 1\n",
    "            temp_regularizer_loss /= i + 1\n",
    "            temp_critic_loss /= i + 1\n",
    "            temp_entropy_loss /= i + 1\n",
    "\n",
    "            # update the main counts\n",
    "            clip_loss += temp_clip_loss\n",
    "            regularizer_loss += temp_regularizer_loss\n",
    "            critic_loss += temp_critic_loss\n",
    "            entropy_loss += temp_entropy_loss\n",
    "\n",
    "            if stopped_early:\n",
    "                # Early Stopping\n",
    "                break\n",
    "        else:\n",
    "            if use_kl and use_adaptive_kl:\n",
    "                target_kl /= 1.2\n",
    "\n",
    "        clip_loss /= it + 1\n",
    "        critic_loss /= it + 1\n",
    "        regularizer_loss /= it + 1\n",
    "        entropy_loss /= it + 1\n",
    "\n",
    "        # Print mean return and length for each epoch\n",
    "        print(\n",
    "            f\"Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum(lengths) / num_episodes}. STD Length: {np.std(lengths)}\"\n",
    "        )\n",
    "        print(\"=\" * 64)\n",
    "\n",
    "        # log scalars\n",
    "        with summary_writer.as_default():\n",
    "            # episode info\n",
    "            tf.summary.scalar(\"episode/win rate\", num_wins / num_episodes, step=epoch)\n",
    "            tf.summary.scalar(\n",
    "                \"episode/loss rate\", num_losses / num_episodes, step=epoch\n",
    "            )\n",
    "            tf.summary.scalar(\"episode/valid percentage\", num_valid / t, step=epoch)\n",
    "            tf.summary.scalar(\n",
    "                \"episode/mean reward\", sum_return / num_episodes, step=epoch\n",
    "            )\n",
    "            tf.summary.scalar(\n",
    "                \"episode/mean length\", sum(lengths) / num_episodes, step=epoch\n",
    "            )\n",
    "            tf.summary.scalar(\"episode/std length\", np.std(lengths), step=epoch)\n",
    "\n",
    "            # training info\n",
    "            tf.summary.scalar(\"train/clip_loss\", clip_loss, step=epoch)\n",
    "            tf.summary.scalar(\"train/regularizer_loss\", regularizer_loss, step=epoch)\n",
    "            tf.summary.scalar(\"train/kl\", kl, step=epoch)\n",
    "            tf.summary.scalar(\"train/critic_loss\", critic_loss, step=epoch)\n",
    "            tf.summary.scalar(\"train/entropy_loss\", entropy_loss, step=epoch)\n",
    "            tf.summary.scalar(\n",
    "                \"train/total_loss\",\n",
    "                clip_loss + regularizer_loss + critic_loss,\n",
    "                step=epoch,\n",
    "            )\n",
    "            tf.summary.scalar(\"train/num_iterations\", it + 1, step=epoch)\n",
    "\n",
    "        # save every 5\n",
    "        if epoch % 5 == 0:\n",
    "            model.save(\"model_temp.keras\")\n",
    "\n",
    "    return epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 / 75000;\t% valid = 1.0000;\tfps: 12.76;\twin rate: 0.00\tloss rate: 0.00\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-17 21:11:22.961783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 75000 / 75000;\t% valid = 0.9906;\tfps: 420.82;\twin rate: 0.92\tloss rate: 0.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-17 21:14:22.926908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8902\n",
      "2023-07-17 21:14:24.802308: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.18GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-07-17 21:14:33.566615: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.18GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4261. Mean Return: 0.6873828330700431. Mean Length: 19.747235387045812. STD Length: 3.8245949570453277\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9911;\tfps: 419.98;\twin rate: 0.91\tloss rate: 0.08\n",
      "Epoch: 4262. Mean Return: 0.6857199159222304. Mean Length: 19.705727798213346. STD Length: 3.806624171255592\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9929;\tfps: 421.30;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4263. Mean Return: 0.7355340314136154. Mean Length: 19.63350785340314. STD Length: 3.563886305807701\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9938;\tfps: 420.73;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4264. Mean Return: 0.7487706422018342. Mean Length: 19.65923984272608. STD Length: 3.546397644155295\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9928;\tfps: 419.57;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4265. Mean Return: 0.7315800470096674. Mean Length: 19.587359623922694. STD Length: 3.557241067597956\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9926;\tfps: 419.42;\twin rate: 0.93\tloss rate: 0.06\n",
      "Epoch: 4266. Mean Return: 0.7354569258968351. Mean Length: 19.638648860958366. STD Length: 3.5353796843436633\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9930;\tfps: 418.65;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4267. Mean Return: 0.7364748953974924. Mean Length: 19.61297071129707. STD Length: 3.6168913296303997\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9913;\tfps: 419.75;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4268. Mean Return: 0.7045073375262092. Mean Length: 19.654088050314467. STD Length: 3.6996584714895415\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9946;\tfps: 422.00;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4269. Mean Return: 0.7703933910306857. Mean Length: 19.66955153422502. STD Length: 3.3958072192202624\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9930;\tfps: 421.62;\twin rate: 0.91\tloss rate: 0.08\n",
      "Epoch: 4270. Mean Return: 0.7267607843137268. Mean Length: 19.607843137254903. STD Length: 3.6635553539701333\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9931;\tfps: 421.47;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4271. Mean Return: 0.7450691364466474. Mean Length: 19.566918862509784. STD Length: 3.5373260425604722\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9911;\tfps: 422.25;\twin rate: 0.93\tloss rate: 0.06\n",
      "Epoch: 4272. Mean Return: 0.705913361169105. Mean Length: 19.572025052192068. STD Length: 3.726286227809728\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9928;\tfps: 421.95;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4273. Mean Return: 0.7364542355100996. Mean Length: 19.66955153422502. STD Length: 3.5776074172079073\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9940;\tfps: 421.93;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4274. Mean Return: 0.756565603763721. Mean Length: 19.602718243596446. STD Length: 3.4951288532098705\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9945;\tfps: 422.09;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4275. Mean Return: 0.7652394661083498. Mean Length: 19.628369536770478. STD Length: 3.4451732501322594\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9930;\tfps: 422.22;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4276. Mean Return: 0.7408405568689282. Mean Length: 19.70055161544523. STD Length: 3.6005803475833016\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9927;\tfps: 421.01;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4277. Mean Return: 0.7315324267782419. Mean Length: 19.61297071129707. STD Length: 3.6367917667623906\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9915;\tfps: 422.16;\twin rate: 0.92\tloss rate: 0.08\n",
      "Epoch: 4278. Mean Return: 0.7025045895620289. Mean Length: 19.66955153422502. STD Length: 3.7357521949231947\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9945;\tfps: 421.42;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4279. Mean Return: 0.7684396236278115. Mean Length: 19.602718243596446. STD Length: 3.4294522357497135\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9899;\tfps: 421.24;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4280. Mean Return: 0.6693707214323358. Mean Length: 19.747235387045812. STD Length: 3.8888366570782433\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9939;\tfps: 420.72;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4281. Mean Return: 0.7498251565762045. Mean Length: 19.572025052192068. STD Length: 3.5923588402261086\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9948;\tfps: 421.25;\twin rate: 0.92\tloss rate: 0.08\n",
      "Epoch: 4282. Mean Return: 0.7650104275286755. Mean Length: 19.551616266944734. STD Length: 3.357977287720225\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9914;\tfps: 421.24;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4283. Mean Return: 0.698982382329744. Mean Length: 19.72127267946358. STD Length: 3.7835378724893856\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9917;\tfps: 421.99;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4284. Mean Return: 0.7059574468085129. Mean Length: 19.70055161544523. STD Length: 3.714482579811844\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9904;\tfps: 421.32;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4285. Mean Return: 0.6852876280535889. Mean Length: 19.70055161544523. STD Length: 3.791195364385715\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9945;\tfps: 421.24;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4286. Mean Return: 0.7655123859191618. Mean Length: 19.55671447196871. STD Length: 3.484606289308087\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9927;\tfps: 421.99;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4287. Mean Return: 0.7343118785975946. Mean Length: 19.623233908948194. STD Length: 3.6266592705114373\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9927;\tfps: 422.28;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4288. Mean Return: 0.7328919769271117. Mean Length: 19.664394336654432. STD Length: 3.551937835718559\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9932;\tfps: 420.99;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4289. Mean Return: 0.742708496732027. Mean Length: 19.607843137254903. STD Length: 3.5037893476046107\n",
      "================================================================\n",
      "Step 60695 / 75000;\t% valid = 0.9931;\tfps: 421.49;\twin rate: 0.92\tloss rate: 0.07\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/07/17 22:45:09 failed to listen on state port\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 75000 / 75000;\t% valid = 0.9931;\tfps: 421.56;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4290. Mean Return: 0.7408468799160995. Mean Length: 19.664394336654432. STD Length: 3.5813427549454153\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9904;\tfps: 423.72;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4291. Mean Return: 0.6849341412012667. Mean Length: 19.75763962065332. STD Length: 3.874378445375159\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9909;\tfps: 423.53;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4292. Mean Return: 0.6925039370078745. Mean Length: 19.68503937007874. STD Length: 3.7785847467985567\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9917;\tfps: 423.12;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4293. Mean Return: 0.7123730597211256. Mean Length: 19.73164956590371. STD Length: 3.715701963720229\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9916;\tfps: 422.26;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4294. Mean Return: 0.7106703325477899. Mean Length: 19.638648860958366. STD Length: 3.7112080311630167\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9919;\tfps: 422.20;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4295. Mean Return: 0.7107720492017815. Mean Length: 19.628369536770478. STD Length: 3.692451991857616\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9940;\tfps: 422.88;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4296. Mean Return: 0.7565437908496749. Mean Length: 19.607843137254903. STD Length: 3.468768770882775\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9902;\tfps: 423.16;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4297. Mean Return: 0.6805824986821304. Mean Length: 19.768054823405375. STD Length: 3.840243037955174\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9907;\tfps: 421.27;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4298. Mean Return: 0.6938131578947402. Mean Length: 19.736842105263158. STD Length: 3.8268731947574985\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9939;\tfps: 422.24;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4299. Mean Return: 0.7558276762402111. Mean Length: 19.5822454308094. STD Length: 3.485875741315092\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9933;\tfps: 421.97;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4300. Mean Return: 0.7449082328264323. Mean Length: 19.664394336654432. STD Length: 3.5697567536544534\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9911;\tfps: 422.64;\twin rate: 0.92\tloss rate: 0.08\n",
      "Epoch: 4301. Mean Return: 0.6910893707033374. Mean Length: 19.830777366472766. STD Length: 3.815056089396497\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9929;\tfps: 421.29;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4302. Mean Return: 0.7355047071129732. Mean Length: 19.61297071129707. STD Length: 3.6362164751802526\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9940;\tfps: 422.25;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4303. Mean Return: 0.7592797494780812. Mean Length: 19.572025052192068. STD Length: 3.4436305111747214\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9915;\tfps: 422.40;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4304. Mean Return: 0.7078351329997384. Mean Length: 19.752436133789836. STD Length: 3.713157706625158\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9946;\tfps: 420.72;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4305. Mean Return: 0.7724622984919413. Mean Length: 19.50078003120125. STD Length: 3.442473444561675\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9925;\tfps: 421.86;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4306. Mean Return: 0.7310160147020235. Mean Length: 19.690207403517984. STD Length: 3.5939348552532873\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9928;\tfps: 421.77;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4307. Mean Return: 0.7314677206851141. Mean Length: 19.76284584980237. STD Length: 3.611887898818154\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9926;\tfps: 422.03;\twin rate: 0.93\tloss rate: 0.06\n",
      "Epoch: 4308. Mean Return: 0.7333683381464973. Mean Length: 19.690207403517984. STD Length: 3.624051273580343\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9928;\tfps: 420.83;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4309. Mean Return: 0.7336056191467236. Mean Length: 19.51092611862643. STD Length: 3.592922177985456\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9932;\tfps: 421.46;\twin rate: 0.93\tloss rate: 0.06\n",
      "Epoch: 4310. Mean Return: 0.7482551742205924. Mean Length: 19.648938957296306. STD Length: 3.5576607502440196\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9919;\tfps: 420.53;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4311. Mean Return: 0.714471204188486. Mean Length: 19.63350785340314. STD Length: 3.7112716027830928\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9909;\tfps: 421.55;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4312. Mean Return: 0.6951508790343766. Mean Length: 19.679874048806088. STD Length: 3.774871907983832\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9933;\tfps: 420.86;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4313. Mean Return: 0.7455981112277006. Mean Length: 19.674711437565584. STD Length: 3.487447216184341\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9898;\tfps: 421.57;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4314. Mean Return: 0.6752587339112215. Mean Length: 19.70055161544523. STD Length: 3.9298627969350024\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9928;\tfps: 420.24;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4315. Mean Return: 0.7358917364016738. Mean Length: 19.61297071129707. STD Length: 3.63700747764527\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9930;\tfps: 421.08;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4316. Mean Return: 0.736629036492521. Mean Length: 19.690207403517984. STD Length: 3.6362012819296825\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9935;\tfps: 421.04;\twin rate: 0.92\tloss rate: 0.08\n",
      "Epoch: 4317. Mean Return: 0.7395031545741333. Mean Length: 19.71608832807571. STD Length: 3.490161001378303\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9937;\tfps: 420.48;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4318. Mean Return: 0.7502675760755532. Mean Length: 19.674711437565584. STD Length: 3.4384347792276713\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9949;\tfps: 421.38;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4319. Mean Return: 0.7725811518324585. Mean Length: 19.63350785340314. STD Length: 3.351678136949901\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9908;\tfps: 420.74;\twin rate: 0.91\tloss rate: 0.08\n",
      "Epoch: 4320. Mean Return: 0.6829295848660051. Mean Length: 19.705727798213346. STD Length: 3.774531431666547\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9929;\tfps: 419.84;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4321. Mean Return: 0.7371424843423802. Mean Length: 19.572025052192068. STD Length: 3.5797695123256976\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9894;\tfps: 420.91;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4322. Mean Return: 0.6692533403196265. Mean Length: 19.648938957296306. STD Length: 3.9480374607061246\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9909;\tfps: 421.03;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4323. Mean Return: 0.6966832983193305. Mean Length: 19.695378151260503. STD Length: 3.793537137905136\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9949;\tfps: 420.26;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4324. Mean Return: 0.7747785304846282. Mean Length: 19.541427826993225. STD Length: 3.4015948568989693\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9940;\tfps: 420.89;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4325. Mean Return: 0.7500417536534466. Mean Length: 19.572025052192068. STD Length: 3.4381699797120655\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9933;\tfps: 420.04;\twin rate: 0.92\tloss rate: 0.08\n",
      "Epoch: 4326. Mean Return: 0.7384147296944396. Mean Length: 19.587359623922694. STD Length: 3.555405148960687\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9918;\tfps: 421.54;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4327. Mean Return: 0.7158539148712567. Mean Length: 19.705727798213346. STD Length: 3.6718531139014856\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9908;\tfps: 421.41;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4328. Mean Return: 0.6909740089262294. Mean Length: 19.690207403517984. STD Length: 3.7772183558518977\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9935;\tfps: 420.14;\twin rate: 0.94\tloss rate: 0.06\n",
      "Epoch: 4329. Mean Return: 0.7617944023018594. Mean Length: 19.618100967826315. STD Length: 3.4790365582349887\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9932;\tfps: 419.99;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4330. Mean Return: 0.7448992937483695. Mean Length: 19.618100967826315. STD Length: 3.529051635235834\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9907;\tfps: 421.11;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4331. Mean Return: 0.6904876769795556. Mean Length: 19.664394336654432. STD Length: 3.836130253278003\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9912;\tfps: 421.65;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4332. Mean Return: 0.7044164699711541. Mean Length: 19.66955153422502. STD Length: 3.747877620683208\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9921;\tfps: 421.84;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4333. Mean Return: 0.7157687287914384. Mean Length: 19.57713390759593. STD Length: 3.613071086433445\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9942;\tfps: 419.77;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4334. Mean Return: 0.7618697698744777. Mean Length: 19.61297071129707. STD Length: 3.501165251809901\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9917;\tfps: 420.81;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4335. Mean Return: 0.7084850078905852. Mean Length: 19.726459758022095. STD Length: 3.7533613953927714\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9915;\tfps: 420.83;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4336. Mean Return: 0.703911332633789. Mean Length: 19.674711437565584. STD Length: 3.7914080457262336\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9939;\tfps: 420.29;\twin rate: 0.93\tloss rate: 0.06\n",
      "Epoch: 4337. Mean Return: 0.7625182481751847. Mean Length: 19.551616266944734. STD Length: 3.480572477390963\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9884;\tfps: 419.86;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4338. Mean Return: 0.6445454545454595. Mean Length: 19.76284584980237. STD Length: 4.015666332112545\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9939;\tfps: 419.41;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4339. Mean Return: 0.7560833333333352. Mean Length: 19.53125. STD Length: 3.4964360574972146\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9922;\tfps: 420.23;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4340. Mean Return: 0.7179472447114166. Mean Length: 19.587359623922694. STD Length: 3.7061716897407697\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9919;\tfps: 420.48;\twin rate: 0.92\tloss rate: 0.08\n",
      "Epoch: 4341. Mean Return: 0.7085714285714305. Mean Length: 19.695378151260503. STD Length: 3.689164759360512\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9932;\tfps: 420.90;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4342. Mean Return: 0.7443059193294947. Mean Length: 19.64379256155055. STD Length: 3.5958607061229237\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9913;\tfps: 420.43;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4343. Mean Return: 0.705252816347919. Mean Length: 19.648938957296306. STD Length: 3.7672519309204144\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9913;\tfps: 420.19;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4344. Mean Return: 0.7061413612565464. Mean Length: 19.63350785340314. STD Length: 3.828144294835935\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9913;\tfps: 419.86;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4345. Mean Return: 0.7009772072308155. Mean Length: 19.648938957296306. STD Length: 3.743602636879298\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9935;\tfps: 419.86;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4346. Mean Return: 0.7466997907949813. Mean Length: 19.61297071129707. STD Length: 3.5375746311715743\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9920;\tfps: 420.23;\twin rate: 0.93\tloss rate: 0.06\n",
      "Epoch: 4347. Mean Return: 0.7249751113439893. Mean Length: 19.648938957296306. STD Length: 3.687404948408012\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9931;\tfps: 420.80;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4348. Mean Return: 0.7443643512450853. Mean Length: 19.65923984272608. STD Length: 3.629102254073678\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9911;\tfps: 419.89;\twin rate: 0.93\tloss rate: 0.06\n",
      "Epoch: 4349. Mean Return: 0.7046360759493693. Mean Length: 19.77848101265823. STD Length: 3.7974815400614963\n",
      "================================================================\n",
      "Step 50201 / 75000;\t% valid = 0.9944;\tfps: 419.50;\twin rate: 0.92\tloss rate: 0.07\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/07/18 01:53:19 failed to listen on state port\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 75000 / 75000;\t% valid = 0.9930;\tfps: 420.63;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4350. Mean Return: 0.7415127598000533. Mean Length: 19.73164956590371. STD Length: 3.5894362005966323\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9929;\tfps: 421.82;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4351. Mean Return: 0.7373555672268917. Mean Length: 19.695378151260503. STD Length: 3.5920020376545208\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9917;\tfps: 421.97;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4352. Mean Return: 0.70928384736017. Mean Length: 19.602718243596446. STD Length: 3.7339001572461683\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9941;\tfps: 422.16;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4353. Mean Return: 0.7570083463745436. Mean Length: 19.561815336463223. STD Length: 3.4937849941139825\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9942;\tfps: 422.15;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4354. Mean Return: 0.7665005219206685. Mean Length: 19.572025052192068. STD Length: 3.445221535913576\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9916;\tfps: 421.79;\twin rate: 0.93\tloss rate: 0.06\n",
      "Epoch: 4355. Mean Return: 0.7158440198900856. Mean Length: 19.628369536770478. STD Length: 3.689615804479836\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9922;\tfps: 421.91;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4356. Mean Return: 0.7262686567164199. Mean Length: 19.638648860958366. STD Length: 3.649738426355017\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9942;\tfps: 421.65;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4357. Mean Return: 0.7618976768467776. Mean Length: 19.57713390759593. STD Length: 3.4882416849016367\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9928;\tfps: 421.24;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4358. Mean Return: 0.7331854943908186. Mean Length: 19.566918862509784. STD Length: 3.638185074599407\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9924;\tfps: 422.05;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4359. Mean Return: 0.7219554390563582. Mean Length: 19.65923984272608. STD Length: 3.605406301527482\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9933;\tfps: 421.51;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4360. Mean Return: 0.7418536969061348. Mean Length: 19.664394336654432. STD Length: 3.557985647023494\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9928;\tfps: 421.85;\twin rate: 0.92\tloss rate: 0.08\n",
      "Epoch: 4361. Mean Return: 0.7262840670859552. Mean Length: 19.654088050314467. STD Length: 3.640681084569581\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9929;\tfps: 422.04;\twin rate: 0.92\tloss rate: 0.08\n",
      "Epoch: 4362. Mean Return: 0.7273591826041415. Mean Length: 19.648938957296306. STD Length: 3.578806206686648\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9888;\tfps: 421.88;\twin rate: 0.91\tloss rate: 0.08\n",
      "Epoch: 4363. Mean Return: 0.6451541501976319. Mean Length: 19.76284584980237. STD Length: 4.0442402402880635\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9907;\tfps: 422.04;\twin rate: 0.91\tloss rate: 0.08\n",
      "Epoch: 4364. Mean Return: 0.6793782791185771. Mean Length: 19.674711437565584. STD Length: 3.8387159756493427\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9919;\tfps: 421.30;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4365. Mean Return: 0.7156376014663538. Mean Length: 19.638648860958366. STD Length: 3.680743839266264\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9925;\tfps: 421.97;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4366. Mean Return: 0.7352610966057456. Mean Length: 19.5822454308094. STD Length: 3.574554426003814\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9919;\tfps: 420.78;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4367. Mean Return: 0.7095354330708666. Mean Length: 19.68503937007874. STD Length: 3.6778419740897657\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9933;\tfps: 420.83;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4368. Mean Return: 0.7421898767374799. Mean Length: 19.66955153422502. STD Length: 3.5058541926350357\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9945;\tfps: 420.47;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4369. Mean Return: 0.7630542514345351. Mean Length: 19.561815336463223. STD Length: 3.416637076296851\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9911;\tfps: 420.95;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4370. Mean Return: 0.6969916361735503. Mean Length: 19.602718243596446. STD Length: 3.7708864839202314\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9919;\tfps: 418.25;\twin rate: 0.93\tloss rate: 0.06\n",
      "Epoch: 4371. Mean Return: 0.7224369747899188. Mean Length: 19.695378151260503. STD Length: 3.7281097312077596\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9907;\tfps: 420.41;\twin rate: 0.91\tloss rate: 0.08\n",
      "Epoch: 4372. Mean Return: 0.6817559055118169. Mean Length: 19.68503937007874. STD Length: 3.8036467942565158\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9927;\tfps: 420.25;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4373. Mean Return: 0.7323133350799055. Mean Length: 19.648938957296306. STD Length: 3.6369706076825428\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9915;\tfps: 420.97;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4374. Mean Return: 0.7106299212598468. Mean Length: 19.68503937007874. STD Length: 3.728870384215844\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9916;\tfps: 420.87;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4375. Mean Return: 0.7114293207448242. Mean Length: 19.66955153422502. STD Length: 3.696726138343957\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9915;\tfps: 418.30;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4376. Mean Return: 0.7052650918635188. Mean Length: 19.68503937007874. STD Length: 3.757129608985091\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9929;\tfps: 420.70;\twin rate: 0.92\tloss rate: 0.08\n",
      "Epoch: 4377. Mean Return: 0.7305631220534328. Mean Length: 19.64379256155055. STD Length: 3.5408843812070665\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9915;\tfps: 421.79;\twin rate: 0.92\tloss rate: 0.08\n",
      "Epoch: 4378. Mean Return: 0.7028747379454943. Mean Length: 19.654088050314467. STD Length: 3.7348364360420367\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9910;\tfps: 421.01;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4379. Mean Return: 0.6937602108036924. Mean Length: 19.76284584980237. STD Length: 3.8023140192702263\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9927;\tfps: 421.19;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4380. Mean Return: 0.7303402250719717. Mean Length: 19.628369536770478. STD Length: 3.576153417307908\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9900;\tfps: 422.13;\twin rate: 0.91\tloss rate: 0.08\n",
      "Epoch: 4381. Mean Return: 0.6707299077733915. Mean Length: 19.76284584980237. STD Length: 3.884313958917576\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9921;\tfps: 421.40;\twin rate: 0.92\tloss rate: 0.07\n",
      "Epoch: 4382. Mean Return: 0.7194601677148879. Mean Length: 19.654088050314467. STD Length: 3.6541164657529697\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9906;\tfps: 420.92;\twin rate: 0.92\tloss rate: 0.08\n",
      "Epoch: 4383. Mean Return: 0.682946123521685. Mean Length: 19.71090670170828. STD Length: 3.839349395028222\n",
      "================================================================\n",
      "Step 16461 / 75000;\t% valid = 0.9923;\tfps: 419.89;\twin rate: 0.90\tloss rate: 0.09\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/07/18 03:38:50 failed to listen on state port\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 75000 / 75000;\t% valid = 0.9921;\tfps: 422.02;\twin rate: 0.91\tloss rate: 0.08\n",
      "Epoch: 4384. Mean Return: 0.7101990570979583. Mean Length: 19.64379256155055. STD Length: 3.652376318515895\n",
      "================================================================\n",
      "Step 75000 / 75000;\t% valid = 0.9936;\tfps: 421.33;\twin rate: 0.93\tloss rate: 0.07\n",
      "Epoch: 4385. Mean Return: 0.7542633228840125. Mean Length: 19.592476489028215. STD Length: 3.4688591999580716\n",
      "================================================================\n",
      "Step 29644 / 75000;\t% valid = 0.9966;\tfps: 417.97;\twin rate: 0.92\tloss rate: 0.08\r"
     ]
    }
   ],
   "source": [
    "epoch = train_model(model, epoch, use_kl=False, use_adaptive_kl=False, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 10:04:18.974878: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 16777216 exceeds 10% of free system memory.\n",
      "2023-07-18 10:04:18.995663: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 16777216 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "# remove previous\n",
    "os.system(f\"rm models/{model_name}.keras\")\n",
    "time.sleep(1)\n",
    "\n",
    "# save\n",
    "model.save(f\"models/{model_name}.keras\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: keras.Model, num_episodes: int = 10):\n",
    "    mean_reward = 0\n",
    "    mean_len = 0\n",
    "    num_won = 0\n",
    "    num_lost = 0\n",
    "    for _ in range(num_episodes):\n",
    "        # initialize vars\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        i = 0\n",
    "\n",
    "        # complete a round\n",
    "        while not done:\n",
    "            # step process\n",
    "            logits, _ = model(np.expand_dims(obs, axis=0))\n",
    "            logits = logits.numpy().flatten()\n",
    "            action = np.argmax(logits)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "\n",
    "            # update counts\n",
    "            mean_reward += reward\n",
    "            i += 1\n",
    "        # update mean length\n",
    "        mean_len += i\n",
    "\n",
    "        if env.won:\n",
    "            num_won += 1\n",
    "        if env.lost:\n",
    "            num_lost += 1\n",
    "\n",
    "    # average them\n",
    "    mean_reward /= num_episodes\n",
    "    mean_len /= num_episodes\n",
    "    print(\"Episode mean reward:\", mean_reward)\n",
    "    print(\"Episode mean length:\", mean_len)\n",
    "    print(f\"win rate: {num_won / num_episodes}, loss rate: {num_lost / num_episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 10:04:30.265398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode mean reward: 0.5906000000000351\n",
      "Episode mean length: 19.63\n",
      "win rate: 0.93, loss rate: 0.055\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, num_episodes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
