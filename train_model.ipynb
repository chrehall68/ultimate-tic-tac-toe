{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultimate TTT Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-23 12:42:26.217276: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-23 12:42:26.805291: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-06-23 12:42:27.443819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-23 12:42:27.460376: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-23 12:42:27.460539: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import scipy.signal\n",
    "\n",
    "# used for fps logging\n",
    "from datetime import datetime\n",
    "\n",
    "# sanity check\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to send data\n",
    "import os\n",
    "import time\n",
    "import socket\n",
    "\n",
    "# proto definitions\n",
    "import py.board_pb2 as pb\n",
    "\n",
    "# misc\n",
    "from typing import Tuple\n",
    "\n",
    "# math\n",
    "import numpy as np\n",
    "\n",
    "# board constants\n",
    "ROWS = 3\n",
    "COLS = 3\n",
    "CELLS = 9\n",
    "\n",
    "# socket constants\n",
    "S_PORT = 8000\n",
    "A_PORT = 8001\n",
    "R_PORT = 8002\n",
    "MAX_MSG_SIZE = 512\n",
    "\n",
    "# reward parameters\n",
    "WIN_REWARD = 10\n",
    "CELL_REWARD = 2\n",
    "INVALID_PENALTY = -10\n",
    "\n",
    "# debugging\n",
    "used_actions = dict()\n",
    "cur_state = None\n",
    "\n",
    "\n",
    "class UltimateTicTacToeEnv:\n",
    "    obs_dim = (9, 9, 4)\n",
    "    n_actions = CELLS * CELLS\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.s_conn, self.a_conn, self.r_conn = None, None, None\n",
    "        self.reset()\n",
    "\n",
    "    def _receive(self, conn: socket.socket, tp: type):\n",
    "        ret = tp()\n",
    "        b = conn.recv(MAX_MSG_SIZE)\n",
    "        ret.ParseFromString(b)\n",
    "        return ret\n",
    "\n",
    "    def _get_return(self) -> pb.ReturnMessage:\n",
    "        return self._receive(self.r_conn, pb.ReturnMessage)\n",
    "\n",
    "    def _get_state(self) -> pb.StateMessage:\n",
    "        return self._receive(self.s_conn, pb.StateMessage)\n",
    "\n",
    "    def _make_coord(self, idx) -> pb.Coord:\n",
    "        return pb.Coord(row=idx // COLS, col=idx % COLS)\n",
    "\n",
    "    def _send_action(self, move) -> None:\n",
    "        action = pb.ActionMessage(move=move)\n",
    "        self.a_conn.send(action.SerializeToString())\n",
    "\n",
    "    def _to_idx(self, coord: pb.Coord) -> int:\n",
    "        return coord.row * COLS + coord.col\n",
    "\n",
    "    def _to_multi_idx(self, move: pb.Move) -> int:\n",
    "        return self._to_idx(move.large) * CELLS + self._to_idx(move.small)\n",
    "\n",
    "    def _process_state(self, state: pb.StateMessage) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        The structure of the state:\n",
    "        (9, 9, 4)\n",
    "        Outer 9 represent board cells\n",
    "        inner 9 represent the cell spaces\n",
    "        each space has 3 objects:\n",
    "            space owner (0, 1, 2) representing if the space is claimed or not\n",
    "            cell owner (0, 1, 2) representing if the cell the space belongs to is claimed or not\n",
    "            curcellornot (0, 1); 1 if the space belongs to the current cell, 0 if not\n",
    "            turn (1, 2) 1 if the current turn is player1, 2 if the current turn is player2\n",
    "        \"\"\"\n",
    "        board_state = np.zeros(self.obs_dim)\n",
    "        for cell_idx in range(len(state.board.cells)):\n",
    "            for space_idx in range(len(state.board.cells[cell_idx].spaces)):\n",
    "                board_state[cell_idx, space_idx, 0] = (\n",
    "                    state.board.cells[cell_idx].spaces[space_idx].val\n",
    "                )\n",
    "                board_state[cell_idx, space_idx, 1] = state.cellowners[cell_idx]\n",
    "                board_state[cell_idx, space_idx, 2] = (\n",
    "                    1 if self._to_idx(state.board.curCell) == cell_idx else 0\n",
    "                )\n",
    "                board_state[cell_idx, space_idx, 3] = state.turn\n",
    "\n",
    "        return board_state\n",
    "\n",
    "    def _get_exploration_reward(self, action: int, msg: pb.ReturnMessage) -> float:\n",
    "        if msg.valid:\n",
    "            return 0\n",
    "        return INVALID_PENALTY\n",
    "\n",
    "    def _get_win_reward(self, msg: pb.ReturnMessage) -> float:\n",
    "        \"\"\"\n",
    "        Get's the reward for winning if the game was won\n",
    "        \"\"\"\n",
    "        # the turn sent in the return message should still be the caller's turn\n",
    "        if msg.state.winner == msg.state.turn:\n",
    "            return WIN_REWARD\n",
    "        return 0\n",
    "\n",
    "    def _get_cell_reward(self, msg: pb.ReturnMessage) -> float:\n",
    "        \"\"\"\n",
    "        Get's the reward for claiming a cell if a cell was claimed\n",
    "        \"\"\"\n",
    "        if self.prev_cellowners == msg.state.cellowners:\n",
    "            return 0\n",
    "        elif list(msg.state.cellowners).count(\n",
    "            msg.state.turn\n",
    "        ) > self.prev_cellowners.count(msg.state.turn):\n",
    "            self.prev_cellowners = list(msg.state.cellowners)\n",
    "            return CELL_REWARD\n",
    "        return 0\n",
    "\n",
    "    def _get_reward(self, action: pb.Move, msg: pb.ReturnMessage) -> float:\n",
    "        return (\n",
    "            self._get_exploration_reward(action, msg)\n",
    "            + self._get_cell_reward(msg)\n",
    "            + self._get_win_reward(msg)\n",
    "        )\n",
    "    \n",
    "    def _reset_vars(self):\n",
    "        self.prev_cellowners = [pb.NONE] * 9\n",
    "        self.stop_when_invalid = True\n",
    "\n",
    "        self.s_conn = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.a_conn = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.r_conn = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "        self.s_conn.connect((\"\", 8000))\n",
    "        self.a_conn.connect((\"\", 8001))\n",
    "        self.r_conn.connect((\"\", 8002))\n",
    "\n",
    "    # public section\n",
    "    def observe(self) -> np.ndarray:\n",
    "        global cur_state\n",
    "        state = self._get_state()\n",
    "        cur_state = state\n",
    "        self._turn = state.turn\n",
    "        return self._process_state(state)\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            - next state\n",
    "            - reward for the action\n",
    "            - done / not done\n",
    "            - valid / invalid\n",
    "        \"\"\"\n",
    "        # update the count\n",
    "        if action not in used_actions:\n",
    "            used_actions[action] = 1\n",
    "        else:\n",
    "            used_actions[action] += 1\n",
    "\n",
    "        # send action and get response\n",
    "        self._send_action(self.to_move(action))\n",
    "        ret_message = self._get_return()\n",
    "\n",
    "        # return information\n",
    "        reward = self._get_reward(action, ret_message)\n",
    "        done = ret_message.state.done\n",
    "        if not done:\n",
    "            if self.stop_when_invalid:\n",
    "                # stop the episode if the move was invalid\n",
    "                return self.observe(), reward, not ret_message.valid, ret_message.valid\n",
    "            else:\n",
    "                return self.observe(), reward, done, ret_message.valid\n",
    "        else:\n",
    "            return (\n",
    "                self._process_state(ret_message.state),\n",
    "                reward,\n",
    "                done,\n",
    "                ret_message.valid,\n",
    "            )\n",
    "\n",
    "    def turn(self):\n",
    "        return self._turn\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        while 1:\n",
    "            try:\n",
    "                self.cleanup()\n",
    "                self.pid = os.spawnl(os.P_NOWAIT, \"uttt\", \"uttt\", \"aivai\")\n",
    "                time.sleep(0.12)\n",
    "                self._reset_vars()\n",
    "                break\n",
    "            except ConnectionRefusedError:\n",
    "                pass\n",
    "        return self.observe()\n",
    "\n",
    "    def cleanup(self):\n",
    "        os.system(\"killall -q uttt\")\n",
    "        if self.s_conn is not None:\n",
    "            self.s_conn.close()\n",
    "            self.r_conn.close()\n",
    "            self.a_conn.close()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.cleanup()\n",
    "\n",
    "    def to_move(self, idx: int) -> pb.Move:\n",
    "        outer_idx = idx // CELLS\n",
    "        inner_idx = idx % CELLS\n",
    "\n",
    "        return pb.Move(\n",
    "            large=self._make_coord(outer_idx), small=self._make_coord(inner_idx)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UltimateTicTacToeEnv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buffers and Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the PPO algorithm\n",
    "clip_ratio = 0.2\n",
    "target_kl = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    # Buffer for storing trajectories\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        # Buffer initialization\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, *observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, action, reward, value, logprobability):\n",
    "        # Append one step of agent-environment interaction\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.logprobability_buffer[self.pointer] = logprobability\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0):\n",
    "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            deltas, self.gamma * self.lam\n",
    "        )\n",
    "        self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            rewards, self.gamma\n",
    "        )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        # Get all data of the buffer and normalize the advantages\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.logprobability_buffer,\n",
    "        )\n",
    "\n",
    "\n",
    "def logprobabilities(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    logprobabilities_all = tf.nn.log_softmax(logits)\n",
    "    logprobability = tf.reduce_sum(\n",
    "        tf.one_hot(a, UltimateTicTacToeEnv.n_actions) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    return logprobability\n",
    "\n",
    "\n",
    "# Sample action from actor\n",
    "@tf.function\n",
    "def sample_action(observation, actor):\n",
    "    logits = actor(observation)\n",
    "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "    return logits, action\n",
    "\n",
    "\n",
    "# Train the policy by maxizing the PPO-Clip objective\n",
    "@tf.function\n",
    "def train_policy(\n",
    "    observation_buffer,\n",
    "    action_buffer,\n",
    "    logprobability_buffer,\n",
    "    advantage_buffer,\n",
    "    actor: keras.Model,\n",
    "    policy_optimizer: tf.keras.optimizers.Optimizer,\n",
    "):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        ratio = tf.exp(\n",
    "            logprobabilities(actor(observation_buffer), action_buffer)\n",
    "            - logprobability_buffer\n",
    "        )\n",
    "        min_advantage = tf.where(\n",
    "            advantage_buffer > 0,\n",
    "            (1 + clip_ratio) * advantage_buffer,\n",
    "            (1 - clip_ratio) * advantage_buffer,\n",
    "        )\n",
    "\n",
    "        policy_loss = -tf.reduce_mean(\n",
    "            tf.minimum(ratio * advantage_buffer, min_advantage)\n",
    "        ) + tf.reduce_sum(actor.losses)\n",
    "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
    "\n",
    "    kl = tf.reduce_mean(\n",
    "        logprobability_buffer\n",
    "        - logprobabilities(actor(observation_buffer), action_buffer)\n",
    "    )\n",
    "    kl = tf.reduce_sum(kl)\n",
    "    return kl\n",
    "\n",
    "\n",
    "# Train the value function by regression on mean-squared error\n",
    "@tf.function\n",
    "def train_value_function(\n",
    "    observation_buffer,\n",
    "    return_buffer,\n",
    "    critic: keras.Model,\n",
    "    value_optimizer: tf.keras.optimizers.Optimizer,\n",
    "):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        value_loss = tf.reduce_mean(\n",
    "            (return_buffer - critic(observation_buffer)) ** 2\n",
    "        ) + tf.reduce_sum(critic.losses)\n",
    "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
    "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-23 12:42:27.655991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-23 12:42:27.656169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-23 12:42:27.656273: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-23 12:42:28.101119: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-23 12:42:28.101309: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-23 12:42:28.101438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-23 12:42:28.101524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4069 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "def create_actor():\n",
    "    # model inputs\n",
    "    inputs = tf.keras.Input(shape=UltimateTicTacToeEnv.obs_dim)\n",
    "\n",
    "    x = layers.Conv1D(\n",
    "        256,\n",
    "        kernel_size=9,\n",
    "        strides=1,\n",
    "        padding=\"valid\",\n",
    "        activation=\"relu\",\n",
    "        activity_regularizer=\"l2\",\n",
    "    )(inputs)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Conv2D(\n",
    "        1024,\n",
    "        kernel_size=(9, 1),\n",
    "        strides=(1, 1),\n",
    "        padding=\"valid\",\n",
    "        activation=\"relu\",\n",
    "        activity_regularizer=\"l2\",\n",
    "    )(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1024, activation=\"relu\", activity_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(1024, activation=\"relu\", activity_regularizer=\"l2\")(x)\n",
    "\n",
    "    logits = tf.keras.layers.Dense(UltimateTicTacToeEnv.n_actions)(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "\n",
    "def create_critic():\n",
    "    inputs = tf.keras.Input(shape=UltimateTicTacToeEnv.obs_dim)\n",
    "\n",
    "    x = layers.Conv1D(\n",
    "        256,\n",
    "        kernel_size=9,\n",
    "        strides=1,\n",
    "        padding=\"valid\",\n",
    "        activation=\"relu\",\n",
    "        activity_regularizer=\"l2\",\n",
    "    )(inputs)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Conv2D(\n",
    "        1024,\n",
    "        kernel_size=(9, 1),\n",
    "        strides=(1, 1),\n",
    "        padding=\"valid\",\n",
    "        activation=\"relu\",\n",
    "        activity_regularizer=\"l2\",\n",
    "    )(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1024, activation=\"relu\", activity_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(1024, activation=\"relu\", activity_regularizer=\"l2\")(x)\n",
    "\n",
    "    values = tf.keras.layers.Dense(1)(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=values)\n",
    "\n",
    "\n",
    "if load_model and os.path.exists(\"actor1.keras\") and os.path.exists(\"critic1.keras\"):\n",
    "    print(\"loading model...\")\n",
    "    actor1 = tf.keras.models.load_model(\"actor1.keras\")\n",
    "    critic1 = tf.keras.models.load_model(\"critic1.keras\")\n",
    "else:\n",
    "    print(\"creating model...\")\n",
    "    actor1 = create_actor()\n",
    "    critic1 = create_critic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 9, 9, 4)]         0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 9, 1, 256)         9472      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 9, 1, 256)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 1, 1, 1024)        2360320   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1, 1, 1024)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 81)                83025     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,552,017\n",
      "Trainable params: 4,552,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "actor1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 9, 9, 4)]         0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 9, 1, 256)         9472      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 9, 1, 256)         0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 1, 1, 1024)        2360320   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1, 1, 1024)        0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,470,017\n",
      "Trainable params: 4,470,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "critic1.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate hyperparams\n",
    "policy_learning_rate = 1e-4  # 3e-4\n",
    "value_function_learning_rate = 8e-4  # 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor1_optimizer = tf.keras.optimizers.Adam(learning_rate=policy_learning_rate)\n",
    "critic1_optimizer = tf.keras.optimizers.Adam(learning_rate=value_function_learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training hyperparameters\n",
    "\n",
    "# alg hyperparameters\n",
    "gamma = 0.99\n",
    "lam = 0.97\n",
    "\n",
    "# training time hyperparameters\n",
    "train_policy_iterations = 30\n",
    "train_value_iterations = 30\n",
    "epochs = 30*3*5  # 25 * 4096 * 2 ~ 1 hr\n",
    "minibatch_size = 128\n",
    "steps_per_epoch = 2048  # should be a multiple of minibatch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 119\n",
    "summary_writer = tf.summary.create_file_writer(f\"./logs/ppo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = Buffer(UltimateTicTacToeEnv.obs_dim, steps_per_epoch, gamma=gamma, lam=lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training function\n",
    "def train_model(\n",
    "    actor, critic, actor_optimizer, critic_optimizer, player1, epoch_start\n",
    ") -> int:\n",
    "    if player1:\n",
    "        cond = lambda turn: turn == pb.Owner.PLAYER1\n",
    "    else:\n",
    "        cond = lambda turn: turn == pb.Owner.PLAYER2\n",
    "\n",
    "    # Iterate over the number of epochs\n",
    "    for epoch in range(epoch_start, epoch_start + epochs):\n",
    "        # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "        sum_return = 0\n",
    "        sum_length = 0\n",
    "        num_episodes = 0\n",
    "        episode_return = 0\n",
    "        episode_length = 0\n",
    "\n",
    "        # logging variables\n",
    "        num_valid = 0\n",
    "        num_wins = 0\n",
    "\n",
    "        # Iterate over the steps of each epoch\n",
    "        observation = env.reset()\n",
    "        t = 0\n",
    "        start_time = datetime.now()\n",
    "        while t < steps_per_epoch:\n",
    "            if cond(env.turn()):  # if it's the training ai's turn\n",
    "                env.stop_when_invalid = True\n",
    "                # Get the logits, action, and take one step in the environment\n",
    "                observation = observation.reshape(1, *env.obs_dim)\n",
    "                logits, action = sample_action(observation, actor)\n",
    "                observation_new, reward, done, valid = env.step(action[0].numpy())\n",
    "                episode_return += reward\n",
    "                episode_length += 1\n",
    "\n",
    "                # logging variables\n",
    "                num_valid += 1 if valid else 0\n",
    "                num_wins += 1 if reward >= WIN_REWARD else 0\n",
    "\n",
    "                # Get the value and log-probability of the action\n",
    "                value_t = critic(observation)\n",
    "                logprobability_t = logprobabilities(logits, action)\n",
    "\n",
    "                # Store obs, act, rew, v_t, logp_pi_t\n",
    "                buffer.store(observation, action, reward, value_t, logprobability_t)\n",
    "\n",
    "                t += 1\n",
    "            else:\n",
    "                env.stop_when_invalid = False\n",
    "                # just keep trying random actions until it works\n",
    "                observation_new, reward, done, valid = env.step(\n",
    "                    np.random.randint(0, env.n_actions)\n",
    "                )\n",
    "\n",
    "            # Update the observation\n",
    "            observation = observation_new\n",
    "\n",
    "            # Finish trajectory if reached to a terminal state\n",
    "            terminal = done\n",
    "            if terminal or t == steps_per_epoch - 1:\n",
    "                last_value = 0 if done else critic(observation.reshape(1, *env.obs_dim))\n",
    "                buffer.finish_trajectory(last_value)\n",
    "                sum_return += episode_return\n",
    "                sum_length += episode_length\n",
    "                num_episodes += 1\n",
    "                observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "            print(\n",
    "                f\"Step {t} / {steps_per_epoch}; \"\n",
    "                + f\"% valid = {num_valid / (t+1)}; \"\n",
    "                + f\"fps: {t/(datetime.now()-start_time).total_seconds()}; \"\n",
    "                + f\"win rate: {num_wins/num_episodes if num_episodes > 0 else 0}\",\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "        print()\n",
    "\n",
    "        # Get values from the buffer\n",
    "        # 0 - observation_buffer,\n",
    "        # 1 - action_buffer,\n",
    "        # 2 - advantage_buffer,\n",
    "        # 3 - return_buffer,\n",
    "        # 4 - logprobability_buffer,\n",
    "        (\n",
    "            observation_buffer,\n",
    "            action_buffer,\n",
    "            advantage_buffer,\n",
    "            return_buffer,\n",
    "            logprobability_buffer,\n",
    "        ) = buffer.get()\n",
    "\n",
    "        # Update the policy and implement early stopping using KL divergence\n",
    "        for _ in range(train_policy_iterations):\n",
    "            kl = 0\n",
    "            for i in range(steps_per_epoch // minibatch_size):\n",
    "                kl += train_policy(\n",
    "                    tf.constant(\n",
    "                        observation_buffer[\n",
    "                            i * minibatch_size : (i + 1) * minibatch_size\n",
    "                        ]\n",
    "                    ),  # obs\n",
    "                    tf.constant(\n",
    "                        action_buffer[i * minibatch_size : (i + 1) * minibatch_size]\n",
    "                    ),  # act\n",
    "                    tf.constant(\n",
    "                        logprobability_buffer[\n",
    "                            i * minibatch_size : (i + 1) * minibatch_size\n",
    "                        ]\n",
    "                    ),  # logprobs\n",
    "                    tf.constant(\n",
    "                        advantage_buffer[i * minibatch_size : (i + 1) * minibatch_size]\n",
    "                    ),  # advantages\n",
    "                    actor,\n",
    "                    actor_optimizer,\n",
    "                )\n",
    "            kl /= steps_per_epoch // minibatch_size\n",
    "            if kl > 1.5 * target_kl:\n",
    "                # Early Stopping\n",
    "                break\n",
    "\n",
    "        # Update the value function\n",
    "        for _ in range(train_value_iterations):\n",
    "            for i in range(steps_per_epoch // minibatch_size):\n",
    "                train_value_function(\n",
    "                    tf.constant(\n",
    "                        observation_buffer[\n",
    "                            i * minibatch_size : (i + 1) * minibatch_size\n",
    "                        ]\n",
    "                    ),  # obs buffer\n",
    "                    tf.constant(\n",
    "                        return_buffer[i * minibatch_size : (i + 1) * minibatch_size]\n",
    "                    ),  # returns\n",
    "                    critic,\n",
    "                    critic_optimizer,\n",
    "                )\n",
    "\n",
    "        # Print mean return and length for each epoch\n",
    "        print(\n",
    "            f\"Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum_length / num_episodes}\"\n",
    "        )\n",
    "        print(\"=\" * 64)\n",
    "\n",
    "        # log scalars\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar(\"episode/win rate\", num_wins / num_episodes, step=epoch)\n",
    "            tf.summary.scalar(\"episode/valid percentage\", num_valid / t, step=epoch)\n",
    "            tf.summary.scalar(\"episode/mean reward\", sum_return / num_episodes, step=epoch)\n",
    "            tf.summary.scalar(\"episode/mean length\", sum_length / num_episodes, step=epoch)\n",
    "\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-23 12:42:29.336930: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2048 / 2048; % valid = 0.9804782820888238; fps: 50.641458891412476; win rate: 0.36842105263157896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-23 12:43:09.856455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-06-23 12:43:10.410055: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7faa264dd410 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-23 12:43:10.410074: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2023-06-23 12:43:10.412897: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-06-23 12:43:10.503080: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 120. Mean Return: 6.684210526315789. Mean Length: 17.95614035087719\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9790141532454856; fps: 49.514736243123544; win rate: 0.44915254237288146\n",
      "Epoch: 121. Mean Return: 7.11864406779661. Mean Length: 17.347457627118644\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9775500244021474; fps: 50.71705948036311; win rate: 0.398230088495575234\n",
      "Epoch: 122. Mean Return: 6.389380530973451. Mean Length: 18.115044247787612\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9799902391410444; fps: 49.60971710955088; win rate: 0.458333333333333366\n",
      "Epoch: 123. Mean Return: 7.466666666666667. Mean Length: 17.058333333333334\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9790141532454856; fps: 50.10041120499611; win rate: 0.385964912280701735\n",
      "Epoch: 124. Mean Return: 6.456140350877193. Mean Length: 17.95614035087719\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9775500244021474; fps: 47.92257957511579; win rate: 0.362903225806451666\n",
      "Epoch: 125. Mean Return: 5.52. Mean Length: 16.376\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9677891654465594; fps: 48.000417003622715; win rate: 0.32283464566929136\n",
      "Epoch: 126. Mean Return: 3.4173228346456694. Mean Length: 16.118110236220474\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9760858955588092; fps: 49.014992508699805; win rate: 0.33613445378151263\n",
      "Epoch: 127. Mean Return: 5.159663865546219. Mean Length: 17.201680672268907\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9716935090287946; fps: 48.286667381535686; win rate: 0.33057851239669423\n",
      "Epoch: 128. Mean Return: 4.231404958677686. Mean Length: 16.917355371900825\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9834065397755003; fps: 49.060189546246185; win rate: 0.45614035087719296\n",
      "Epoch: 129. Mean Return: 8.263157894736842. Mean Length: 17.95614035087719\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9726695949243533; fps: 48.306232074743704; win rate: 0.34426229508196724\n",
      "Epoch: 130. Mean Return: 4.721311475409836. Mean Length: 16.778688524590162\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9760858955588092; fps: 48.52176132287759; win rate: 0.322314049586776845\n",
      "Epoch: 131. Mean Return: 5.041322314049586. Mean Length: 16.917355371900825\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9790141532454856; fps: 47.892597109373085; win rate: 0.38333333333333336\n",
      "Epoch: 132. Mean Return: 6.283333333333333. Mean Length: 17.058333333333334\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.977061981454368; fps: 47.77846804081242; win rate: 0.3305785123966942335\n",
      "Epoch: 133. Mean Return: 5.140495867768595. Mean Length: 16.917355371900825\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9795021961932651; fps: 48.48390956822853; win rate: 0.449152542372881466\n",
      "Epoch: 134. Mean Return: 7.271186440677966. Mean Length: 17.347457627118644\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9858467545143973; fps: 48.77356048158461; win rate: 0.464912280701754436\n",
      "Epoch: 135. Mean Return: 8.596491228070175. Mean Length: 17.95614035087719\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9760858955588092; fps: 47.73236759815015; win rate: 0.344537815126050415\n",
      "Epoch: 136. Mean Return: 5.226890756302521. Mean Length: 17.201680672268907\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9863347974621767; fps: 49.99731703850462; win rate: 0.518181818181818274\n",
      "Epoch: 137. Mean Return: 9.709090909090909. Mean Length: 18.60909090909091\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9834065397755003; fps: 47.7869720254179; win rate: 0.4188034188034188335\n",
      "Epoch: 138. Mean Return: 7.897435897435898. Mean Length: 17.495726495726494\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9814543679843827; fps: 48.49850246665088; win rate: 0.470085470085470166\n",
      "Epoch: 139. Mean Return: 7.863247863247863. Mean Length: 17.495726495726494\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9795021961932651; fps: 47.84761320195082; win rate: 0.300884955752212455\n",
      "Epoch: 140. Mean Return: 5.628318584070796. Mean Length: 18.115044247787612\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9760858955588092; fps: 46.16795908021974; win rate: 0.349593495934959366\n",
      "Epoch: 141. Mean Return: 5.365853658536586. Mean Length: 16.642276422764226\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9804782820888238; fps: 48.69722557068488; win rate: 0.380530973451327455\n",
      "Epoch: 142. Mean Return: 7.0265486725663715. Mean Length: 18.115044247787612\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9814543679843827; fps: 48.32977215910614; win rate: 0.417391304347826065\n",
      "Epoch: 143. Mean Return: 7.234782608695652. Mean Length: 17.8\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9765739385065886; fps: 47.0855276882905; win rate: 0.3916666666666666635\n",
      "Epoch: 144. Mean Return: 6.083333333333333. Mean Length: 17.058333333333334\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9819424109321621; fps: 47.757116632369616; win rate: 0.36752136752136755\n",
      "Epoch: 145. Mean Return: 6.564102564102564. Mean Length: 17.495726495726494\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9795021961932651; fps: 46.68767581682802; win rate: 0.364406779661016955\n",
      "Epoch: 146. Mean Return: 6.661016949152542. Mean Length: 17.347457627118644\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9819424109321621; fps: 47.77866198889925; win rate: 0.464912280701754446\n",
      "Epoch: 147. Mean Return: 8.175438596491228. Mean Length: 17.95614035087719\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9809663250366032; fps: 47.12359248179897; win rate: 0.347826086956521734\n",
      "Epoch: 148. Mean Return: 6.71304347826087. Mean Length: 17.8\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.984382625671059; fps: 47.044505871317426; win rate: 0.438596491228070155\n",
      "Epoch: 149. Mean Return: 8.228070175438596. Mean Length: 17.95614035087719\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.977061981454368; fps: 46.313987760533706; win rate: 0.341666666666666743\n",
      "Epoch: 150. Mean Return: 5.5. Mean Length: 17.058333333333334\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.984382625671059; fps: 46.68551852828097; win rate: 0.4871794871794871757\n",
      "Epoch: 151. Mean Return: 8.666666666666666. Mean Length: 17.495726495726494\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9814543679843827; fps: 47.90729003688604; win rate: 0.473214285714285757\n",
      "Epoch: 152. Mean Return: 8.142857142857142. Mean Length: 18.276785714285715\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9785261102977062; fps: 46.53097495206719; win rate: 0.393162393162393155\n",
      "Epoch: 153. Mean Return: 6.666666666666667. Mean Length: 17.495726495726494\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9809663250366032; fps: 46.8900624022475; win rate: 0.4260869565217391883\n",
      "Epoch: 154. Mean Return: 7.530434782608696. Mean Length: 17.8\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9848706686188384; fps: 46.554123134791894; win rate: 0.44736842105263166\n",
      "Epoch: 155. Mean Return: 8.491228070175438. Mean Length: 17.95614035087719\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9795021961932651; fps: 45.9036855504298; win rate: 0.3697478991596639446\n",
      "Epoch: 156. Mean Return: 6.38655462184874. Mean Length: 17.201680672268907\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9829184968277208; fps: 47.53035368918978; win rate: 0.491071428571428557\n",
      "Epoch: 157. Mean Return: 8.821428571428571. Mean Length: 18.276785714285715\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9824304538799414; fps: 46.74589029899064; win rate: 0.426086956521739185\n",
      "Epoch: 158. Mean Return: 7.6869565217391305. Mean Length: 17.8\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9795021961932651; fps: 45.256746072662445; win rate: 0.37551260504201677\n",
      "Epoch: 159. Mean Return: 6.55. Mean Length: 17.058333333333334\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9824304538799414; fps: 46.18467664592446; win rate: 0.460176991150442476\n",
      "Epoch: 160. Mean Return: 8.265486725663717. Mean Length: 18.115044247787612\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.984382625671059; fps: 45.80336098263954; win rate: 0.3859649122807017335\n",
      "Epoch: 161. Mean Return: 7.807017543859649. Mean Length: 17.95614035087719\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9824304538799414; fps: 46.22150691230449; win rate: 0.401785714285714343\n",
      "Epoch: 162. Mean Return: 7.571428571428571. Mean Length: 18.276785714285715\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.974621766715471; fps: 44.74885122401; win rate: 0.3666666666666666439996\n",
      "Epoch: 163. Mean Return: 5.366666666666666. Mean Length: 17.058333333333334\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9795021961932651; fps: 45.55559564408428; win rate: 0.330434782608695634\n",
      "Epoch: 164. Mean Return: 6.121739130434783. Mean Length: 17.8\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9790141532454856; fps: 45.152621372032065; win rate: 0.37606837606837606\n",
      "Epoch: 165. Mean Return: 6.153846153846154. Mean Length: 17.495726495726494\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9726695949243533; fps: 44.65008288171635; win rate: 0.308333333333333357\n",
      "Epoch: 166. Mean Return: 4.3. Mean Length: 17.058333333333334\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9799902391410444; fps: 45.323145953033666; win rate: 0.33913043478260875\n",
      "Epoch: 167. Mean Return: 6.3130434782608695. Mean Length: 17.8\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9785261102977062; fps: 45.504600919539556; win rate: 0.37719298245614036\n",
      "Epoch: 168. Mean Return: 6.385964912280702. Mean Length: 17.95614035087719\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9790141532454856; fps: 45.05191892687209; win rate: 0.387931034482758666\n",
      "Epoch: 169. Mean Return: 6.5. Mean Length: 17.646551724137932\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9824304538799414; fps: 45.612682445997024; win rate: 0.43243243243243246\n",
      "Epoch: 170. Mean Return: 7.945945945945946. Mean Length: 18.44144144144144\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.972181551976574; fps: 44.999947705138894; win rate: 0.372881355932203486\n",
      "Epoch: 171. Mean Return: 5.101694915254237. Mean Length: 17.347457627118644\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9824304538799414; fps: 44.345887441866125; win rate: 0.38461538461538464\n",
      "Epoch: 172. Mean Return: 7.264957264957265. Mean Length: 17.495726495726494\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9731576378721327; fps: 44.6097532037285; win rate: 0.3589743589743591445\n",
      "Epoch: 173. Mean Return: 5.0256410256410255. Mean Length: 17.495726495726494\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9814543679843827; fps: 44.18772031019952; win rate: 0.410256410256410247\n",
      "Epoch: 174. Mean Return: 7.316239316239316. Mean Length: 17.495726495726494\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9775500244021474; fps: 44.45166398119; win rate: 0.356521739130434817555\n",
      "Epoch: 175. Mean Return: 5.756521739130434. Mean Length: 17.8\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9799902391410444; fps: 44.54606612450247; win rate: 0.391304347826087166\n",
      "Epoch: 176. Mean Return: 6.782608695652174. Mean Length: 17.8\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9712054660810151; fps: 43.0996910585524; win rate: 0.3114754098360656555\n",
      "Epoch: 177. Mean Return: 3.8688524590163933. Mean Length: 16.778688524590162\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9716935090287946; fps: 43.25241428936431; win rate: 0.341666666666666745\n",
      "Epoch: 178. Mean Return: 4.5. Mean Length: 17.058333333333334\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9741337237676916; fps: 43.44084773456827; win rate: 0.355932203389830595\n",
      "Epoch: 179. Mean Return: 5.338983050847458. Mean Length: 17.347457627118644\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9775500244021474; fps: 43.391563413072745; win rate: 0.39316239316239315\n",
      "Epoch: 180. Mean Return: 6.3076923076923075. Mean Length: 17.495726495726494\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9751098096632503; fps: 43.335669185150394; win rate: 0.31932773109243695\n",
      "Epoch: 181. Mean Return: 5.07563025210084. Mean Length: 17.201680672268907\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9751098096632503; fps: 42.427830684697966; win rate: 0.34426229508196724\n",
      "Epoch: 182. Mean Return: 5.295081967213115. Mean Length: 16.778688524590162\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9809663250366032; fps: 44.59517575137318; win rate: 0.396396396396396474\n",
      "Epoch: 183. Mean Return: 7.387387387387387. Mean Length: 18.44144144144144\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9755978526110298; fps: 42.710428839816664; win rate: 0.35294117647058826\n",
      "Epoch: 184. Mean Return: 5.378151260504202. Mean Length: 17.201680672268907\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9780380673499268; fps: 44.17969226945988; win rate: 0.495575221238938056\n",
      "Epoch: 185. Mean Return: 7.628318584070796. Mean Length: 18.115044247787612\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9741337237676916; fps: 42.941417485707454; win rate: 0.36206896551724145\n",
      "Epoch: 186. Mean Return: 5.379310344827586. Mean Length: 17.646551724137932\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9775500244021474; fps: 42.776958141367906; win rate: 0.41025641025641024\n",
      "Epoch: 187. Mean Return: 6.495726495726496. Mean Length: 17.495726495726494\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9809663250366032; fps: 43.50377075518064; win rate: 0.412280701754385975\n",
      "Epoch: 188. Mean Return: 7.385964912280702. Mean Length: 17.95614035087719\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9775500244021474; fps: 42.82323644847914; win rate: 0.394736842105263163\n",
      "Epoch: 189. Mean Return: 6.350877192982456. Mean Length: 17.95614035087719\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9790141532454856; fps: 41.82871036898192; win rate: 0.303278688524590173\n",
      "Epoch: 190. Mean Return: 5.360655737704918. Mean Length: 16.778688524590162\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9780380673499268; fps: 42.272926065065136; win rate: 0.36974789915966396\n",
      "Epoch: 191. Mean Return: 6.151260504201681. Mean Length: 17.201680672268907\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9765739385065886; fps: 42.76141348687465; win rate: 0.376068376068376066\n",
      "Epoch: 192. Mean Return: 5.965811965811966. Mean Length: 17.495726495726494\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9775500244021474; fps: 42.01552494161986; win rate: 0.393162393162393155\n",
      "Epoch: 193. Mean Return: 6.52991452991453. Mean Length: 17.495726495726494\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9814543679843827; fps: 42.39824338777864; win rate: 0.379310344827586293\n",
      "Epoch: 194. Mean Return: 6.879310344827586. Mean Length: 17.646551724137932\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9804782820888238; fps: 43.39164431597956; win rate: 0.415929203539823155\n",
      "Epoch: 195. Mean Return: 7.433628318584071. Mean Length: 18.115044247787612\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9790141532454856; fps: 42.034141287134254; win rate: 0.41880341880341885\n",
      "Epoch: 196. Mean Return: 6.666666666666667. Mean Length: 17.495726495726494\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9853587115666179; fps: 41.856353816990854; win rate: 0.45762711864406786\n",
      "Epoch: 197. Mean Return: 8.135593220338983. Mean Length: 17.347457627118644\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9819424109321621; fps: 42.06112146368759; win rate: 0.417391304347826065\n",
      "Epoch: 198. Mean Return: 7.530434782608696. Mean Length: 17.8\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9780380673499268; fps: 41.9927383319086; win rate: 0.3739130434782609366\n",
      "Epoch: 199. Mean Return: 6.208695652173913. Mean Length: 17.8\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9809663250366032; fps: 42.51978649336975; win rate: 0.398230088495575233\n",
      "Epoch: 200. Mean Return: 7.150442477876106. Mean Length: 18.115044247787612\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.977061981454368; fps: 41.49642259502904; win rate: 0.3247863247863248773\n",
      "Epoch: 201. Mean Return: 5.282051282051282. Mean Length: 17.495726495726494\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9790141532454856; fps: 42.01129742866244; win rate: 0.439473684210526316\n",
      "Epoch: 202. Mean Return: 6.765217391304348. Mean Length: 17.8\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9785261102977062; fps: 41.013042167435145; win rate: 0.42016806722689076\n",
      "Epoch: 203. Mean Return: 6.705882352941177. Mean Length: 17.201680672268907\n",
      "================================================================\n",
      "Step 2048 / 2048; % valid = 0.9760858955588092; fps: 41.49305883729925; win rate: 0.358974358974359145\n",
      "Epoch: 204. Mean Return: 5.47008547008547. Mean Length: 17.495726495726494\n",
      "================================================================\n",
      "Step 1328 / 2048; % valid = 0.9781790820165538; fps: 40.71888454785787; win rate: 0.394736842105263166\r"
     ]
    }
   ],
   "source": [
    "epoch = train_model(actor1, critic1, actor1_optimizer, critic1_optimizer, True, epoch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# remove previous\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m os\u001b[39m.\u001b[39msystem(\u001b[39m\"\u001b[39m\u001b[39mrm actor1.keras critic1.keras\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39m# save\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# remove previous\n",
    "os.system(\"rm actor1.keras critic1.keras\")\n",
    "time.sleep(1)\n",
    "\n",
    "# save\n",
    "actor1.save(\"actor1.keras\")\n",
    "critic1.save(\"critic1.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
