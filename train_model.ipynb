{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultimate TTT Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 21:36:35.331523: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-17 21:36:36.040134: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-17 21:36:37.085484: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 21:36:37.110091: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 21:36:37.110310: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import scipy.signal\n",
    "\n",
    "# used for fps logging\n",
    "from datetime import datetime\n",
    "\n",
    "# sanity check\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train.ini']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"train.ini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample action from actor\n",
    "@tf.function\n",
    "def sample_action_value(observation, model: keras.Model):\n",
    "    logits, value = model(observation, training=False)\n",
    "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "    return logits, action, value\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def best_action_value(observation, model: keras.Model):\n",
    "    logits, value = model(observation, training=False)\n",
    "    action = tf.argmax(logits, axis=-1)\n",
    "    return logits, action, value\n",
    "\n",
    "@tf.function\n",
    "def best_action(observation, model:keras.Model):\n",
    "    logits, _ = model(observation, training=False)\n",
    "    action = tf.argmax(logits, axis=-1)\n",
    "    return action[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env Constants\n",
    "MAX_TIMESTEPS = config[\"ENV\"].getint(\"MAX_TIMESTEPS\")\n",
    "\n",
    "# board constants\n",
    "ROWS = config[\"ENV\"].getint(\"ROWS\")\n",
    "COLS = config[\"ENV\"].getint(\"COLS\")\n",
    "CELLS = config[\"ENV\"].getint(\"CELLS\")\n",
    "\n",
    "# socket constants\n",
    "S_PORT = config[\"ENV\"].getint(\"S_PORT\")\n",
    "A_PORT = config[\"ENV\"].getint(\"A_PORT\")\n",
    "R_PORT = config[\"ENV\"].getint(\"R_PORT\")\n",
    "MAX_MSG_SIZE = config[\"ENV\"].getint(\"MAX_MSG_SIZE\")\n",
    "\n",
    "# reward parameters\n",
    "WIN_REWARD = config[\"REWARD\"].getfloat(\"WIN_REWARD\")\n",
    "CELL_REWARD = config[\"REWARD\"].getfloat(\"CELL_REWARD\")\n",
    "VALID_REWARD = config[\"REWARD\"].getfloat(\"VALID_REWARD\")\n",
    "TIMEOUT_PENALTY = config[\"REWARD\"].getfloat(\"TIMEOUT_PENALTY\")\n",
    "INVALID_PENALTY = config[\"REWARD\"].getfloat(\"INVALID_PENALTY\")\n",
    "LOSS_PENALTY = config[\"REWARD\"].getfloat(\"LOSS_PENALTY\")\n",
    "TIE_REWARD = config[\"REWARD\"].getfloat(\"TIE_REWARD\")\n",
    "\n",
    "# misc\n",
    "SLEEP_TIME = config[\"ENV\"].getfloat(\"SLEEP_TIME\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opponents\n",
    "\n",
    "# math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# opponents\n",
    "class Opponent:\n",
    "    def __init__(self, _env=None) -> None:\n",
    "        self.env = _env\n",
    "    def get_action(self, obs) -> int:\n",
    "        pass\n",
    "\n",
    "\n",
    "class ValidRandomOpponent(Opponent):\n",
    "    \"\"\"\n",
    "    Makes valid moves when there is a cur cell\n",
    "    \"\"\"\n",
    "    def __init__(self, _env=None) -> None:\n",
    "        super().__init__(_env)\n",
    "\n",
    "    def get_action(self, obs: np.ndarray) -> int:\n",
    "        return np.random.choice(self.env.validmoves)\n",
    "\n",
    "\n",
    "class CellWinningRandomOpponent(ValidRandomOpponent):\n",
    "    \"\"\"\n",
    "    Wins a cell if possible\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, _env = None) -> None:\n",
    "        super().__init__(_env)\n",
    "\n",
    "    def get_cur_cell(self, obs: np.ndarray) -> tuple[bool, int]:\n",
    "        # determine if there is a current cell\n",
    "        cur_cell_exists = False\n",
    "        cur_cell = -1\n",
    "        for outer in range(obs.shape[0]):\n",
    "            if obs[outer, 0, 2] == 1:\n",
    "                cur_cell_exists = True\n",
    "                cur_cell = outer\n",
    "                break\n",
    "        return cur_cell_exists, cur_cell\n",
    "\n",
    "    def get_winning_cells(self, cell: list[int], turn: int) -> list[bool]:\n",
    "        \"\"\"\n",
    "        Returns a list of bools, representing whether that space\n",
    "        will win the given cell\n",
    "        Note: this works as long as the parameters are the correct types\n",
    "        \"\"\"\n",
    "        cell = np.array(cell).reshape((ROWS, COLS)).tolist()\n",
    "        ret = [[False for _ in range(COLS)] for x in range(ROWS)]\n",
    "\n",
    "        # check horizontals\n",
    "        for row in range(ROWS):\n",
    "            if cell[row].count(turn) == 2 and cell[row].count(0) == 1:\n",
    "                # this row is winnable\n",
    "                winning_cell = cell[row].index(0)\n",
    "                ret[row][winning_cell] = True\n",
    "\n",
    "        # check verticals\n",
    "        for c in range(COLS):\n",
    "            col = [cell[i][c] for i in range(ROWS)]\n",
    "            if col.count(turn) == 2 and col.count(0) == 1:\n",
    "                # this col is winnable\n",
    "                winning_row = col.index(0)\n",
    "                ret[winning_row][c] = True\n",
    "\n",
    "        # check diagonal left\n",
    "        left_diagonal = [cell[i][i] for i in range(ROWS)]\n",
    "        if left_diagonal.count(turn) == 2 and left_diagonal.count(0) == 1:\n",
    "            winning_space = left_diagonal.index(0)\n",
    "            ret[winning_space][winning_space] = True\n",
    "\n",
    "        # check diagonal right\n",
    "        right_diagonal = [cell[i][ROWS - 1 - i] for i in range(ROWS)]\n",
    "        if right_diagonal.count(turn) == 2 and right_diagonal.count(0) == 1:\n",
    "            winning_space = right_diagonal.index(0)\n",
    "            ret[winning_space][ROWS - 1 - winning_space] = True\n",
    "\n",
    "        return np.array(ret).flatten().tolist()\n",
    "\n",
    "    def get_turn(self, obs: np.ndarray) -> int:\n",
    "        # whose turn it is\n",
    "        return obs[0, 0, 3]\n",
    "\n",
    "    def _get_cellwinning_action(self, cell: int, obs: np.ndarray) -> tuple[bool, int]:\n",
    "        \"\"\"\n",
    "        Get whether there is a cell winning action, and if there is,\n",
    "        return a random action from those\n",
    "        Returns:\n",
    "            * bool - whether there is a cell winning action\n",
    "            * int - the action number, or -1 if none\n",
    "        \"\"\"\n",
    "        spaces = [obs[cell, i, 0] for i in range(CELLS)]\n",
    "        winning_spaces = self.get_winning_cells(spaces, self.get_turn(obs))\n",
    "\n",
    "        # if any of them are cell-winning spaces, choose one of them\n",
    "        if any(winning_spaces):\n",
    "            idxs = [i for i in range(CELLS) if winning_spaces[i]]\n",
    "            return True, cell * CELLS + np.random.choice(idxs)\n",
    "        return False, -1\n",
    "\n",
    "    def get_action(self, obs: np.ndarray) -> int:\n",
    "        cur_cell_exists, cur_cell = self.get_cur_cell(obs)\n",
    "        if cur_cell_exists:\n",
    "            valid, action = self._get_cellwinning_action(cur_cell, obs)\n",
    "            if valid:\n",
    "                return action\n",
    "        return super().get_action(obs)\n",
    "\n",
    "\n",
    "class WinningRandomOpponent(CellWinningRandomOpponent):\n",
    "    def __init__(self, _env=None) -> None:\n",
    "        super().__init__(_env)\n",
    "\n",
    "    def get_winning_action(self, obs: np.ndarray) -> tuple[bool, int]:\n",
    "        turn = self.get_turn(obs)\n",
    "\n",
    "        cur_cell_exists, cur_cell = self.get_cur_cell(obs)\n",
    "\n",
    "        # go through all the cells and see which are claimed\n",
    "        owners: list[int] = []\n",
    "        for outer in range(obs.shape[0]):\n",
    "            owners.append(obs[outer, 0, 1])\n",
    "\n",
    "        # see which cells are winning cells\n",
    "        winning_cells = self.get_winning_cells(owners, turn)\n",
    "        if True in winning_cells:\n",
    "            if cur_cell_exists:\n",
    "                # if the cur cell is a possible winning cell\n",
    "                if winning_cells[cur_cell]:\n",
    "                    # get the space owners\n",
    "                    space_owners = [obs[cur_cell, i, 0] for i in range(obs.shape[1])]\n",
    "                    winning_spaces = self.get_winning_cells(space_owners, turn)\n",
    "                    # it is possible\n",
    "                    if True in winning_spaces:\n",
    "                        return True, cur_cell * CELLS + winning_spaces.index(True)\n",
    "            else:\n",
    "                # we can go anywhere\n",
    "                for potential_winning_cell_idx in range(CELLS):\n",
    "                    if winning_cells[potential_winning_cell_idx]:\n",
    "                        # get the space owners\n",
    "                        space_owners = [\n",
    "                            obs[potential_winning_cell_idx, i, 0]\n",
    "                            for i in range(obs.shape[1])\n",
    "                        ]\n",
    "                        winning_spaces = self.get_winning_cells(space_owners, turn)\n",
    "                        # it is possible\n",
    "                        if True in winning_spaces:\n",
    "                            return (\n",
    "                                True,\n",
    "                                potential_winning_cell_idx * CELLS\n",
    "                                + winning_spaces.index(True),\n",
    "                            )\n",
    "\n",
    "        # winning isn't possible currently\n",
    "        return False, -1\n",
    "\n",
    "    def get_action(self, obs: np.ndarray) -> int:\n",
    "        winnable, action = self.get_winning_action(obs)\n",
    "        if winnable:\n",
    "            return action\n",
    "        else:\n",
    "            cur_cell_exists, _ = self.get_cur_cell(obs)\n",
    "            if not cur_cell_exists:\n",
    "                # check if any of the cells can be won\n",
    "                for cell in range(CELLS):\n",
    "                    # if it's already claimed, move on\n",
    "                    if obs[cell, 0, 1] != 0:\n",
    "                        continue\n",
    "                    valid, action = self._get_cellwinning_action(cell, obs)\n",
    "                    if valid:\n",
    "                        return action\n",
    "\n",
    "        return super().get_action(obs)\n",
    "    \n",
    "class AIOpponent(ValidRandomOpponent):\n",
    "    def __init__(self, model_name, _env=None) -> None:\n",
    "        super().__init__(_env)\n",
    "        self.model = keras.models.load_model(f\"./models/{model_name}.keras\")\n",
    "    \n",
    "    def get_action(self, obs: np.ndarray) -> int:\n",
    "        action = best_action(obs.reshape(1, *obs.shape), self.model)\n",
    "        if action not in self.env.validmoves:\n",
    "            return super().get_action(obs)\n",
    "        return action\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to send data\n",
    "import os\n",
    "import time\n",
    "import socket\n",
    "\n",
    "# proto definitions\n",
    "import py.board_pb2 as pb\n",
    "\n",
    "# misc\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "# env\n",
    "class UltimateTicTacToeEnv:\n",
    "    obs_dim = (9, 9, 4)\n",
    "    n_actions = CELLS * CELLS\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        opponent: Opponent = ValidRandomOpponent(),\n",
    "        max_timesteps: int = 81,\n",
    "        player1: bool = True,\n",
    "    ) -> None:\n",
    "        opponent.env = self\n",
    "        self.s_conn, self.a_conn, self.r_conn = None, None, None\n",
    "        self.opponent = opponent\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.player1 = player1\n",
    "        os.system(\"./uttt aivai &\")\n",
    "        self._reset_connection_vars()\n",
    "        self._reset_vars()\n",
    "        self.reset()\n",
    "\n",
    "    def _receive(self, conn: socket.socket, tp: type):\n",
    "        ret = tp()\n",
    "        b = conn.recv(MAX_MSG_SIZE)\n",
    "        ret.ParseFromString(b)\n",
    "        return ret\n",
    "\n",
    "    def _get_return(self) -> pb.ReturnMessage:\n",
    "        return self._receive(self.r_conn, pb.ReturnMessage)\n",
    "\n",
    "    def _get_state(self) -> pb.StateMessage:\n",
    "        return self._receive(self.s_conn, pb.StateMessage)\n",
    "\n",
    "    def _make_coord(self, idx) -> pb.Coord:\n",
    "        return pb.Coord(row=idx // COLS, col=idx % COLS)\n",
    "\n",
    "    def _send_action(self, move) -> None:\n",
    "        action = pb.ActionMessage(move=move)\n",
    "        self.a_conn.send(action.SerializeToString())\n",
    "\n",
    "    def _to_idx(self, coord: pb.Coord) -> int:\n",
    "        return coord.row * COLS + coord.col\n",
    "\n",
    "    def _to_multi_idx(self, move: pb.Move) -> int:\n",
    "        return self._to_idx(move.large) * CELLS + self._to_idx(move.small)\n",
    "\n",
    "    def _process_state(self, state: pb.StateMessage) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        The structure of the state:\n",
    "        (9, 9, 4)\n",
    "        Outer 9 represent board cells\n",
    "        inner 9 represent the cell spaces\n",
    "        each space has 5 objects:\n",
    "            space owner (0, 1, 2) representing if the space is claimed or not\n",
    "            cell owner (0, 1, 2) representing if the cell the space belongs to is claimed or not\n",
    "            curcellornot (0, 1); 1 if the space belongs to the current cell, 0 if not\n",
    "            valid (0, 1); 1 if the space is a valid move, 0 if not\n",
    "        \"\"\"\n",
    "        board_state = np.zeros(self.obs_dim)\n",
    "        try:\n",
    "            for cell_idx in range(len(state.board.cells)):\n",
    "                for space_idx in range(len(state.board.cells[cell_idx].spaces)):\n",
    "                    board_state[cell_idx, space_idx, 0] = (\n",
    "                        state.board.cells[cell_idx].spaces[space_idx].val\n",
    "                    )\n",
    "                    board_state[cell_idx, space_idx, 1] = state.cellowners[cell_idx]\n",
    "                    board_state[cell_idx, space_idx, 2] = (\n",
    "                        1 if self._to_idx(state.board.curCell) == cell_idx else 0\n",
    "                    )\n",
    "        except Exception as e:\n",
    "            print(state)\n",
    "            raise e\n",
    "\n",
    "        self.validmoves = list(map(lambda move: self._to_multi_idx(move), state.validmoves))\n",
    "        for idx in self.validmoves:\n",
    "            board_state[idx//CELLS, idx % CELLS, 3] = 1\n",
    "\n",
    "        return board_state\n",
    "\n",
    "    def _get_exploration_reward(self, action: int, msg: pb.ReturnMessage) -> float:\n",
    "        if msg.valid:\n",
    "            return VALID_REWARD\n",
    "        return INVALID_PENALTY\n",
    "\n",
    "    def _get_win_reward(self, msg: pb.ReturnMessage) -> float:\n",
    "        \"\"\"\n",
    "        Get's the reward for winning if the game was won\n",
    "        THIS ONLY APPLIES WHEN it is the player turn;\n",
    "        everything else is passed indirectly\n",
    "        \"\"\"\n",
    "        # the turn sent in the return message should still be the caller's turn\n",
    "        if msg.state.winner == msg.state.turn:\n",
    "            if self.player_turn:\n",
    "                self.won = True\n",
    "                return WIN_REWARD\n",
    "            else:\n",
    "                self.lost = True\n",
    "                return LOSS_PENALTY\n",
    "        elif self.done:\n",
    "            self.tied = True\n",
    "            return TIE_REWARD\n",
    "        return 0\n",
    "\n",
    "    def _get_cell_reward(self, msg: pb.ReturnMessage) -> float:\n",
    "        \"\"\"\n",
    "        Get's the reward for claiming a cell if a cell was claimed\n",
    "        \"\"\"\n",
    "        if self.prev_cellowners == msg.state.cellowners:\n",
    "            return 0\n",
    "        elif list(msg.state.cellowners).count(\n",
    "            msg.state.turn\n",
    "        ) > self.prev_cellowners.count(msg.state.turn):\n",
    "            self.prev_cellowners = list(msg.state.cellowners)\n",
    "            return CELL_REWARD\n",
    "        return 0\n",
    "\n",
    "    def _get_timeout_reward(self):\n",
    "        if self.cur_timestep > self.max_timesteps:\n",
    "            return TIMEOUT_PENALTY\n",
    "        return 0\n",
    "\n",
    "    def _get_reward(self, action: pb.Move, msg: pb.ReturnMessage) -> float:\n",
    "        return (\n",
    "            self._get_exploration_reward(action, msg)\n",
    "            + self._get_cell_reward(msg)\n",
    "            + self._get_win_reward(msg)\n",
    "            + self._get_timeout_reward()\n",
    "        )\n",
    "\n",
    "    def _step(self, action: int) -> Tuple[np.ndarray, float, bool, bool]:\n",
    "        \"\"\"\n",
    "        Updates self.done\n",
    "        \"\"\"\n",
    "        # send action and get response\n",
    "        self._send_action(self.to_move(action))\n",
    "        ret_message = self._get_return()\n",
    "\n",
    "        # return information\n",
    "        self.done = ret_message.state.done\n",
    "        reward = self._get_reward(action, ret_message)\n",
    "        if not self.done:\n",
    "            return self.observe(), reward, self.done, ret_message.valid\n",
    "        else:\n",
    "            return (\n",
    "                self._process_state(ret_message.state),\n",
    "                reward,\n",
    "                self.done,\n",
    "                ret_message.valid,\n",
    "            )\n",
    "\n",
    "    def _take_opponent_turn(self) -> Tuple[np.ndarray, float, bool, bool]:\n",
    "        valid = False\n",
    "        while not valid:\n",
    "            obs, reward, done, valid = self._step(\n",
    "                self.opponent.get_action(self.cur_state)\n",
    "            )\n",
    "        return obs, reward, done, valid\n",
    "\n",
    "    def _reset_vars(self):\n",
    "        self.prev_cellowners = [pb.NONE] * 9\n",
    "        self.cur_state = None  # the current state; used for debugging\n",
    "        self.won = False  # whether or not the player won\n",
    "        self.done = False  # if the game is over\n",
    "        self.lost = False  # whether or not the player lost\n",
    "        self.tied = False  # whether or not the player tied\n",
    "        self.player_turn = True  # whether or not it is the player's turn\n",
    "        self.cur_timestep = 0\n",
    "\n",
    "    def _reset_connection_vars(self):\n",
    "        self.s_conn = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.a_conn = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.r_conn = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "        time.sleep(2)\n",
    "        self.s_conn.connect((\"\", 8000))\n",
    "        self.a_conn.connect((\"\", 8001))\n",
    "        self.r_conn.connect((\"\", 8002))\n",
    "\n",
    "        # drain the state connection\n",
    "        self.observe()\n",
    "    # public section\n",
    "    def observe(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Updates self.cur_state and self._turn\n",
    "        \"\"\"\n",
    "        state = self._get_state()\n",
    "        self._turn = state.turn\n",
    "        self.cur_state = self._process_state(state)\n",
    "        return self.cur_state\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool]:\n",
    "        \"\"\"\n",
    "        Updates current timestep\n",
    "\n",
    "        Returns:\n",
    "            - next state\n",
    "            - reward for the action\n",
    "            - done / not done\n",
    "            - valid / invalid\n",
    "        \"\"\"\n",
    "        self.player_turn = True\n",
    "        self.cur_timestep += 1\n",
    "        obs, reward, done, valid = self._step(action)\n",
    "        done = done or self.cur_timestep > self.max_timesteps\n",
    "\n",
    "        # take opponents turn\n",
    "        if valid and not done:\n",
    "            self.player_turn = False\n",
    "            obs, reward2, done, _ = self._take_opponent_turn()\n",
    "\n",
    "            # add the \"lost\" penalty\n",
    "            if self.done:\n",
    "                if self.lost:\n",
    "                    return obs, reward + reward2, done, valid\n",
    "                else:\n",
    "                    # tie\n",
    "                    assert self.tied\n",
    "                    return obs, reward + TIE_REWARD, done, valid\n",
    "            # penalize losing cells\n",
    "            elif reward2 == CELL_REWARD + VALID_REWARD:\n",
    "                return obs, reward - CELL_REWARD * 0.5, done, valid\n",
    "            # nothing special, just return the reward\n",
    "            return obs, reward, done, valid\n",
    "        return obs, reward, done, valid\n",
    "\n",
    "    def turn(self):\n",
    "        return self._turn\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        if not self.done:\n",
    "            # send invalid move\n",
    "            self._send_action(self.to_move(-1))\n",
    "        self._reset_vars()\n",
    "        if self.player1:\n",
    "            return self.observe()\n",
    "        else:\n",
    "            self.observe()\n",
    "            obs, _, _, _ = self._take_opponent_turn()\n",
    "            return obs\n",
    "\n",
    "    def cleanup(self):\n",
    "        if self.s_conn is not None:\n",
    "            self.s_conn.close()\n",
    "            self.r_conn.close()\n",
    "            self.a_conn.close()\n",
    "\n",
    "    def __del__(self):\n",
    "        os.system(\"killall -q uttt\")\n",
    "        self.cleanup()\n",
    "\n",
    "    def to_move(self, idx: int) -> pb.Move:\n",
    "        outer_idx = idx // CELLS\n",
    "        inner_idx = idx % CELLS\n",
    "\n",
    "        return pb.Move(\n",
    "            large=self._make_coord(outer_idx), small=self._make_coord(inner_idx)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Could not deserialize class 'Functional' because its parent module keras.src.engine.functional cannot be imported. Full object config: {'module': 'keras.src.engine.functional', 'class_name': 'Functional', 'config': {'name': 'model', 'trainable': True, 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_input_shape': [None, 9, 9, 4], 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'input_1'}, 'registered_name': None, 'name': 'input_1', 'inbound_nodes': []}, {'module': 'keras.layers', 'class_name': 'Reshape', 'config': {'name': 'reshape', 'trainable': True, 'dtype': 'float32', 'target_shape': [9, 36]}, 'registered_name': None, 'build_config': {'input_shape': [None, 9, 9, 4]}, 'name': 'reshape', 'inbound_nodes': [[['input_1', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense', 'trainable': True, 'dtype': 'float32', 'units': 256, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 9, 36]}, 'name': 'dense', 'inbound_nodes': [[['reshape', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_1', 'trainable': True, 'dtype': 'float32', 'units': 256, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 9, 36]}, 'name': 'dense_1', 'inbound_nodes': [[['reshape', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 256, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 9, 36]}, 'name': 'dense_2', 'inbound_nodes': [[['reshape', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Add', 'config': {'name': 'add', 'trainable': True, 'dtype': 'float32'}, 'registered_name': None, 'build_config': {'input_shape': [[None, 9, 256], [None, 9, 256], [None, 9, 256]]}, 'name': 'add', 'inbound_nodes': [[['dense', 0, 0, {}], ['dense_1', 0, 0, {}], ['dense_2', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 9, 256]}, 'name': 'layer_normalization', 'inbound_nodes': [[['add', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Flatten', 'config': {'name': 'flatten', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}, 'registered_name': None, 'build_config': {'input_shape': [None, 9, 256]}, 'name': 'flatten', 'inbound_nodes': [[['layer_normalization', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_3', 'trainable': True, 'dtype': 'float32', 'units': 2048, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': {'module': 'keras.regularizers', 'class_name': 'L2', 'config': {'l2': 0.009999999776482582}, 'registered_name': None}, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2304]}, 'name': 'dense_3', 'inbound_nodes': [[['flatten', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'AlphaDropout', 'config': {'name': 'alpha_dropout', 'trainable': True, 'dtype': 'float32', 'rate': 0.2, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2048]}, 'name': 'alpha_dropout', 'inbound_nodes': [[['dense_3', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_4', 'trainable': True, 'dtype': 'float32', 'units': 2048, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': {'module': 'keras.regularizers', 'class_name': 'L2', 'config': {'l2': 0.009999999776482582}, 'registered_name': None}, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2048]}, 'name': 'dense_4', 'inbound_nodes': [[['alpha_dropout', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_5', 'trainable': True, 'dtype': 'float32', 'units': 2048, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': {'module': 'keras.regularizers', 'class_name': 'L2', 'config': {'l2': 0.009999999776482582}, 'registered_name': None}, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2048]}, 'name': 'dense_5', 'inbound_nodes': [[['dense_4', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_6', 'trainable': True, 'dtype': 'float32', 'units': 2048, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': {'module': 'keras.regularizers', 'class_name': 'L2', 'config': {'l2': 0.009999999776482582}, 'registered_name': None}, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2048]}, 'name': 'dense_6', 'inbound_nodes': [[['dense_4', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_7', 'trainable': True, 'dtype': 'float32', 'units': 2048, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': {'module': 'keras.regularizers', 'class_name': 'L2', 'config': {'l2': 0.009999999776482582}, 'registered_name': None}, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2048]}, 'name': 'dense_7', 'inbound_nodes': [[['dense_4', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Add', 'config': {'name': 'add_1', 'trainable': True, 'dtype': 'float32'}, 'registered_name': None, 'build_config': {'input_shape': [[None, 2048], [None, 2048], [None, 2048]]}, 'name': 'add_1', 'inbound_nodes': [[['dense_5', 0, 0, {}], ['dense_6', 0, 0, {}], ['dense_7', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_1', 'trainable': True, 'dtype': 'float32', 'axis': [1], 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2048]}, 'name': 'layer_normalization_1', 'inbound_nodes': [[['add_1', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_8', 'trainable': True, 'dtype': 'float32', 'units': 2048, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': {'module': 'keras.regularizers', 'class_name': 'L2', 'config': {'l2': 0.009999999776482582}, 'registered_name': None}, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2048]}, 'name': 'dense_8', 'inbound_nodes': [[['layer_normalization_1', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'AlphaDropout', 'config': {'name': 'alpha_dropout_1', 'trainable': True, 'dtype': 'float32', 'rate': 0.3, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2048]}, 'name': 'alpha_dropout_1', 'inbound_nodes': [[['dense_8', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_9', 'trainable': True, 'dtype': 'float32', 'units': 2048, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': {'module': 'keras.regularizers', 'class_name': 'L2', 'config': {'l2': 0.009999999776482582}, 'registered_name': None}, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2048]}, 'name': 'dense_9', 'inbound_nodes': [[['alpha_dropout_1', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'AlphaDropout', 'config': {'name': 'alpha_dropout_2', 'trainable': True, 'dtype': 'float32', 'rate': 0.2, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2048]}, 'name': 'alpha_dropout_2', 'inbound_nodes': [[['dense_9', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_10', 'trainable': True, 'dtype': 'float32', 'units': 1024, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2048]}, 'name': 'dense_10', 'inbound_nodes': [[['alpha_dropout_2', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_12', 'trainable': True, 'dtype': 'float32', 'units': 512, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 1024]}, 'name': 'dense_12', 'inbound_nodes': [[['dense_10', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_14', 'trainable': True, 'dtype': 'float32', 'units': 512, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 1024]}, 'name': 'dense_14', 'inbound_nodes': [[['dense_10', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_11', 'trainable': True, 'dtype': 'float32', 'units': 81, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 512]}, 'name': 'dense_11', 'inbound_nodes': [[['dense_12', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_13', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 512]}, 'name': 'dense_13', 'inbound_nodes': [[['dense_14', 0, 0, {}]]]}], 'input_layers': [['input_1', 0, 0]], 'output_layers': [['dense_11', 0, 0], ['dense_13', 0, 0]]}, 'registered_name': 'Functional', 'build_config': {'input_shape': [None, 9, 9, 4]}, 'compile_config': {'optimizer': {'module': 'keras.optimizers', 'class_name': 'Adam', 'config': {'name': 'Adam', 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'jit_compile': True, 'is_legacy_optimizer': False, 'learning_rate': 4.999999873689376e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': None}, 'loss': None, 'metrics': None, 'loss_weights': None, 'weighted_metrics': None, 'run_eagerly': None, 'steps_per_execution': None, 'jit_compile': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/keras/saving/serialization_lib.py:541\u001b[0m, in \u001b[0;36m_retrieve_class_or_fn\u001b[0;34m(name, registered_name, module, obj_type, full_config, custom_objects)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 541\u001b[0m     mod \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(module)\n\u001b[1;32m    542\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1206\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1128\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1206\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1128\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1206\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1142\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.src'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[39m=\u001b[39m UltimateTicTacToeEnv(\n\u001b[1;32m      2\u001b[0m     max_timesteps\u001b[39m=\u001b[39mMAX_TIMESTEPS,\n\u001b[0;32m----> 3\u001b[0m     opponent\u001b[39m=\u001b[39m AIOpponent(\u001b[39m\"\u001b[39;49m\u001b[39mattenppo8_p2\u001b[39;49m\u001b[39m\"\u001b[39;49m), \u001b[39m# WinningRandomOpponent(), # AIOpponent(\"attenppo8\"),\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     player1\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m )\n",
      "Cell \u001b[0;32mIn[5], line 181\u001b[0m, in \u001b[0;36mAIOpponent.__init__\u001b[0;34m(self, model_name, _env)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model_name, _env\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(_env)\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mload_model(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./models/\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel_name\u001b[39m}\u001b[39;49;00m\u001b[39m.keras\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/keras/saving/saving_api.py:204\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[39mif\u001b[39;00m kwargs:\n\u001b[1;32m    200\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    201\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe following argument(s) are not supported \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwith the native Keras format: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m         )\n\u001b[0;32m--> 204\u001b[0m     \u001b[39mreturn\u001b[39;00m saving_lib\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m    205\u001b[0m         filepath,\n\u001b[1;32m    206\u001b[0m         custom_objects\u001b[39m=\u001b[39;49mcustom_objects,\n\u001b[1;32m    207\u001b[0m         \u001b[39mcompile\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mcompile\u001b[39;49m,\n\u001b[1;32m    208\u001b[0m         safe_mode\u001b[39m=\u001b[39;49msafe_mode,\n\u001b[1;32m    209\u001b[0m     )\n\u001b[1;32m    211\u001b[0m \u001b[39m# Legacy case.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[39mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[39m.\u001b[39mload_model(\n\u001b[1;32m    213\u001b[0m     filepath, custom_objects\u001b[39m=\u001b[39mcustom_objects, \u001b[39mcompile\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcompile\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    214\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/keras/saving/saving_lib.py:277\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    274\u001b[0m             asset_store\u001b[39m.\u001b[39mclose()\n\u001b[1;32m    276\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 277\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/keras/saving/saving_lib.py:242\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[39mwith\u001b[39;00m ObjectSharingScope():\n\u001b[0;32m--> 242\u001b[0m     model \u001b[39m=\u001b[39m deserialize_keras_object(\n\u001b[1;32m    243\u001b[0m         config_dict, custom_objects, safe_mode\u001b[39m=\u001b[39;49msafe_mode\n\u001b[1;32m    244\u001b[0m     )\n\u001b[1;32m    246\u001b[0m all_filenames \u001b[39m=\u001b[39m zf\u001b[39m.\u001b[39mnamelist()\n\u001b[1;32m    247\u001b[0m \u001b[39mif\u001b[39;00m _VARS_FNAME \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.h5\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m all_filenames:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/keras/saving/serialization_lib.py:482\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m         \u001b[39mreturn\u001b[39;00m obj\n\u001b[0;32m--> 482\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m _retrieve_class_or_fn(\n\u001b[1;32m    483\u001b[0m     class_name,\n\u001b[1;32m    484\u001b[0m     registered_name,\n\u001b[1;32m    485\u001b[0m     module,\n\u001b[1;32m    486\u001b[0m     obj_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mclass\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    487\u001b[0m     full_config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    488\u001b[0m     custom_objects\u001b[39m=\u001b[39;49mcustom_objects,\n\u001b[1;32m    489\u001b[0m )\n\u001b[1;32m    490\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfrom_config\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    491\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    492\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to reconstruct an instance of \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mclass_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m because \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    493\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthe class is missing a `from_config()` method. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    494\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFull object config: \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    495\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/keras/saving/serialization_lib.py:543\u001b[0m, in \u001b[0;36m_retrieve_class_or_fn\u001b[0;34m(name, registered_name, module, obj_type, full_config, custom_objects)\u001b[0m\n\u001b[1;32m    541\u001b[0m     mod \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(module)\n\u001b[1;32m    542\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m:\n\u001b[0;32m--> 543\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    544\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not deserialize \u001b[39m\u001b[39m{\u001b[39;00mobj_type\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m because \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mits parent module \u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m}\u001b[39;00m\u001b[39m cannot be imported. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFull object config: \u001b[39m\u001b[39m{\u001b[39;00mfull_config\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n\u001b[1;32m    548\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mvars\u001b[39m(mod)\u001b[39m.\u001b[39mget(name, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    549\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not deserialize class 'Functional' because its parent module keras.src.engine.functional cannot be imported. Full object config: {'module': 'keras.src.engine.functional', 'class_name': 'Functional', 'config': {'name': 'model', 'trainable': True, 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_input_shape': [None, 9, 9, 4], 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'input_1'}, 'registered_name': None, 'name': 'input_1', 'inbound_nodes': []}, {'module': 'keras.layers', 'class_name': 'Reshape', 'config': {'name': 'reshape', 'trainable': True, 'dtype': 'float32', 'target_shape': [9, 36]}, 'registered_name': None, 'build_config': {'input_shape': [None, 9, 9, 4]}, 'name': 'reshape', 'inbound_nodes': [[['input_1', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense', 'trainable': True, 'dtype': 'float32', 'units': 256, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 9, 36]}, 'name': 'dense', 'inbound_nodes': [[['reshape', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_1', 'trainable': True, 'dtype': 'float32', 'units': 256, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 9, 36]}, 'name': 'dense_1', 'inbound_nodes': [[['reshape', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 256, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 9, 36]}, 'name': 'dense_2', 'inbound_nodes': [[['reshape', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Add', 'config': {'name': 'add', 'trainable': True, 'dtype': 'float32'}, 'registered_name': None, 'build_config': {'input_shape': [[None, 9, 256], [None, 9, 256], [None, 9, 256]]}, 'name': 'add', 'inbound_nodes': [[['dense', 0, 0, {}], ['dense_1', 0, 0, {}], ['dense_2', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization', 'trainable': True, 'dtype': 'float32', 'axis': [2], 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 9, 256]}, 'name': 'layer_normalization', 'inbound_nodes': [[['add', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Flatten', 'config': {'name': 'flatten', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last'}, 'registered_name': None, 'build_config': {'input_shape': [None, 9, 256]}, 'name': 'flatten', 'inbound_nodes': [[['layer_normalization', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_3', 'trainable': True, 'dtype': 'float32', 'units': 2048, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': {'module': 'keras.regularizers', 'class_name': 'L2', 'config': {'l2': 0.009999999776482582}, 'registered_name': None}, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2304]}, 'name': 'dense_3', 'inbound_nodes': [[['flatten', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'AlphaDropout', 'config': {'name': 'alpha_dropout', 'trainable': True, 'dtype': 'float32', 'rate': 0.2, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2048]}, 'name': 'alpha_dropout', 'inbound_nodes': [[['dense_3', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_4', 'trainable': True, 'dtype': 'float32', 'units': 2048, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': {'module': 'keras.regularizers', 'class_name': 'L2', 'config': {'l2': 0.009999999776482582}, 'registered_name': None}, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2048]}, 'name': 'dense_4', 'inbound_nodes': [[['alpha_dropout', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_5', 'trainable': True, 'dtype': 'float32', 'units': 2048, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': {'module': 'keras.regularizers', 'class_name': 'L2', 'config': {'l2': 0.009999999776482582}, 'registered_name': None}, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2048]}, 'name': 'dense_5', 'inbound_nodes': [[['dense_4', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_6', 'trainable': True, 'dtype': 'float32', 'units': 2048, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': {'module': 'keras.regularizers', 'class_name': 'L2', 'config': {'l2': 0.009999999776482582}, 'registered_name': None}, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2048]}, 'name': 'dense_6', 'inbound_nodes': [[['dense_4', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_7', 'trainable': True, 'dtype': 'float32', 'units': 2048, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': {'module': 'keras.regularizers', 'class_name': 'L2', 'config': {'l2': 0.009999999776482582}, 'registered_name': None}, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2048]}, 'name': 'dense_7', 'inbound_nodes': [[['dense_4', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Add', 'config': {'name': 'add_1', 'trainable': True, 'dtype': 'float32'}, 'registered_name': None, 'build_config': {'input_shape': [[None, 2048], [None, 2048], [None, 2048]]}, 'name': 'add_1', 'inbound_nodes': [[['dense_5', 0, 0, {}], ['dense_6', 0, 0, {}], ['dense_7', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'LayerNormalization', 'config': {'name': 'layer_normalization_1', 'trainable': True, 'dtype': 'float32', 'axis': [1], 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2048]}, 'name': 'layer_normalization_1', 'inbound_nodes': [[['add_1', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_8', 'trainable': True, 'dtype': 'float32', 'units': 2048, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': {'module': 'keras.regularizers', 'class_name': 'L2', 'config': {'l2': 0.009999999776482582}, 'registered_name': None}, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2048]}, 'name': 'dense_8', 'inbound_nodes': [[['layer_normalization_1', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'AlphaDropout', 'config': {'name': 'alpha_dropout_1', 'trainable': True, 'dtype': 'float32', 'rate': 0.3, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2048]}, 'name': 'alpha_dropout_1', 'inbound_nodes': [[['dense_8', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_9', 'trainable': True, 'dtype': 'float32', 'units': 2048, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': {'module': 'keras.regularizers', 'class_name': 'L2', 'config': {'l2': 0.009999999776482582}, 'registered_name': None}, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2048]}, 'name': 'dense_9', 'inbound_nodes': [[['alpha_dropout_1', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'AlphaDropout', 'config': {'name': 'alpha_dropout_2', 'trainable': True, 'dtype': 'float32', 'rate': 0.2, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2048]}, 'name': 'alpha_dropout_2', 'inbound_nodes': [[['dense_9', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_10', 'trainable': True, 'dtype': 'float32', 'units': 1024, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 2048]}, 'name': 'dense_10', 'inbound_nodes': [[['alpha_dropout_2', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_12', 'trainable': True, 'dtype': 'float32', 'units': 512, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 1024]}, 'name': 'dense_12', 'inbound_nodes': [[['dense_10', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_14', 'trainable': True, 'dtype': 'float32', 'units': 512, 'activation': 'selu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 1024]}, 'name': 'dense_14', 'inbound_nodes': [[['dense_10', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_11', 'trainable': True, 'dtype': 'float32', 'units': 81, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 512]}, 'name': 'dense_11', 'inbound_nodes': [[['dense_12', 0, 0, {}]]]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_13', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 512]}, 'name': 'dense_13', 'inbound_nodes': [[['dense_14', 0, 0, {}]]]}], 'input_layers': [['input_1', 0, 0]], 'output_layers': [['dense_11', 0, 0], ['dense_13', 0, 0]]}, 'registered_name': 'Functional', 'build_config': {'input_shape': [None, 9, 9, 4]}, 'compile_config': {'optimizer': {'module': 'keras.optimizers', 'class_name': 'Adam', 'config': {'name': 'Adam', 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'jit_compile': True, 'is_legacy_optimizer': False, 'learning_rate': 4.999999873689376e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': None}, 'loss': None, 'metrics': None, 'loss_weights': None, 'weighted_metrics': None, 'run_eagerly': None, 'steps_per_execution': None, 'jit_compile': None}}"
     ]
    }
   ],
   "source": [
    "env = UltimateTicTacToeEnv(\n",
    "    max_timesteps=MAX_TIMESTEPS,\n",
    "    opponent= AIOpponent(\"attenppo8_p2\"), # WinningRandomOpponent(), # AIOpponent(\"attenppo8\"),\n",
    "    player1=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer related hyperparameters\n",
    "gamma = config[\"BUFFER\"].getfloat(\"GAMMA\")\n",
    "lam = config[\"BUFFER\"].getfloat(\"LAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "def special_discounted_cumulative_sums(x, discount, invalid_locats):\n",
    "    \"\"\"\n",
    "    invalid_locats: np.ndarray[bool], True if invalid, False if valid\n",
    "    \"\"\"\n",
    "    # discounts, but doesn't stack INVALID_PENALTY\n",
    "    # where it was invalid, use 0, else use the positive x\n",
    "    zeros = np.where(invalid_locats, 0, x)\n",
    "    # filter this\n",
    "    filtered = scipy.signal.lfilter([1], [1, float(-discount)], zeros[::-1], axis=0)[\n",
    "        ::-1\n",
    "    ]\n",
    "    # replace so that you have the invalid penalty where it was invalid\n",
    "    # and the gamma'd reward on valid moves\n",
    "    return np.where(invalid_locats, x, filtered)\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    # Buffer for storing trajectories\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        # Buffer initialization\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, *observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.valid_action_buffer = np.zeros(\n",
    "            (size, UltimateTicTacToeEnv.n_actions), dtype=np.bool_\n",
    "        )\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, action, reward, value, logprobability, valid_actions):\n",
    "        # Append one step of agent-environment interaction\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.logprobability_buffer[self.pointer] = logprobability\n",
    "        self.valid_action_buffer[self.pointer] = valid_actions\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0, use_special: bool = False):\n",
    "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        if use_special:\n",
    "            self.advantage_buffer[path_slice] = special_discounted_cumulative_sums(\n",
    "                deltas, self.gamma * self.lam, rewards[:-1] == INVALID_PENALTY\n",
    "            )\n",
    "            self.return_buffer[path_slice] = special_discounted_cumulative_sums(\n",
    "                rewards, self.gamma, rewards == INVALID_PENALTY\n",
    "            )[:-1]\n",
    "        else:\n",
    "            self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "                deltas, self.gamma * self.lam\n",
    "            )\n",
    "            self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
    "                rewards, self.gamma\n",
    "            )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        # Get all data of the buffer and normalize the advantages\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.logprobability_buffer,\n",
    "            self.valid_action_buffer,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss related hyperparameters\n",
    "clip_ratio = config[\"LOSS\"].getfloat(\"CLIP_RATIO\")\n",
    "target_kl = config[\"LOSS\"].getfloat(\"TARGET_KL\")\n",
    "clip_coef = config[\"LOSS\"].getfloat(\"CLIP_COEF\")\n",
    "v_coef = config[\"LOSS\"].getfloat(\"VALUE_COEFFICIENT\")\n",
    "entropy_coef = config[\"LOSS\"].getfloat(\"ENTROPY_COEFFICIENT\")\n",
    "invalid_coef = config[\"LOSS\"].getfloat(\"INVALID_COEFFICIENT\")\n",
    "reg_coef = config[\"LOSS\"].getfloat(\"REGULARIZER_COEFFICIENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def logprobabilities(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    logprobabilities_all = tf.nn.log_softmax(logits)\n",
    "    logprobability = tf.reduce_sum(\n",
    "        tf.one_hot(a, UltimateTicTacToeEnv.n_actions) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    return logprobability\n",
    "\n",
    "\n",
    "@tf.function(reduce_retracing=True)\n",
    "def vector_slice(A: tf.Tensor, B: tf.Tensor):\n",
    "    \"\"\"Returns values of rows i of A at column B[i]\n",
    "\n",
    "    where A is a 2D Tensor with shape [None, D]\n",
    "    and B is a 1D Tensor with shape [None]\n",
    "    with type int32 elements in [0,D)\n",
    "\n",
    "    Example:\n",
    "      A =[[1,2], B = [0,1], vector_slice(A,B) -> [1,4]\n",
    "          [3,4]]\n",
    "\n",
    "    Credit:\n",
    "        https://stackoverflow.com/questions/38492608/tensorflow-indexing-into-2d-tensor-with-1d-tensor\n",
    "    \"\"\"\n",
    "    linear_index = tf.shape(A)[1] * tf.range(0, tf.shape(A)[0])\n",
    "    linear_A = tf.reshape(A, [-1])\n",
    "    return tf.gather(linear_A, B + linear_index)\n",
    "\n",
    "\n",
    "kl_loss = tf.keras.losses.KLDivergence()\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_mod_on_probs(\n",
    "    observation_buffer, desired_probs, return_buffer, model: keras.Model\n",
    "):\n",
    "    \"\"\"\n",
    "    Pretraining the model just training it to imitate\n",
    "    valid moves (mse)\n",
    "    Arguments:\n",
    "        * observation_buffer\n",
    "        * desired_probs\n",
    "        * return_buffer\n",
    "        * model\n",
    "    Returns:\n",
    "        total loss\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, values = model(observation_buffer, training=True)\n",
    "\n",
    "        actor_loss = kl_loss(desired_probs, tf.nn.softmax(logits))\n",
    "\n",
    "        regularizer_loss = tf.reduce_sum(model.losses)\n",
    "        # regularizer_loss = 0\n",
    "\n",
    "        # critic loss = MSE\n",
    "        critic_loss = tf.keras.losses.mse(tf.squeeze(return_buffer), tf.squeeze(values))\n",
    "\n",
    "        loss = actor_loss + critic_loss + regularizer_loss\n",
    "\n",
    "    policy_grads = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(policy_grads, model.trainable_variables))\n",
    "    return actor_loss, critic_loss, regularizer_loss, loss\n",
    "\n",
    "\n",
    "# Train the policy by maxizing the PPO-Clip objective\n",
    "@tf.function\n",
    "def train_mod(\n",
    "    observation_buffer,\n",
    "    action_buffer,\n",
    "    logprobability_buffer,\n",
    "    advantage_buffer,\n",
    "    return_buffer,\n",
    "    valid_moves_buffer,\n",
    "    model: keras.Model,\n",
    "):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, values = model(observation_buffer, training=True)\n",
    "        softmax = tf.nn.softmax(logits)\n",
    "\n",
    "        new_probs = logprobabilities(logits, action_buffer)\n",
    "        # ratio = E(new_probs / old_probs)\n",
    "        # this subtraction method is a way to do this\n",
    "        ratio = tf.exp(new_probs - logprobability_buffer)\n",
    "\n",
    "        # L_clip = E_t * ( min( r_t*A_t, clip(r_t, 1-e, 1+e)*A_t ) )\n",
    "        clip_loss = -tf.reduce_mean(\n",
    "            tf.minimum(\n",
    "                ratio * advantage_buffer,\n",
    "                tf.clip_by_value(ratio, 1 - clip_ratio, 1 + clip_ratio)\n",
    "                * advantage_buffer,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        regularizer_loss = tf.reduce_sum(model.losses)\n",
    "\n",
    "        # critic loss = MSE\n",
    "        critic_loss = tf.keras.losses.mse(return_buffer, tf.squeeze(values))\n",
    "\n",
    "        # entropy loss to encourage exploration\n",
    "        entropy_loss = -tf.reduce_mean(-new_probs)\n",
    "\n",
    "        # penalize large changes\n",
    "        # kl_loss = tf.reduce_mean(logprobability_buffer - new_probs)\n",
    "        # kl_loss = tf.reduce_sum(kl_loss)\n",
    "\n",
    "        # penalize invalids\n",
    "        invalid_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum(\n",
    "                tf.where(\n",
    "                    valid_moves_buffer,\n",
    "                    tf.constant(0, dtype=tf.float32),  # if it was valid, don't penalize\n",
    "                    softmax,  # else, penalize w/ the probability\n",
    "                ),\n",
    "                axis=-1,\n",
    "            )\n",
    "        )\n",
    "        # invalid_loss = tf.reduce_mean(tf.where(return_buffer == INVALID_PENALTY, vector_slice(softmax, action_buffer), tf.constant(0, dtype=tf.float32)))\n",
    "\n",
    "        # full loss\n",
    "        loss = (\n",
    "            clip_loss * clip_coef\n",
    "            + critic_loss * v_coef\n",
    "            + entropy_loss * entropy_coef\n",
    "            + regularizer_loss * reg_coef\n",
    "            + invalid_loss * invalid_coef\n",
    "        )\n",
    "\n",
    "    # apply gradients\n",
    "    policy_grads = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(policy_grads, model.trainable_variables))\n",
    "\n",
    "    # check new kl divergence\n",
    "    logits, _ = model(observation_buffer, training=False)\n",
    "    kl = tf.reduce_mean(logprobability_buffer - logprobabilities(logits, action_buffer))\n",
    "    kl = tf.reduce_sum(kl)\n",
    "\n",
    "    return kl, clip_loss, critic_loss, invalid_loss, entropy_loss, regularizer_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"attenppo8\"\n",
    "load_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 21:23:01.618953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 21:23:01.619465: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 21:23:01.619641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 21:23:01.708813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 21:23:01.709056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 21:23:01.709228: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-17 21:23:01.709355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10101 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:06:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "def create_shared_layers(input_layer):\n",
    "    # imitation attention 1:\n",
    "    x1 = layers.Dense(32, activation=\"selu\", use_bias=True)(input_layer)\n",
    "    x2 = layers.Dense(32, activation=\"selu\", use_bias=True)(input_layer)\n",
    "    x3 = layers.Dense(32, activation=\"selu\", use_bias=True)(input_layer)\n",
    "    x = layers.Add()([x1, x2, x3])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "\n",
    "    # feedforward 1:\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(2592, activation=\"selu\", activity_regularizer=\"l2\")(x)\n",
    "    x = layers.AlphaDropout(0.2)(x)\n",
    "    x = layers.Dense(2592, activation=\"selu\", activity_regularizer=\"l2\")(x)\n",
    "\n",
    "    # imitation attention 2\n",
    "    x1 = layers.Dense(3000, activation=\"selu\", use_bias=True, activity_regularizer=\"l2\")(x)\n",
    "    x2 = layers.Dense(3000, activation=\"selu\", use_bias=True, activity_regularizer=\"l2\")(x)\n",
    "    x3 = layers.Dense(3000, activation=\"selu\", use_bias=True, activity_regularizer=\"l2\")(x)\n",
    "    x = layers.Add()([x1, x2, x3])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "\n",
    "    # feedforward 2\n",
    "    x = layers.Dense(2048, activation=\"selu\", activity_regularizer=\"l2\")(x)\n",
    "    x = layers.AlphaDropout(0.2)(x)\n",
    "    x = layers.Dense(1024, activation=\"selu\")(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    # model inputs\n",
    "    inputs = tf.keras.Input(shape=UltimateTicTacToeEnv.obs_dim)\n",
    "    x = create_shared_layers(inputs)\n",
    "    logits = layers.Dense(UltimateTicTacToeEnv.n_actions)(\n",
    "        layers.Dense(512, activation=\"selu\")(x)\n",
    "    )\n",
    "    values = tf.keras.layers.Dense(1)(layers.Dense(512, activation=\"selu\")(x))\n",
    "    return tf.keras.Model(inputs=inputs, outputs=(logits, values))\n",
    "\n",
    "\n",
    "if load_model and os.path.exists(f\"models/{model_name}.keras\"):\n",
    "    print(\"loading model...\")\n",
    "    model = tf.keras.models.load_model(f\"models/{model_name}.keras\")\n",
    "else:\n",
    "    print(\"creating model...\")\n",
    "    model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 9, 9, 4)]    0           []                               \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 9, 36)        0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 9, 256)       9472        ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 9, 256)       9472        ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 9, 256)       9472        ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 9, 256)       0           ['dense[0][0]',                  \n",
      "                                                                  'dense_1[0][0]',                \n",
      "                                                                  'dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 9, 256)      512         ['add[0][0]']                    \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2304)         0           ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 2048)         4720640     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " alpha_dropout (AlphaDropout)   (None, 2048)         0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 2048)         4196352     ['alpha_dropout[0][0]']          \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 2048)         4196352     ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 2048)         4196352     ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 2048)         4196352     ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 2048)         0           ['dense_5[0][0]',                \n",
      "                                                                  'dense_6[0][0]',                \n",
      "                                                                  'dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 2048)        4096        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 2048)         4196352     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " alpha_dropout_1 (AlphaDropout)  (None, 2048)        0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 2048)         4196352     ['alpha_dropout_1[0][0]']        \n",
      "                                                                                                  \n",
      " alpha_dropout_2 (AlphaDropout)  (None, 2048)        0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 1024)         2098176     ['alpha_dropout_2[0][0]']        \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 512)          524800      ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 512)          524800      ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 81)           41553       ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 1)            513         ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 33,121,618\n",
      "Trainable params: 33,121,618\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate hyperparams\n",
    "learning_rate = config[\"OPTIMIZER\"].getfloat(\"LEARNING_RATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model and os.path.exists(f\"models/{model_name}.keras\"):\n",
    "    optim: tf.keras.optimizers.Adam = model.optimizer\n",
    "    optim.learning_rate = learning_rate\n",
    "else:\n",
    "    optim = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=5e-06>\n"
     ]
    }
   ],
   "source": [
    "print(optim.learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training time hyperparameters\n",
    "train_iterations = config[\"TRAIN\"].getint(\"TRAIN_ITERATIONS\")\n",
    "epochs = config[\"TRAIN\"].getint(\"EPOCHS\")\n",
    "minibatch_size = config[\"TRAIN\"].getint(\"MINIBATCH_SIZE\")\n",
    "steps_per_epoch = config[\"TRAIN\"].getint(\"STEPS_PER_EPOCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = config[\"TRAIN\"].getint(\"EPOCH\")\n",
    "summary_writer = tf.summary.create_file_writer(f\"./logs/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = Buffer(UltimateTicTacToeEnv.obs_dim, steps_per_epoch, gamma=gamma, lam=lam)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain = False\n",
    "load_timesteps = True\n",
    "pretrain_timesteps = 600000\n",
    "pretrain_batch_size = 10000\n",
    "\n",
    "if pretrain:\n",
    "    if load_timesteps:\n",
    "        pretrain_observations = np.load(\"pretrain_observations.npy\")\n",
    "        pretrain_desired_probs = np.load(\"pretrain_desired_probs.npy\")\n",
    "        pretrain_rewards = np.load(\"pretrain_rewards.npy\")\n",
    "    else:\n",
    "        pretrain_observations = np.zeros((pretrain_timesteps, *env.obs_dim))\n",
    "        pretrain_desired_probs = np.zeros((pretrain_timesteps, env.n_actions))\n",
    "        pretrain_rewards = np.zeros((pretrain_timesteps, 1))\n",
    "else:\n",
    "    pretrain_observations = np.zeros((steps_per_epoch, *env.obs_dim))\n",
    "    pretrain_desired_probs = np.zeros((steps_per_epoch, env.n_actions))\n",
    "    pretrain_rewards = np.zeros((steps_per_epoch, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.utils as utils\n",
    "\n",
    "\n",
    "def collect_pretrain_trajectories():\n",
    "    def preprocess_obs_action_use_map():\n",
    "        global pretrain_observations, pretrain_desired_probs\n",
    "        # initialize obs:probs map and calculate probs\n",
    "        observations_probs_map = {}\n",
    "        for hashable_obs, action_use_map in observations_actions_uses.items():\n",
    "            # initialize the probability matrix\n",
    "            observations_probs_map[hashable_obs] = np.zeros((env.n_actions))\n",
    "\n",
    "            # calculate the probability of each action\n",
    "            total_uses = sum(action_use_map.values())\n",
    "            for action, uses in action_use_map.items():\n",
    "                observations_probs_map[hashable_obs][action] = uses / total_uses\n",
    "\n",
    "            assert round(sum(observations_probs_map[hashable_obs]), 4) == 1.0\n",
    "\n",
    "        # put probs into pretrain_desired_probs\n",
    "        for i in range(pretrain_timesteps):\n",
    "            pretrain_desired_probs[i] = observations_probs_map[\n",
    "                pretrain_observations[i].tobytes()\n",
    "            ]\n",
    "\n",
    "    t = 0\n",
    "    num_valid = 0\n",
    "    start_time = datetime.now()\n",
    "    opponent = WinningRandomOpponent(env.n_actions)\n",
    "\n",
    "    # map observations -> map of actions -> uses\n",
    "    observations_actions_uses = {}\n",
    "    while t < pretrain_timesteps:\n",
    "        # Iterate over the steps of each epoch\n",
    "        observation = env.reset()\n",
    "        start_timestep = t\n",
    "        while not env.done and t < pretrain_timesteps:\n",
    "            # update the map w/ the observation\n",
    "            hashable = observation.tobytes()\n",
    "            if hashable not in observations_actions_uses:\n",
    "                observations_actions_uses[hashable] = {}\n",
    "\n",
    "            # get the action\n",
    "            action = opponent.get_action(observation)\n",
    "\n",
    "            # step\n",
    "            observation_new, reward, done, valid = env.step(action)\n",
    "            if valid:\n",
    "                num_valid += 1\n",
    "            else:\n",
    "                print(\"action was\", action)\n",
    "                print(observation)\n",
    "                raise Exception(\"failed\")\n",
    "\n",
    "            # update the pretrain buffers and the map\n",
    "            pretrain_observations[t] = observation\n",
    "            pretrain_rewards[t] = reward\n",
    "            # update the map w/ the action\n",
    "            if action not in observations_actions_uses[hashable]:\n",
    "                observations_actions_uses[hashable][action] = 1\n",
    "            else:\n",
    "                observations_actions_uses[hashable][action] += 1\n",
    "\n",
    "            # Update the observation\n",
    "            observation = observation_new\n",
    "\n",
    "            # Finish trajectory if reached to a terminal state\n",
    "            t += 1\n",
    "            if done:\n",
    "                observation = env.reset()\n",
    "\n",
    "                # do the discounting\n",
    "                pretrain_rewards[start_timestep:t] = discounted_cumulative_sums(\n",
    "                    pretrain_rewards[start_timestep:t], gamma\n",
    "                )\n",
    "                start_timestep = t\n",
    "\n",
    "            # log\n",
    "            print(\n",
    "                f\"Step {t} / {pretrain_timesteps};\\t\"\n",
    "                + f\"% valid = {num_valid / (t):.4f};\\t\"\n",
    "                + f\"fps: {(t)/(datetime.now()-start_time).total_seconds():.2f};\\t\",\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "    print()\n",
    "\n",
    "    preprocess_obs_action_use_map()\n",
    "\n",
    "    np.save(\"pretrain_observations\", pretrain_observations)\n",
    "    np.save(\"pretrain_desired_probs\", pretrain_desired_probs)\n",
    "    np.save(\"pretrain_rewards\", pretrain_rewards)\n",
    "\n",
    "\n",
    "def pretrain_model(\n",
    "    model: keras.Model, num_epochs=15, collect_trajectories: bool = True\n",
    "):\n",
    "    global pretrain_rewards, pretrain_desired_probs, pretrain_observations\n",
    "    if collect_trajectories:\n",
    "        collect_pretrain_trajectories()\n",
    "\n",
    "    for epoch in range(1, 1 + num_epochs):\n",
    "        pretrain_observations, pretrain_desired_probs, pretrain_rewards = utils.shuffle(\n",
    "            pretrain_observations, pretrain_desired_probs, pretrain_rewards\n",
    "        )\n",
    "        actor_loss, critic_loss, regularizer_loss, loss = 0, 0, 0, 0\n",
    "        # do minibatches\n",
    "        for batch in range(pretrain_timesteps // pretrain_batch_size):\n",
    "            start_idx = batch * pretrain_batch_size\n",
    "            end_idx = (batch + 1) * pretrain_batch_size\n",
    "            _actor_loss, _critic_loss, _regularizer_loss, _loss = train_mod_on_probs(\n",
    "                pretrain_observations[start_idx:end_idx],\n",
    "                pretrain_desired_probs[start_idx:end_idx],\n",
    "                pretrain_rewards[start_idx:end_idx],\n",
    "                model,\n",
    "            )\n",
    "            actor_loss += _actor_loss\n",
    "            critic_loss += _critic_loss\n",
    "            regularizer_loss += _regularizer_loss\n",
    "            loss += _loss\n",
    "\n",
    "        actor_loss /= batch + 1\n",
    "        critic_loss /= batch + 1\n",
    "        regularizer_loss /= batch + 1\n",
    "        loss /= batch + 1\n",
    "        # write summaries\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar(\"pretrain/actor loss\", actor_loss, step=epoch)\n",
    "            tf.summary.scalar(\"pretrain/critic loss\", critic_loss, step=epoch)\n",
    "            tf.summary.scalar(\"pretrain/regularizer loss\", regularizer_loss, step=epoch)\n",
    "            tf.summary.scalar(\"pretrain/loss\", loss, step=epoch)\n",
    "\n",
    "        print(\n",
    "            f\"epoch {epoch}: actor loss: {actor_loss}; critic loss: {critic_loss}; regularizer_loss: {regularizer_loss}, total loss {loss}\"\n",
    "        )\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 1., 1.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on real observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training function\n",
    "def train_model(\n",
    "    model: keras.Model,\n",
    "    epoch_start: int,\n",
    "    use_kl: bool = True,\n",
    "    use_adaptive_kl: bool = True,\n",
    "    shuffle: bool = True,\n",
    "    use_special: bool = True,\n",
    "    just_train_critic: bool = False,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Train the model for `epochs` epochs\n",
    "    Arguments:\n",
    "        * model: keras.Model - the model to train\n",
    "        * epoch_start: int - the epoch to start logging at\n",
    "        * use_kl: bool - whether or not to stop early if kl divergence > target_kl\n",
    "        * use_adaptive_kl: bool - whether or not to adapt kl (increase it if divergence > target_kl, decrease it otherwise)\n",
    "        * shuffle: bool - whether or not to shuffle the observations\n",
    "        * use_special: bool -whether or not to use special version of discounted sums\n",
    "        * just_train_critic: bool - whether or not to just train the critic; defaults to False\n",
    "    \"\"\"\n",
    "    global target_kl, pretrain_desired_probs, WIN_REWARD\n",
    "    # Iterate over the number of epochs\n",
    "    for epoch in range(epoch_start, epoch_start + epochs):\n",
    "        # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "        sum_return = 0\n",
    "        num_episodes = 0\n",
    "        episode_return = 0\n",
    "        episode_length = 0\n",
    "        lengths = []\n",
    "\n",
    "        # logging variables\n",
    "        num_valid = 0\n",
    "        num_wins = 0\n",
    "        num_losses = 0\n",
    "        num_ties = 0\n",
    "\n",
    "        # Iterate over the steps of each epoch\n",
    "        observation = env.reset()\n",
    "        start_time = datetime.now()\n",
    "        for t in range(steps_per_epoch):\n",
    "            valid_actions = env.validmoves\n",
    "            valid_actions = np.array(\n",
    "                [True if i in valid_actions else False for i in range(env.n_actions)]\n",
    "            )\n",
    "\n",
    "            # Get the logits, action, and take one step in the environment\n",
    "            observation = observation.reshape(1, *env.obs_dim)\n",
    "\n",
    "            logits, action, value_t = sample_action_value(observation, model)\n",
    "            # logits, action, value_t = best_action_value(observation, model)\n",
    "\n",
    "            observation_new, reward, done, valid = env.step(action[0].numpy())\n",
    "            episode_return += reward\n",
    "            episode_length += 1\n",
    "\n",
    "            # logging variables\n",
    "            num_valid += 1 if valid else 0\n",
    "            num_wins += 1 if env.won else 0\n",
    "            num_losses += 1 if env.lost else 0\n",
    "            num_ties += 1 if env.tied else 0\n",
    "\n",
    "            # Get the value and log-probability of the action\n",
    "            logprobability_t = logprobabilities(logits, action)\n",
    "\n",
    "            if just_train_critic:\n",
    "                pretrain_desired_probs[t] = tf.nn.softmax(logits)\n",
    "\n",
    "            # Store obs, act, rew, v_t, logp_pi_t\n",
    "            buffer.store(\n",
    "                observation, action, reward, value_t, logprobability_t, valid_actions\n",
    "            )\n",
    "\n",
    "            # Update the observation\n",
    "            observation = observation_new\n",
    "\n",
    "            # Finish trajectory if reached to a terminal state\n",
    "            terminal = done\n",
    "            if terminal or t == steps_per_epoch - 1:\n",
    "                last_value = (\n",
    "                    0\n",
    "                    if done\n",
    "                    else model(observation.reshape(1, *env.obs_dim), training=False)[1]\n",
    "                )\n",
    "                buffer.finish_trajectory(last_value, use_special=use_special)\n",
    "                sum_return += episode_return\n",
    "                lengths.append(episode_length)\n",
    "                num_episodes += 1\n",
    "                observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "            # log\n",
    "            print(\n",
    "                f\"Step {t+1} / {steps_per_epoch};\\t\"\n",
    "                + f\"% valid = {num_valid / (t+1):.4f};\\t\"\n",
    "                + f\"fps: {(t+1)/(datetime.now()-start_time).total_seconds():.2f};\\t\"\n",
    "                + f\"win rate: {(num_wins/num_episodes if num_episodes > 0 else 0):.2f}\\t\"\n",
    "                + f\"tie rate: {(num_ties/num_episodes if num_episodes > 0 else 0):.2f}\\t\"\n",
    "                + f\"loss rate: {(num_losses/num_episodes if num_episodes > 0 else 0):.2f}\",\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "        print()\n",
    "\n",
    "        (\n",
    "            observation_buffer,\n",
    "            action_buffer,\n",
    "            advantage_buffer,\n",
    "            return_buffer,\n",
    "            logprobability_buffer,\n",
    "            valid_action_buffer,\n",
    "        ) = buffer.get()\n",
    "\n",
    "        if just_train_critic:\n",
    "            for it in range(train_iterations):\n",
    "                train_mod_on_probs(\n",
    "                    observation_buffer,\n",
    "                    pretrain_desired_probs[:steps_per_epoch],\n",
    "                    return_buffer,\n",
    "                    model,\n",
    "                )\n",
    "            continue\n",
    "\n",
    "        # Update the policy and implement early stopping using KL divergence\n",
    "        kl = 0\n",
    "        clip_loss = 0\n",
    "        critic_loss = 0\n",
    "        invalid_loss = 0\n",
    "        entropy_loss = 0\n",
    "        regularizer_loss = 0\n",
    "        for it in range(train_iterations):\n",
    "            if shuffle:\n",
    "                (\n",
    "                    _observation_buffer,\n",
    "                    _action_buffer,\n",
    "                    _advantage_buffer,\n",
    "                    _return_buffer,\n",
    "                    _logprobability_buffer,\n",
    "                    _valid_action_buffer,\n",
    "                ) = utils.shuffle(\n",
    "                    observation_buffer,\n",
    "                    action_buffer,\n",
    "                    advantage_buffer,\n",
    "                    return_buffer,\n",
    "                    logprobability_buffer,\n",
    "                    valid_action_buffer,\n",
    "                )\n",
    "            else:\n",
    "                (\n",
    "                    _observation_buffer,\n",
    "                    _action_buffer,\n",
    "                    _advantage_buffer,\n",
    "                    _return_buffer,\n",
    "                    _logprobability_buffer,\n",
    "                    _valid_action_buffer,\n",
    "                ) = (\n",
    "                    observation_buffer,\n",
    "                    action_buffer,\n",
    "                    advantage_buffer,\n",
    "                    return_buffer,\n",
    "                    logprobability_buffer,\n",
    "                    valid_action_buffer,\n",
    "                )\n",
    "            temp_clip_loss = 0\n",
    "            temp_critic_loss = 0\n",
    "            temp_invalid_loss = 0\n",
    "            temp_entropy_loss = 0\n",
    "            temp_regularizer_loss = 0\n",
    "            stopped_early = False\n",
    "            # do minibatches\n",
    "            for i in range(steps_per_epoch // minibatch_size):\n",
    "                # get the parts of the kl and losses\n",
    "                (\n",
    "                    kl,\n",
    "                    clip_loss_part,\n",
    "                    critic_loss_part,\n",
    "                    invalid_loss_part,\n",
    "                    entropy_loss_part,\n",
    "                    regularizer_loss_part,\n",
    "                ) = train_mod(\n",
    "                    tf.constant(\n",
    "                        _observation_buffer[\n",
    "                            i * minibatch_size : (i + 1) * minibatch_size\n",
    "                        ]\n",
    "                    ),  # obs\n",
    "                    tf.constant(\n",
    "                        _action_buffer[i * minibatch_size : (i + 1) * minibatch_size]\n",
    "                    ),  # act\n",
    "                    tf.constant(\n",
    "                        _logprobability_buffer[\n",
    "                            i * minibatch_size : (i + 1) * minibatch_size\n",
    "                        ]\n",
    "                    ),  # logprobs\n",
    "                    tf.constant(\n",
    "                        _advantage_buffer[i * minibatch_size : (i + 1) * minibatch_size]\n",
    "                    ),  # advantages\n",
    "                    tf.constant(\n",
    "                        _return_buffer[i * minibatch_size : (i + 1) * minibatch_size]\n",
    "                    ),  # returns\n",
    "                    tf.constant(\n",
    "                        _valid_action_buffer[\n",
    "                            i * minibatch_size : (i + 1) * minibatch_size\n",
    "                        ]\n",
    "                    ),  # valid actions\n",
    "                    model,\n",
    "                )\n",
    "                # update the temps\n",
    "                temp_clip_loss += clip_loss_part\n",
    "                temp_critic_loss += critic_loss_part\n",
    "                temp_invalid_loss += invalid_loss_part\n",
    "                temp_entropy_loss += entropy_loss_part\n",
    "                temp_regularizer_loss += regularizer_loss_part\n",
    "                if use_kl and kl > 1.5 * target_kl:\n",
    "                    stopped_early = True\n",
    "                    if use_adaptive_kl:\n",
    "                        target_kl *= 1.5\n",
    "                    break\n",
    "\n",
    "            # average the temps to get the amount per that pass\n",
    "            temp_clip_loss /= i + 1\n",
    "            temp_critic_loss /= i + 1\n",
    "            temp_invalid_loss /= i + 1\n",
    "            temp_entropy_loss /= i + 1\n",
    "            temp_regularizer_loss /= i + 1\n",
    "\n",
    "            # update the main counts\n",
    "            clip_loss += temp_clip_loss\n",
    "            critic_loss += temp_critic_loss\n",
    "            invalid_loss += temp_invalid_loss\n",
    "            entropy_loss += temp_entropy_loss\n",
    "            regularizer_loss += temp_regularizer_loss\n",
    "\n",
    "            if stopped_early:\n",
    "                # Early Stopping\n",
    "                break\n",
    "        else:\n",
    "            if use_kl and use_adaptive_kl:\n",
    "                target_kl /= 1.2\n",
    "\n",
    "        clip_loss /= it + 1\n",
    "        critic_loss /= it + 1\n",
    "        invalid_loss /= it + 1\n",
    "        entropy_loss /= it + 1\n",
    "        regularizer_loss /= it + 1\n",
    "\n",
    "        # Print mean return and length for each epoch\n",
    "        print(\n",
    "            f\"Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum(lengths) / num_episodes}. STD Length: {np.std(lengths)}\"\n",
    "        )\n",
    "        print(\"=\" * 64)\n",
    "\n",
    "        # log scalars\n",
    "        with summary_writer.as_default():\n",
    "            # episode info\n",
    "            tf.summary.scalar(\"episode/win rate\", num_wins / num_episodes, step=epoch)\n",
    "            tf.summary.scalar(\"episode/tie rate\", num_ties / num_episodes, step=epoch)\n",
    "            tf.summary.scalar(\n",
    "                \"episode/loss rate\", num_losses / num_episodes, step=epoch\n",
    "            )\n",
    "            tf.summary.scalar(\"episode/valid percentage\", num_valid / t, step=epoch)\n",
    "            tf.summary.scalar(\n",
    "                \"episode/mean reward\", sum_return / num_episodes, step=epoch\n",
    "            )\n",
    "            tf.summary.scalar(\n",
    "                \"episode/mean length\", sum(lengths) / num_episodes, step=epoch\n",
    "            )\n",
    "            tf.summary.scalar(\"episode/std length\", np.std(lengths), step=epoch)\n",
    "\n",
    "            # training info\n",
    "            tf.summary.scalar(\"train/clip_loss\", clip_loss, step=epoch)\n",
    "            tf.summary.scalar(\"train/critic_loss\", critic_loss, step=epoch)\n",
    "            tf.summary.scalar(\"train/invalid_loss\", invalid_loss, step=epoch)\n",
    "            tf.summary.scalar(\"train/entropy_loss\", entropy_loss, step=epoch)\n",
    "            tf.summary.scalar(\"train/regularizer_loss\", regularizer_loss, step=epoch)\n",
    "            tf.summary.scalar(\"train/kl\", kl, step=epoch)\n",
    "            tf.summary.scalar(\n",
    "                \"train/total_loss\",\n",
    "                clip_loss\n",
    "                + critic_loss * v_coef\n",
    "                + invalid_loss * invalid_coef\n",
    "                + entropy_loss * entropy_coef\n",
    "                + regularizer_loss * reg_coef,\n",
    "                step=epoch,\n",
    "            )\n",
    "            tf.summary.scalar(\"train/num_iterations\", it + 1, step=epoch)\n",
    "\n",
    "        # save every 5\n",
    "        if epoch % 5 == 0:\n",
    "            model.save(\"model_temp.keras\")\n",
    "\n",
    "        WIN_REWARD += 0.1 / 240\n",
    "\n",
    "    return epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 21:23:04.312953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-08-17 21:23:04.333480: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2560 / 2560;\t% valid = 0.9723;\tfps: 176.24;\twin rate: 0.39\ttie rate: 0.20\tloss rate: 0.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 21:23:20.467192: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7fca6cfd2750 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-17 21:23:20.467240: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2023-08-17 21:23:20.476333: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-08-17 21:23:20.632821: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2292. Mean Return: 0.03677083333333338. Mean Length: 26.666666666666668. STD Length: 5.3864542037307706\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9852;\tfps: 173.41;\twin rate: 0.41\ttie rate: 0.23\tloss rate: 0.33\n",
      "Epoch: 2293. Mean Return: 0.4299609375000002. Mean Length: 26.666666666666668. STD Length: 4.799450199993977\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9680;\tfps: 179.54;\twin rate: 0.38\ttie rate: 0.26\tloss rate: 0.30\n",
      "Epoch: 2294. Mean Return: -0.0720199275362318. Mean Length: 27.82608695652174. STD Length: 4.986086501569565\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9648;\tfps: 177.14;\twin rate: 0.33\ttie rate: 0.21\tloss rate: 0.40\n",
      "Epoch: 2295. Mean Return: -0.23911842105263176. Mean Length: 26.94736842105263. STD Length: 5.892668810171771\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9641;\tfps: 194.31;\twin rate: 0.46\ttie rate: 0.14\tloss rate: 0.34\n",
      "Epoch: 2296. Mean Return: -0.1560992907801418. Mean Length: 27.23404255319149. STD Length: 5.787781576101271\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9906;\tfps: 190.01;\twin rate: 0.41\ttie rate: 0.19\tloss rate: 0.38\n",
      "Epoch: 2297. Mean Return: 0.5760652920962198. Mean Length: 26.391752577319586. STD Length: 4.115739717634985\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9770;\tfps: 184.99;\twin rate: 0.44\ttie rate: 0.20\tloss rate: 0.32\n",
      "Epoch: 2298. Mean Return: 0.2232291666666665. Mean Length: 26.666666666666668. STD Length: 4.913134324327891\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9375;\tfps: 178.64;\twin rate: 0.33\ttie rate: 0.13\tloss rate: 0.45\n",
      "Epoch: 2299. Mean Return: -1.0996467391304345. Mean Length: 27.82608695652174. STD Length: 6.583260550114923\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9270;\tfps: 181.18;\twin rate: 0.31\ttie rate: 0.16\tloss rate: 0.42\n",
      "Epoch: 2300. Mean Return: -1.453863636363636. Mean Length: 29.09090909090909. STD Length: 7.102554539863494\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9512;\tfps: 195.46;\twin rate: 0.42\ttie rate: 0.16\tloss rate: 0.33\n",
      "Epoch: 2301. Mean Return: -0.554442934782609. Mean Length: 27.82608695652174. STD Length: 6.349612613401281\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9484;\tfps: 173.82;\twin rate: 0.40\ttie rate: 0.14\tloss rate: 0.40\n",
      "Epoch: 2302. Mean Return: -0.6362455197132618. Mean Length: 27.526881720430108. STD Length: 6.1828611497236965\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9328;\tfps: 190.47;\twin rate: 0.37\ttie rate: 0.20\tloss rate: 0.33\n",
      "Epoch: 2303. Mean Return: -1.181558988764045. Mean Length: 28.764044943820224. STD Length: 6.337018885685177\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9477;\tfps: 199.92;\twin rate: 0.35\ttie rate: 0.24\tloss rate: 0.33\n",
      "Epoch: 2304. Mean Return: -0.7135326086956522. Mean Length: 27.82608695652174. STD Length: 6.291140588596161\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9254;\tfps: 210.49;\twin rate: 0.44\ttie rate: 0.17\tloss rate: 0.28\n",
      "Epoch: 2305. Mean Return: -1.3310925925925932. Mean Length: 28.444444444444443. STD Length: 7.011278392087853\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9324;\tfps: 189.93;\twin rate: 0.45\ttie rate: 0.17\tloss rate: 0.27\n",
      "Epoch: 2306. Mean Return: -1.1478277153558052. Mean Length: 28.764044943820224. STD Length: 6.871243565237045\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9379;\tfps: 179.60;\twin rate: 0.44\ttie rate: 0.14\tloss rate: 0.32\n",
      "Epoch: 2307. Mean Return: -0.9311693548387099. Mean Length: 27.526881720430108. STD Length: 6.869899120965788\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9426;\tfps: 180.55;\twin rate: 0.37\ttie rate: 0.17\tloss rate: 0.37\n",
      "Epoch: 2308. Mean Return: -0.8309219858156026. Mean Length: 27.23404255319149. STD Length: 6.818974671653892\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9016;\tfps: 189.89;\twin rate: 0.35\ttie rate: 0.10\tloss rate: 0.39\n",
      "Epoch: 2309. Mean Return: -2.220232007575758. Mean Length: 29.09090909090909. STD Length: 7.815538665255208\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9332;\tfps: 179.72;\twin rate: 0.39\ttie rate: 0.19\tloss rate: 0.32\n",
      "Epoch: 2310. Mean Return: -1.114731182795699. Mean Length: 27.526881720430108. STD Length: 6.930672666463614\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9156;\tfps: 183.38;\twin rate: 0.29\ttie rate: 0.18\tloss rate: 0.39\n",
      "Epoch: 2311. Mean Return: -1.8537595785440613. Mean Length: 29.42528735632184. STD Length: 7.0639642152737245\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9367;\tfps: 197.75;\twin rate: 0.33\ttie rate: 0.10\tloss rate: 0.48\n",
      "Epoch: 2312. Mean Return: -1.0880496453900717. Mean Length: 27.23404255319149. STD Length: 6.74840490517179\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.8402;\tfps: 186.04;\twin rate: 0.42\ttie rate: 0.12\tloss rate: 0.25\n",
      "Epoch: 2313. Mean Return: -4.148. Mean Length: 30.11764705882353. STD Length: 9.136110338143178\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.8398;\tfps: 185.56;\twin rate: 0.42\ttie rate: 0.10\tloss rate: 0.24\n",
      "Epoch: 2314. Mean Return: -4.3058935742971896. Mean Length: 30.843373493975903. STD Length: 9.156578947244913\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.8406;\tfps: 179.29;\twin rate: 0.32\ttie rate: 0.13\tloss rate: 0.30\n",
      "Epoch: 2315. Mean Return: -4.430985772357722. Mean Length: 31.21951219512195. STD Length: 8.881364966292367\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.8180;\tfps: 183.54;\twin rate: 0.26\ttie rate: 0.07\tloss rate: 0.37\n",
      "Epoch: 2316. Mean Return: -5.294629629629632. Mean Length: 31.604938271604937. STD Length: 9.499420401221991\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.8270;\tfps: 184.83;\twin rate: 0.27\ttie rate: 0.16\tloss rate: 0.30\n",
      "Epoch: 2317. Mean Return: -4.92604674796748. Mean Length: 31.21951219512195. STD Length: 9.643681604242108\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.7906;\tfps: 178.95;\twin rate: 0.32\ttie rate: 0.09\tloss rate: 0.27\n",
      "Epoch: 2318. Mean Return: -6.30315400843882. Mean Length: 32.40506329113924. STD Length: 9.801212773500902\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.8660;\tfps: 178.23;\twin rate: 0.39\ttie rate: 0.16\tloss rate: 0.26\n",
      "Epoch: 2319. Mean Return: -3.353632352941177. Mean Length: 30.11764705882353. STD Length: 8.619283121147102\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.8629;\tfps: 195.79;\twin rate: 0.33\ttie rate: 0.11\tloss rate: 0.33\n",
      "Epoch: 2320. Mean Return: -3.585158730158731. Mean Length: 30.476190476190474. STD Length: 8.812161767202067\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9156;\tfps: 186.30;\twin rate: 0.32\ttie rate: 0.14\tloss rate: 0.42\n",
      "Epoch: 2321. Mean Return: -1.7274130036630042. Mean Length: 28.13186813186813. STD Length: 7.558139184922091\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9309;\tfps: 188.39;\twin rate: 0.26\ttie rate: 0.19\tloss rate: 0.46\n",
      "Epoch: 2322. Mean Return: -1.3555277777777779. Mean Length: 28.444444444444443. STD Length: 6.997001121291752\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9352;\tfps: 185.21;\twin rate: 0.46\ttie rate: 0.17\tloss rate: 0.26\n",
      "Epoch: 2323. Mean Return: -0.9956250000000009. Mean Length: 27.82608695652174. STD Length: 6.883516378390408\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9156;\tfps: 189.25;\twin rate: 0.36\ttie rate: 0.20\tloss rate: 0.32\n",
      "Epoch: 2324. Mean Return: -1.6680219780219796. Mean Length: 28.13186813186813. STD Length: 7.965873352516036\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9180;\tfps: 188.65;\twin rate: 0.31\ttie rate: 0.23\tloss rate: 0.34\n",
      "Epoch: 2325. Mean Return: -1.7000994318181823. Mean Length: 29.09090909090909. STD Length: 7.132888557488734\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9570;\tfps: 198.39;\twin rate: 0.29\ttie rate: 0.19\tloss rate: 0.44\n",
      "Epoch: 2326. Mean Return: -0.47656140350877224. Mean Length: 26.94736842105263. STD Length: 6.278398087767011\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.9230;\tfps: 185.05;\twin rate: 0.32\ttie rate: 0.16\tloss rate: 0.40\n",
      "Epoch: 2327. Mean Return: -1.5319675925925935. Mean Length: 28.444444444444443. STD Length: 7.238954975395445\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.8973;\tfps: 215.17;\twin rate: 0.32\ttie rate: 0.24\tloss rate: 0.28\n",
      "Epoch: 2328. Mean Return: -2.4449411764705884. Mean Length: 30.11764705882353. STD Length: 8.065046974380369\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.8375;\tfps: 181.66;\twin rate: 0.39\ttie rate: 0.11\tloss rate: 0.27\n",
      "Epoch: 2329. Mean Return: -4.30383823529412. Mean Length: 30.11764705882353. STD Length: 9.351202206078208\n",
      "================================================================\n",
      "Step 2560 / 2560;\t% valid = 0.7426;\tfps: 198.55;\twin rate: 0.36\ttie rate: 0.06\tloss rate: 0.27\n",
      "Epoch: 2330. Mean Return: -7.68229423868313. Mean Length: 31.604938271604937. STD Length: 10.442882876579981\n",
      "================================================================\n",
      "Step 1629 / 2560;\t% valid = 0.7403;\tfps: 193.47;\twin rate: 0.29\ttie rate: 0.10\tloss rate: 0.31\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m pretrain:\n\u001b[0;32m----> 2\u001b[0m     epoch \u001b[39m=\u001b[39m train_model(\n\u001b[1;32m      3\u001b[0m         model,\n\u001b[1;32m      4\u001b[0m         epoch,\n\u001b[1;32m      5\u001b[0m         use_kl\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      6\u001b[0m         use_adaptive_kl\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      7\u001b[0m         shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      8\u001b[0m         use_special\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      9\u001b[0m         just_train_critic\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     10\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[24], line 50\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, epoch_start, use_kl, use_adaptive_kl, shuffle, use_special, just_train_critic)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39m# Get the logits, action, and take one step in the environment\u001b[39;00m\n\u001b[1;32m     48\u001b[0m observation \u001b[39m=\u001b[39m observation\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39menv\u001b[39m.\u001b[39mobs_dim)\n\u001b[0;32m---> 50\u001b[0m logits, action, value_t \u001b[39m=\u001b[39m sample_action_value(observation, model)\n\u001b[1;32m     51\u001b[0m \u001b[39m# logits, action, value_t = best_action_value(observation, model)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m observation_new, reward, done, valid \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:933\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    931\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    934\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    935\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    936\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not pretrain:\n",
    "    epoch = train_model(\n",
    "        model,\n",
    "        epoch,\n",
    "        use_kl=True,\n",
    "        use_adaptive_kl=False,\n",
    "        shuffle=True,\n",
    "        use_special=False,\n",
    "        just_train_critic=False,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove previous\n",
    "os.system(f\"rm models/{model_name}.keras\")\n",
    "time.sleep(1)\n",
    "\n",
    "# save\n",
    "model.save(f\"models/{model_name}.keras\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: keras.Model, num_episodes: int = 10):\n",
    "    mean_reward = 0\n",
    "    mean_len = 0\n",
    "    num_won = 0\n",
    "    num_lost = 0\n",
    "    for _ in range(num_episodes):\n",
    "        # initialize vars\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        i = 0\n",
    "\n",
    "        # complete a round\n",
    "        while not done:\n",
    "            # step process\n",
    "            _, action, _ = best_action_value(obs.reshape(1, *obs.shape), model)\n",
    "            obs, reward, done, _ = env.step(action[0].numpy())\n",
    "\n",
    "            # update counts\n",
    "            mean_reward += reward\n",
    "            i += 1\n",
    "        # update mean length\n",
    "        mean_len += i\n",
    "\n",
    "        if env.won:\n",
    "            num_won += 1\n",
    "        if env.lost:\n",
    "            num_lost += 1\n",
    "\n",
    "    # average them\n",
    "    mean_reward /= num_episodes\n",
    "    mean_len /= num_episodes\n",
    "    print(\"Episode mean reward:\", mean_reward)\n",
    "    print(\"Episode mean length:\", mean_len)\n",
    "    print(f\"win rate: {num_won / num_episodes}, loss rate: {num_lost / num_episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, num_episodes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
