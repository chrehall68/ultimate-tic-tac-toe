{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultimate TTT Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 17:32:04.333807: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-22 17:32:05.068200: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-06-22 17:32:06.227149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-22 17:32:06.250990: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-22 17:32:06.251207: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import scipy.signal\n",
    "\n",
    "# sanity check\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to send data\n",
    "import os\n",
    "import time\n",
    "\n",
    "# proto definitions\n",
    "import py.board_pb2 as pb\n",
    "\n",
    "# misc\n",
    "from typing import Tuple\n",
    "\n",
    "# math\n",
    "import numpy as np\n",
    "\n",
    "# constants\n",
    "ROWS = 3\n",
    "COLS = 3\n",
    "CELLS = 9\n",
    "\n",
    "# exploration parameters\n",
    "used_valid_actions = dict()  # maps action: how many times it's been used validly\n",
    "exploration_reward_decay_rate = 0.001  # decay rate for rewarding exploration\n",
    "# exploration_reward = lambda times: np.exp(-exploration_reward_decay_rate * times)\n",
    "exploration_reward = lambda times: 1\n",
    "\n",
    "# reward parameters\n",
    "win_reward = 10\n",
    "cell_reward = 2\n",
    "\n",
    "# debugging\n",
    "used_actions = dict()\n",
    "cur_state = None\n",
    "\n",
    "\n",
    "class UltimateTicTacToeEnv:\n",
    "    obs_dim = (9, 9, 4)\n",
    "    n_actions = CELLS * CELLS\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.reset()\n",
    "\n",
    "    def _receive(self, path: str, tp: type):\n",
    "        while not os.path.exists(path) or os.stat(path).st_size == 0:\n",
    "            time.sleep(0.0001)\n",
    "\n",
    "        ret = tp()\n",
    "        with open(path, \"rb\") as file:\n",
    "            ret.ParseFromString(file.read())\n",
    "        with open(path, \"wb\") as file:\n",
    "            file.truncate()\n",
    "        return ret\n",
    "\n",
    "    def _get_return(self) -> pb.ReturnMessage:\n",
    "        return self._receive(\"./returnmessage.b\", pb.ReturnMessage)\n",
    "\n",
    "    def _get_state(self) -> pb.StateMessage:\n",
    "        return self._receive(\"./statemessage.b\", pb.StateMessage)\n",
    "\n",
    "    def _make_coord(self, idx) -> pb.Coord:\n",
    "        return pb.Coord(row=idx // COLS, col=idx % COLS)\n",
    "\n",
    "    def _send_action(self, move) -> None:\n",
    "        action = pb.ActionMessage(move=move)\n",
    "\n",
    "        with open(\"./action.b\", \"wb\") as file:\n",
    "            file.write(action.SerializeToString())\n",
    "\n",
    "    def _to_idx(self, coord: pb.Coord) -> int:\n",
    "        return coord.row * COLS + coord.col\n",
    "\n",
    "    def _to_multi_idx(self, move: pb.Move) -> int:\n",
    "        return self._to_idx(move.large) * CELLS + self._to_idx(move.small)\n",
    "\n",
    "    def _process_state(self, state: pb.StateMessage) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        The structure of the state:\n",
    "        (9, 9, 4)\n",
    "        Outer 9 represent board cells\n",
    "        inner 9 represent the cell spaces\n",
    "        each space has 3 objects:\n",
    "            space owner (0, 1, 2) representing if the space is claimed or not\n",
    "            cell owner (0, 1, 2) representing if the cell the space belongs to is claimed or not\n",
    "            curcellornot (0, 1); 1 if the space belongs to the current cell, 0 if not\n",
    "            turn (1, 2) 1 if the current turn is player1, 2 if the current turn is player2\n",
    "        \"\"\"\n",
    "        board_state = np.zeros(self.obs_dim)\n",
    "        for cell_idx in range(len(state.board.cells)):\n",
    "            for space_idx in range(len(state.board.cells[cell_idx].spaces)):\n",
    "                board_state[cell_idx, space_idx, 0] = (\n",
    "                    state.board.cells[cell_idx].spaces[space_idx].val\n",
    "                )\n",
    "                board_state[cell_idx, space_idx, 1] = state.cellowners[cell_idx]\n",
    "                board_state[cell_idx, space_idx, 2] = (\n",
    "                    1 if self._to_idx(state.board.curCell) == cell_idx else 0\n",
    "                )\n",
    "                board_state[cell_idx, space_idx, 3] = state.turn\n",
    "\n",
    "        return board_state\n",
    "\n",
    "    def _get_exploration_reward(self, action: int, msg: pb.ReturnMessage) -> float:\n",
    "        if msg.valid:\n",
    "            if action not in used_valid_actions:\n",
    "                used_valid_actions[action] = 1\n",
    "            else:\n",
    "                used_valid_actions[action] += 1\n",
    "            # return exploration_reward(used_valid_actions[action])\n",
    "            return 0\n",
    "        return -1\n",
    "\n",
    "    def _get_win_reward(self, msg: pb.ReturnMessage) -> float:\n",
    "        \"\"\"\n",
    "        Get's the reward for winning if the game was won\n",
    "        \"\"\"\n",
    "        # the turn sent in the return message should still be the caller's turn\n",
    "        if msg.state.winner == msg.state.turn:\n",
    "            return win_reward\n",
    "        return 0\n",
    "\n",
    "    def _get_cell_reward(self, msg: pb.ReturnMessage) -> float:\n",
    "        \"\"\"\n",
    "        Get's the reward for claiming a cell if a cell was claimed\n",
    "        \"\"\"\n",
    "        if self.prev_cellowners == msg.state.cellowners:\n",
    "            return 0\n",
    "        elif list(msg.state.cellowners).count(\n",
    "            msg.state.turn\n",
    "        ) > self.prev_cellowners.count(msg.state.turn):\n",
    "            self.prev_cellowners = list(msg.state.cellowners)\n",
    "            return cell_reward\n",
    "        return 0\n",
    "\n",
    "    def _get_reward(self, action: pb.Move, msg: pb.ReturnMessage) -> float:\n",
    "        return (\n",
    "            self._get_exploration_reward(action, msg)\n",
    "            + self._get_cell_reward(msg)\n",
    "            + self._get_win_reward(msg)\n",
    "        )\n",
    "\n",
    "    # public section\n",
    "    def observe(self) -> np.ndarray:\n",
    "        global cur_state\n",
    "        state = self._get_state()\n",
    "        cur_state = state\n",
    "        self._turn = state.turn\n",
    "        return self._process_state(state)\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            - next state\n",
    "            - reward for the action\n",
    "            - done / not done\n",
    "            - valid / invalid\n",
    "        \"\"\"\n",
    "        # update the count\n",
    "        if action not in used_actions:\n",
    "            used_actions[action] = 1\n",
    "        else:\n",
    "            used_actions[action] += 1\n",
    "\n",
    "        # send action and get response\n",
    "        self._send_action(self.to_move(action))\n",
    "        ret_message = self._get_return()\n",
    "\n",
    "        # return information\n",
    "        reward = self._get_reward(action, ret_message)\n",
    "        done = ret_message.state.done\n",
    "        if not done:\n",
    "            return self.observe(), reward, done, ret_message.valid\n",
    "        else:\n",
    "            return (\n",
    "                self._process_state(ret_message.state),\n",
    "                reward,\n",
    "                done,\n",
    "                ret_message.valid,\n",
    "            )\n",
    "\n",
    "    def turn(self):\n",
    "        return self._turn\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        self.cleanup()\n",
    "        self.pid = os.spawnl(os.P_NOWAIT, \"uttt\", \"uttt\", \"aivai\")\n",
    "        time.sleep(0.1)\n",
    "        self.prev_cellowners = [pb.NONE] * 9\n",
    "        return self.observe()\n",
    "\n",
    "    def cleanup(self):\n",
    "        os.system(\"killall -q uttt\")\n",
    "        os.system(\"rm -f ./statemessage.b ./returnmessage.b ./action.b\")\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.cleanup()\n",
    "\n",
    "    def to_move(self, idx: int) -> pb.Move:\n",
    "        outer_idx = idx // CELLS\n",
    "        inner_idx = idx % CELLS\n",
    "\n",
    "        return pb.Move(\n",
    "            large=self._make_coord(outer_idx), small=self._make_coord(inner_idx)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UltimateTicTacToeEnv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buffers and Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the PPO algorithm\n",
    "clip_ratio = 0.2\n",
    "target_kl = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    # Buffer for storing trajectories\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        # Buffer initialization\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, *observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, action, reward, value, logprobability):\n",
    "        # Append one step of agent-environment interaction\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.logprobability_buffer[self.pointer] = logprobability\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0):\n",
    "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            deltas, self.gamma * self.lam\n",
    "        )\n",
    "        self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            rewards, self.gamma\n",
    "        )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        # Get all data of the buffer and normalize the advantages\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.logprobability_buffer,\n",
    "        )\n",
    "\n",
    "\n",
    "def logprobabilities(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    logprobabilities_all = tf.nn.log_softmax(logits)\n",
    "    logprobability = tf.reduce_sum(\n",
    "        tf.one_hot(a, UltimateTicTacToeEnv.n_actions) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    return logprobability\n",
    "\n",
    "\n",
    "# Sample action from actor\n",
    "@tf.function\n",
    "def sample_action(observation, actor):\n",
    "    logits = actor(observation)\n",
    "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "    return logits, action\n",
    "\n",
    "\n",
    "# Train the policy by maxizing the PPO-Clip objective\n",
    "@tf.function\n",
    "def train_policy(\n",
    "    observation_buffer,\n",
    "    action_buffer,\n",
    "    logprobability_buffer,\n",
    "    advantage_buffer,\n",
    "    actor: keras.Model,\n",
    "    policy_optimizer: tf.keras.optimizers.Optimizer,\n",
    "):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        ratio = tf.exp(\n",
    "            logprobabilities(actor(observation_buffer), action_buffer)\n",
    "            - logprobability_buffer\n",
    "        )\n",
    "        min_advantage = tf.where(\n",
    "            advantage_buffer > 0,\n",
    "            (1 + clip_ratio) * advantage_buffer,\n",
    "            (1 - clip_ratio) * advantage_buffer,\n",
    "        )\n",
    "\n",
    "        policy_loss = -tf.reduce_mean(\n",
    "            tf.minimum(ratio * advantage_buffer, min_advantage)\n",
    "        ) + tf.reduce_sum(actor.losses)\n",
    "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
    "\n",
    "    kl = tf.reduce_mean(\n",
    "        logprobability_buffer\n",
    "        - logprobabilities(actor(observation_buffer), action_buffer)\n",
    "    )\n",
    "    kl = tf.reduce_sum(kl)\n",
    "    return kl\n",
    "\n",
    "\n",
    "# Train the value function by regression on mean-squared error\n",
    "@tf.function\n",
    "def train_value_function(\n",
    "    observation_buffer,\n",
    "    return_buffer,\n",
    "    critic: keras.Model,\n",
    "    value_optimizer: tf.keras.optimizers.Optimizer,\n",
    "):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        value_loss = tf.reduce_mean(\n",
    "            (return_buffer - critic(observation_buffer)) ** 2\n",
    "        ) + tf.reduce_sum(critic.losses)\n",
    "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
    "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 17:32:07.184872: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-22 17:32:07.185383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-22 17:32:07.185533: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-22 17:32:07.856465: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-22 17:32:07.856667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-22 17:32:07.856825: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-22 17:32:07.856940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9855 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:06:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "def create_actor():\n",
    "    # model inputs\n",
    "    inputs = tf.keras.Input(shape=UltimateTicTacToeEnv.obs_dim)\n",
    "\n",
    "    x = layers.Conv1D(\n",
    "        256,\n",
    "        kernel_size=9,\n",
    "        strides=1,\n",
    "        padding=\"valid\",\n",
    "        activation=\"relu\",\n",
    "        activity_regularizer=\"l2\",\n",
    "    )(inputs)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Conv2D(\n",
    "        1024,\n",
    "        kernel_size=(9, 1),\n",
    "        strides=(1, 1),\n",
    "        padding=\"valid\",\n",
    "        activation=\"relu\",\n",
    "        activity_regularizer=\"l2\",\n",
    "    )(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1024, activation=\"relu\", activity_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(1024, activation=\"relu\", activity_regularizer=\"l2\")(x)\n",
    "\n",
    "    logits = tf.keras.layers.Dense(UltimateTicTacToeEnv.n_actions)(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "\n",
    "def create_critic():\n",
    "    inputs = tf.keras.Input(shape=UltimateTicTacToeEnv.obs_dim)\n",
    "\n",
    "    x = layers.Conv1D(\n",
    "        256,\n",
    "        kernel_size=9,\n",
    "        strides=1,\n",
    "        padding=\"valid\",\n",
    "        activation=\"relu\",\n",
    "        activity_regularizer=\"l2\",\n",
    "    )(inputs)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Conv2D(\n",
    "        1024,\n",
    "        kernel_size=(9, 1),\n",
    "        strides=(1, 1),\n",
    "        padding=\"valid\",\n",
    "        activation=\"relu\",\n",
    "        activity_regularizer=\"l2\",\n",
    "    )(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1024, activation=\"relu\", activity_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(1024, activation=\"relu\", activity_regularizer=\"l2\")(x)\n",
    "\n",
    "    values = tf.keras.layers.Dense(1)(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=values)\n",
    "\n",
    "\n",
    "if load_model and os.path.exists(\"actor1.keras\") and os.path.exists(\"critic1.keras\"):\n",
    "    print(\"loading model...\")\n",
    "    actor1 = tf.keras.models.load_model(\"actor1.keras\")\n",
    "    critic1 = tf.keras.models.load_model(\"critic1.keras\")\n",
    "else:\n",
    "    print(\"creating model...\")\n",
    "    actor1 = create_actor()\n",
    "    critic1 = create_critic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 9, 9, 4)]         0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 9, 1, 256)         9472      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 9, 1, 256)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 1, 1, 1024)        2360320   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1, 1, 1024)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 81)                83025     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,552,017\n",
      "Trainable params: 4,552,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "actor1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 9, 9, 4)]         0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 9, 1, 256)         9472      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 9, 1, 256)         0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 1, 1, 1024)        2360320   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1, 1, 1024)        0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,470,017\n",
      "Trainable params: 4,470,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "critic1.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate hyperparams\n",
    "policy_learning_rate = 3e-4  # 3e-4\n",
    "value_function_learning_rate = 1e-3  # 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic1_optimizer = tf.keras.optimizers.Adam(learning_rate=value_function_learning_rate)\n",
    "\n",
    "actor1_optimizer = tf.keras.optimizers.Adam(learning_rate=policy_learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training hyperparameters\n",
    "\n",
    "# alg hyperparameters\n",
    "gamma = 0.01  # 0.99\n",
    "lam = 0.01  # 0.97\n",
    "\n",
    "# training time hyperparameters\n",
    "train_policy_iterations = 50\n",
    "train_value_iterations = 50\n",
    "epochs = 25\n",
    "minibatch_size = 512\n",
    "steps_per_epoch = 4096  # should be a multiple of minibatch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = Buffer(UltimateTicTacToeEnv.obs_dim, steps_per_epoch, gamma=gamma, lam=lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for fps logging\n",
    "from datetime import datetime\n",
    "# main training function\n",
    "def train_model(actor, critic, actor_optimizer, critic_optimizer, player1):\n",
    "    if player1:\n",
    "        cond = lambda turn: turn == pb.Owner.PLAYER1\n",
    "    else:\n",
    "        cond = lambda turn: turn == pb.Owner.PLAYER2\n",
    "\n",
    "    # Iterate over the number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "        sum_return = 0\n",
    "        sum_length = 0\n",
    "        num_episodes = 0\n",
    "        episode_return = 0\n",
    "        episode_length = 0\n",
    "        num_valid = 0\n",
    "\n",
    "        # Iterate over the steps of each epoch\n",
    "        observation = env.reset()\n",
    "        t = 0\n",
    "        start_time = datetime.now()\n",
    "        while t < steps_per_epoch:\n",
    "            if cond(env.turn()):  # if it's the training ai's turn\n",
    "                # Get the logits, action, and take one step in the environment\n",
    "                observation = observation.reshape(1, *env.obs_dim)\n",
    "                logits, action = sample_action(observation, actor)\n",
    "                observation_new, reward, done, valid = env.step(action[0].numpy())\n",
    "                episode_return += reward\n",
    "                episode_length += 1\n",
    "                num_valid += 1 if valid else 0\n",
    "\n",
    "                # Get the value and log-probability of the action\n",
    "                value_t = critic(observation)\n",
    "                logprobability_t = logprobabilities(logits, action)\n",
    "\n",
    "                # Store obs, act, rew, v_t, logp_pi_t\n",
    "                buffer.store(\n",
    "                    observation, action, reward, value_t, logprobability_t\n",
    "                )\n",
    "\n",
    "                t += 1\n",
    "            else:\n",
    "                # just keep trying random actions until it works\n",
    "                observation_new, reward, done, valid = env.step(\n",
    "                    np.random.randint(0, env.n_actions)\n",
    "                )\n",
    "\n",
    "            # Update the observation\n",
    "            observation = observation_new\n",
    "\n",
    "            # Finish trajectory if reached to a terminal state\n",
    "            terminal = done\n",
    "            if terminal or (t % minibatch_size == minibatch_size - 1):\n",
    "                last_value = 0 if done else critic(observation.reshape(1, *env.obs_dim))\n",
    "                buffer.finish_trajectory(last_value)\n",
    "                sum_return += episode_return\n",
    "                sum_length += episode_length\n",
    "                num_episodes += 1\n",
    "                observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "            print(\n",
    "                f\"Step {t} / {steps_per_epoch}; % valid = {num_valid / (t+1)}; fps: {t/(datetime.now()-start_time).total_seconds()}\",\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "        print()\n",
    "\n",
    "        # Get values from the buffer\n",
    "        # 0 - observation_buffer,\n",
    "        # 1 - action_buffer,\n",
    "        # 2 - advantage_buffer,\n",
    "        # 3 - return_buffer,\n",
    "        # 4 - logprobability_buffer,\n",
    "        (observation_buffer, action_buffer, advantage_buffer, return_buffer, logprobability_buffer) = buffer.get()\n",
    "\n",
    "        # Update the policy and implement early stopping using KL divergence\n",
    "        for _ in range(train_policy_iterations):\n",
    "            kl = 0\n",
    "            for i in range(steps_per_epoch//minibatch_size):\n",
    "                kl += train_policy(\n",
    "                    tf.constant(observation_buffer[i*minibatch_size:(i+1)*minibatch_size]),  # obs\n",
    "                    tf.constant(action_buffer[i*minibatch_size:(i+1)*minibatch_size]),  # act\n",
    "                    tf.constant(logprobability_buffer[i*minibatch_size:(i+1)*minibatch_size]),  # logprobs\n",
    "                    tf.constant(advantage_buffer[i*minibatch_size:(i+1)*minibatch_size]),  # advantages\n",
    "                    actor,\n",
    "                    actor_optimizer,\n",
    "                )\n",
    "            kl /= steps_per_epoch//minibatch_size\n",
    "            if kl > 1.5 * target_kl:\n",
    "                # Early Stopping\n",
    "                break\n",
    "\n",
    "        # Update the value function\n",
    "        for _ in range(train_value_iterations):\n",
    "            for i in range(steps_per_epoch//minibatch_size):\n",
    "                train_value_function(\n",
    "                    tf.constant(observation_buffer[i*minibatch_size:(i+1)*minibatch_size]),  # obs buffer\n",
    "                    tf.constant(return_buffer[i*minibatch_size:(i+1)*minibatch_size]),  # returns\n",
    "                    critic,\n",
    "                    critic_optimizer,\n",
    "                )\n",
    "\n",
    "        # Print mean return and length for each epoch\n",
    "        print(\n",
    "            f\"Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum_length / num_episodes}\"\n",
    "        )\n",
    "        print(\"=\" * 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4096 / 4096; % valid = 0.6170368562362705; fps: 23.756078432873682\n",
      "Epoch: 1. Mean Return: -4.640776699029126. Mean Length: 39.75728155339806\n",
      "================================================================\n",
      "Step 4096 / 4096; % valid = 0.6983158408591652; fps: 21.122178762640388\n",
      "Epoch: 2. Mean Return: -0.06956521739130435. Mean Length: 35.608695652173914\n",
      "================================================================\n",
      "Step 4096 / 4096; % valid = 0.6709787649499633; fps: 21.726716815352166\n",
      "Epoch: 3. Mean Return: -1.6846846846846846. Mean Length: 36.891891891891895\n",
      "================================================================\n",
      "Step 4096 / 4096; % valid = 0.6787893580668782; fps: 21.670723128698803\n",
      "Epoch: 4. Mean Return: -0.41228070175438597. Mean Length: 35.921052631578945\n",
      "================================================================\n",
      "Step 4096 / 4096; % valid = 0.6924578960214791; fps: 21.078755975461938\n",
      "Epoch: 5. Mean Return: 0.5083333333333333. Mean Length: 34.125\n",
      "================================================================\n",
      "Step 4096 / 4096; % valid = 0.7036856236270442; fps: 20.977169296014348\n",
      "Epoch: 6. Mean Return: 0.12295081967213115. Mean Length: 33.5655737704918\n",
      "================================================================\n",
      "Step 4096 / 4096; % valid = 0.7139370270929949; fps: 21.028059141868113\n",
      "Epoch: 7. Mean Return: 1.125984251968504. Mean Length: 32.24409448818898\n",
      "================================================================\n",
      "Step 4096 / 4096; % valid = 0.7495728581889187; fps: 19.869123981868686\n",
      "Epoch: 8. Mean Return: 2.3507462686567164. Mean Length: 30.559701492537314\n",
      "================================================================\n",
      "Step 4096 / 4096; % valid = 0.7337075909201854; fps: 20.364519833781966\n",
      "Epoch: 9. Mean Return: 0.5606060606060606. Mean Length: 31.022727272727273\n",
      "================================================================\n",
      "Step 4096 / 4096; % valid = 0.7029533805223334; fps: 20.398275634558058\n",
      "Epoch: 10. Mean Return: 0.359375. Mean Length: 31.9921875\n",
      "================================================================\n",
      "Step 4096 / 4096; % valid = 0.6780571149621675; fps: 20.709029335487754\n",
      "Epoch: 11. Mean Return: -1.0661157024793388. Mean Length: 33.84297520661157\n",
      "================================================================\n",
      "Step 4096 / 4096; % valid = 0.719794971930681; fps: 19.4202139687503463\n",
      "Epoch: 12. Mean Return: 0.28888888888888886. Mean Length: 30.333333333333332\n",
      "================================================================\n",
      "Step 4096 / 4096; % valid = 0.772516475469856; fps: 19.4431139935675358\n",
      "Epoch: 13. Mean Return: 3.0551724137931036. Mean Length: 28.24137931034483\n",
      "================================================================\n",
      "Step 4096 / 4096; % valid = 0.740541859897486; fps: 19.2403366442620728\n",
      "Epoch: 14. Mean Return: 1.5070422535211268. Mean Length: 28.838028169014084\n",
      "================================================================\n",
      "Step 4096 / 4096; % valid = 0.8059555772516476; fps: 18.773243174832988\n",
      "Epoch: 15. Mean Return: 4.62987012987013. Mean Length: 26.59090909090909\n",
      "================================================================\n",
      "Step 4096 / 4096; % valid = 0.6570661459604589; fps: 21.4825898923659335\n",
      "Epoch: 16. Mean Return: -1.8487394957983194. Mean Length: 34.411764705882355\n",
      "================================================================\n",
      "Step 4096 / 4096; % valid = 0.7712960702953381; fps: 19.159944220612388\n",
      "Epoch: 17. Mean Return: 2.4081632653061225. Mean Length: 27.857142857142858\n",
      "================================================================\n",
      "Step 4096 / 4096; % valid = 0.7485965340493044; fps: 19.416319828274787\n",
      "Epoch: 18. Mean Return: 2.5. Mean Length: 28.838028169014084\n",
      "================================================================\n",
      "Step 67 / 4096; % valid = 0.6764705882352942; fps: 18.079737579355207\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model(actor1, critic1, actor1_optimizer, critic1_optimizer, \u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[18], line 35\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(actor, critic, actor_optimizer, critic_optimizer, player1)\u001b[0m\n\u001b[1;32m     32\u001b[0m num_valid \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m valid \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[39m# Get the value and log-probability of the action\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m value_t \u001b[39m=\u001b[39m critic(observation)\n\u001b[1;32m     36\u001b[0m logprobability_t \u001b[39m=\u001b[39m logprobabilities(logits, action)\n\u001b[1;32m     38\u001b[0m \u001b[39m# Store obs, act, rew, v_t, logp_pi_t\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/keras/engine/training.py:558\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    556\u001b[0m     layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[0;32m--> 558\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/keras/engine/base_layer.py:1145\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1142\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1144\u001b[0m ):\n\u001b[0;32m-> 1145\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/keras/engine/functional.py:512\u001b[0m, in \u001b[0;36mFunctional.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[39m@doc_controls\u001b[39m\u001b[39m.\u001b[39mdo_not_doc_inheritable\n\u001b[1;32m    494\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs, training\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    495\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \n\u001b[1;32m    497\u001b[0m \u001b[39m    In this case `call` just reapplies\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[39m        a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_internal_graph(inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/keras/engine/functional.py:669\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[1;32m    668\u001b[0m args, kwargs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[0;32m--> 669\u001b[0m outputs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39;49mlayer(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    671\u001b[0m \u001b[39m# Update tensor_dict.\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[39mfor\u001b[39;00m x_id, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[1;32m    673\u001b[0m     node\u001b[39m.\u001b[39mflat_output_ids, tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(outputs)\n\u001b[1;32m    674\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/keras/engine/base_layer.py:1148\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m-> 1148\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_activity_regularization(inputs, outputs)\n\u001b[1;32m   1149\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_supports_masking:\n\u001b[1;32m   1150\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_mask_metadata(\n\u001b[1;32m   1151\u001b[0m         inputs, outputs, input_masks, \u001b[39mnot\u001b[39;00m eager\n\u001b[1;32m   1152\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/keras/engine/base_layer.py:2859\u001b[0m, in \u001b[0;36mLayer._handle_activity_regularization\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m   2854\u001b[0m \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m output_list:\n\u001b[1;32m   2855\u001b[0m     activity_loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(\n\u001b[1;32m   2856\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer(output)\n\u001b[1;32m   2857\u001b[0m     )\n\u001b[1;32m   2858\u001b[0m     batch_size \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(\n\u001b[0;32m-> 2859\u001b[0m         tf\u001b[39m.\u001b[39;49mshape(output)[\u001b[39m0\u001b[39;49m], activity_loss\u001b[39m.\u001b[39mdtype\n\u001b[1;32m   2860\u001b[0m     )\n\u001b[1;32m   2861\u001b[0m     \u001b[39m# Make activity regularization strength batch-agnostic.\u001b[39;00m\n\u001b[1;32m   2862\u001b[0m     mean_activity_loss \u001b[39m=\u001b[39m activity_loss \u001b[39m/\u001b[39m batch_size\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/ops/array_ops.py:1090\u001b[0m, in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(\n\u001b[1;32m   1085\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1086\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mstrided_slice\u001b[39m\u001b[39m\"\u001b[39m, [tensor] \u001b[39m+\u001b[39m begin \u001b[39m+\u001b[39m end \u001b[39m+\u001b[39m strides,\n\u001b[1;32m   1087\u001b[0m     skip_on_eager\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m name:\n\u001b[1;32m   1088\u001b[0m   \u001b[39mif\u001b[39;00m begin:\n\u001b[1;32m   1089\u001b[0m     packed_begin, packed_end, packed_strides \u001b[39m=\u001b[39m (stack(begin), stack(end),\n\u001b[0;32m-> 1090\u001b[0m                                                 stack(strides))\n\u001b[1;32m   1091\u001b[0m     \u001b[39m# TODO(mdan): Instead of implicitly casting, it's better to enforce the\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m     \u001b[39m# same dtypes.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[39mif\u001b[39;00m (packed_begin\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m dtypes\u001b[39m.\u001b[39mint64 \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m         packed_end\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m dtypes\u001b[39m.\u001b[39mint64 \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m         packed_strides\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m dtypes\u001b[39m.\u001b[39mint64):\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/ops/array_ops.py:1484\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1482\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1483\u001b[0m     \u001b[39m# If the input is a constant list, it can be converted to a constant op\u001b[39;00m\n\u001b[0;32m-> 1484\u001b[0m     \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(values, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   1485\u001b[0m   \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m, \u001b[39mNotImplementedError\u001b[39;00m):\n\u001b[1;32m   1486\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Input list contains non-constant tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:1642\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1633\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1634\u001b[0m           _add_error_prefix(\n\u001b[1;32m   1635\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1638\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1639\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[1;32m   1641\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1642\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m   1644\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1645\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:344\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    342\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    343\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[0;32m--> 344\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:268\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    173\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \n\u001b[1;32m    175\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    269\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:280\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    279\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 280\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    282\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[1;32m    283\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:305\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    304\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[1;32m    306\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(actor1, critic1, actor1_optimizer, critic1_optimizer, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# remove previous\n",
    "os.system(\"rm actor1.keras critic1.keras\")\n",
    "time.sleep(1)\n",
    "\n",
    "# save\n",
    "actor1.save(\"actor1.keras\")\n",
    "critic1.save(\"critic1.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
