{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultimate TTT Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 09:31:45.175040: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-07 09:31:45.713344: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-07 09:31:48.347337: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 09:31:48.363044: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-07 09:31:48.363212: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import scipy.signal\n",
    "\n",
    "# used for fps logging\n",
    "from datetime import datetime\n",
    "\n",
    "# sanity check\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train.ini']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"train.ini\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env Constants\n",
    "MAX_TIMESTEPS = config[\"ENV\"].getint(\"MAX_TIMESTEPS\")\n",
    "\n",
    "# board constants\n",
    "ROWS = config[\"ENV\"].getint(\"ROWS\")\n",
    "COLS = config[\"ENV\"].getint(\"COLS\")\n",
    "CELLS = config[\"ENV\"].getint(\"CELLS\")\n",
    "\n",
    "# socket constants\n",
    "S_PORT = config[\"ENV\"].getint(\"S_PORT\")\n",
    "A_PORT = config[\"ENV\"].getint(\"A_PORT\")\n",
    "R_PORT = config[\"ENV\"].getint(\"R_PORT\")\n",
    "MAX_MSG_SIZE = config[\"ENV\"].getint(\"MAX_MSG_SIZE\")\n",
    "\n",
    "# reward parameters\n",
    "WIN_REWARD = config[\"REWARD\"].getfloat(\"WIN_REWARD\")\n",
    "CELL_REWARD = config[\"REWARD\"].getfloat(\"CELL_REWARD\")\n",
    "VALID_REWARD = config[\"REWARD\"].getfloat(\"VALID_REWARD\")\n",
    "TIMEOUT_PENALTY = config[\"REWARD\"].getfloat(\"TIMEOUT_PENALTY\")\n",
    "INVALID_PENALTY = config[\"REWARD\"].getfloat(\"INVALID_PENALTY\")\n",
    "LOSS_PENALTY = config[\"REWARD\"].getfloat(\"LOSS_PENALTY\")\n",
    "TIE_REWARD = config[\"REWARD\"].getfloat(\"TIE_REWARD\")\n",
    "\n",
    "# misc\n",
    "SLEEP_TIME = config[\"ENV\"].getfloat(\"SLEEP_TIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opponents\n",
    "\n",
    "# math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# opponents\n",
    "class Opponent:\n",
    "    def __init__(self, _env=None) -> None:\n",
    "        self.env = _env\n",
    "    def get_action(self, obs) -> int:\n",
    "        pass\n",
    "\n",
    "\n",
    "class ValidRandomOpponent(Opponent):\n",
    "    \"\"\"\n",
    "    Makes valid moves when there is a cur cell\n",
    "    \"\"\"\n",
    "    def __init__(self, _env=None) -> None:\n",
    "        super().__init__(_env)\n",
    "\n",
    "    def get_action(self, obs: np.ndarray) -> int:\n",
    "        return np.random.choice(self.env.validmoves)\n",
    "\n",
    "\n",
    "class CellWinningRandomOpponent(ValidRandomOpponent):\n",
    "    \"\"\"\n",
    "    Wins a cell if possible\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_actions) -> None:\n",
    "        super().__init__(n_actions)\n",
    "\n",
    "    def get_cur_cell(self, obs: np.ndarray) -> tuple[bool, int]:\n",
    "        # determine if there is a current cell\n",
    "        cur_cell_exists = False\n",
    "        cur_cell = -1\n",
    "        for outer in range(obs.shape[0]):\n",
    "            if obs[outer, 0, 2] == 1:\n",
    "                cur_cell_exists = True\n",
    "                cur_cell = outer\n",
    "                break\n",
    "        return cur_cell_exists, cur_cell\n",
    "\n",
    "    def get_winning_cells(self, cell: list[int], turn: int) -> list[bool]:\n",
    "        \"\"\"\n",
    "        Returns a list of bools, representing whether that space\n",
    "        will win the given cell\n",
    "        Note: this works as long as the parameters are the correct types\n",
    "        \"\"\"\n",
    "        cell = np.array(cell).reshape((ROWS, COLS)).tolist()\n",
    "        ret = [[False for _ in range(COLS)] for x in range(ROWS)]\n",
    "\n",
    "        # check horizontals\n",
    "        for row in range(ROWS):\n",
    "            if cell[row].count(turn) == 2 and cell[row].count(0) == 1:\n",
    "                # this row is winnable\n",
    "                winning_cell = cell[row].index(0)\n",
    "                ret[row][winning_cell] = True\n",
    "\n",
    "        # check verticals\n",
    "        for c in range(COLS):\n",
    "            col = [cell[i][c] for i in range(ROWS)]\n",
    "            if col.count(turn) == 2 and col.count(0) == 1:\n",
    "                # this col is winnable\n",
    "                winning_row = col.index(0)\n",
    "                ret[winning_row][c] = True\n",
    "\n",
    "        # check diagonal left\n",
    "        left_diagonal = [cell[i][i] for i in range(ROWS)]\n",
    "        if left_diagonal.count(turn) == 2 and left_diagonal.count(0) == 1:\n",
    "            winning_space = left_diagonal.index(0)\n",
    "            ret[winning_space][winning_space] = True\n",
    "\n",
    "        # check diagonal right\n",
    "        right_diagonal = [cell[i][ROWS - 1 - i] for i in range(ROWS)]\n",
    "        if right_diagonal.count(turn) == 2 and right_diagonal.count(0) == 1:\n",
    "            winning_space = right_diagonal.index(0)\n",
    "            ret[winning_space][ROWS - 1 - winning_space] = True\n",
    "\n",
    "        return np.array(ret).flatten().tolist()\n",
    "\n",
    "    def get_turn(self, obs: np.ndarray) -> int:\n",
    "        # whose turn it is\n",
    "        return obs[0, 0, 3]\n",
    "\n",
    "    def _get_cellwinning_action(self, cell: int, obs: np.ndarray) -> tuple[bool, int]:\n",
    "        \"\"\"\n",
    "        Get whether there is a cell winning action, and if there is,\n",
    "        return a random action from those\n",
    "        Returns:\n",
    "            * bool - whether there is a cell winning action\n",
    "            * int - the action number, or -1 if none\n",
    "        \"\"\"\n",
    "        spaces = [obs[cell, i, 0] for i in range(CELLS)]\n",
    "        winning_spaces = self.get_winning_cells(spaces, self.get_turn(obs))\n",
    "\n",
    "        # if any of them are cell-winning spaces, choose one of them\n",
    "        if any(winning_spaces):\n",
    "            idxs = [i for i in range(CELLS) if winning_spaces[i]]\n",
    "            return True, cell * CELLS + np.random.choice(idxs)\n",
    "        return False, -1\n",
    "\n",
    "    def get_action(self, obs: np.ndarray) -> int:\n",
    "        cur_cell_exists, cur_cell = self.get_cur_cell(obs)\n",
    "        if cur_cell_exists:\n",
    "            valid, action = self._get_cellwinning_action(cur_cell, obs)\n",
    "            if valid:\n",
    "                return action\n",
    "        return super().get_action(obs)\n",
    "\n",
    "\n",
    "class WinningRandomOpponent(CellWinningRandomOpponent):\n",
    "    def __init__(self, n_actions) -> None:\n",
    "        super().__init__(n_actions)\n",
    "\n",
    "    def get_winning_action(self, obs: np.ndarray) -> tuple[bool, int]:\n",
    "        turn = self.get_turn(obs)\n",
    "\n",
    "        cur_cell_exists, cur_cell = self.get_cur_cell(obs)\n",
    "\n",
    "        # go through all the cells and see which are claimed\n",
    "        owners: list[int] = []\n",
    "        for outer in range(obs.shape[0]):\n",
    "            owners.append(obs[outer, 0, 1])\n",
    "\n",
    "        # see which cells are winning cells\n",
    "        winning_cells = self.get_winning_cells(owners, turn)\n",
    "        if True in winning_cells:\n",
    "            if cur_cell_exists:\n",
    "                # if the cur cell is a possible winning cell\n",
    "                if winning_cells[cur_cell]:\n",
    "                    # get the space owners\n",
    "                    space_owners = [obs[cur_cell, i, 0] for i in range(obs.shape[1])]\n",
    "                    winning_spaces = self.get_winning_cells(space_owners, turn)\n",
    "                    # it is possible\n",
    "                    if True in winning_spaces:\n",
    "                        return True, cur_cell * CELLS + winning_spaces.index(True)\n",
    "            else:\n",
    "                # we can go anywhere\n",
    "                for potential_winning_cell_idx in range(CELLS):\n",
    "                    if winning_cells[potential_winning_cell_idx]:\n",
    "                        # get the space owners\n",
    "                        space_owners = [\n",
    "                            obs[potential_winning_cell_idx, i, 0]\n",
    "                            for i in range(obs.shape[1])\n",
    "                        ]\n",
    "                        winning_spaces = self.get_winning_cells(space_owners, turn)\n",
    "                        # it is possible\n",
    "                        if True in winning_spaces:\n",
    "                            return (\n",
    "                                True,\n",
    "                                potential_winning_cell_idx * CELLS\n",
    "                                + winning_spaces.index(True),\n",
    "                            )\n",
    "\n",
    "        # winning isn't possible currently\n",
    "        return False, -1\n",
    "\n",
    "    def get_action(self, obs: np.ndarray) -> int:\n",
    "        winnable, action = self.get_winning_action(obs)\n",
    "        if winnable:\n",
    "            return action\n",
    "        else:\n",
    "            cur_cell_exists, _ = self.get_cur_cell(obs)\n",
    "            if not cur_cell_exists:\n",
    "                # check if any of the cells can be won\n",
    "                for cell in range(CELLS):\n",
    "                    # if it's already claimed, move on\n",
    "                    if obs[cell, 0, 1] != 0:\n",
    "                        continue\n",
    "                    valid, action = self._get_cellwinning_action(cell, obs)\n",
    "                    if valid:\n",
    "                        return action\n",
    "\n",
    "        return super().get_action(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to send data\n",
    "import os\n",
    "import time\n",
    "import socket\n",
    "\n",
    "# proto definitions\n",
    "import py.board_pb2 as pb\n",
    "\n",
    "# misc\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "# env\n",
    "class UltimateTicTacToeEnv:\n",
    "    obs_dim = (9, 9, 4)\n",
    "    n_actions = CELLS * CELLS\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        opponent: Opponent = ValidRandomOpponent(),\n",
    "        max_timesteps: int = 81,\n",
    "        player1: bool = True,\n",
    "    ) -> None:\n",
    "        opponent.env = self\n",
    "        self.s_conn, self.a_conn, self.r_conn = None, None, None\n",
    "        self.opponent = opponent\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.player1 = player1\n",
    "        self.reset()\n",
    "\n",
    "    def _receive(self, conn: socket.socket, tp: type):\n",
    "        ret = tp()\n",
    "        b = conn.recv(MAX_MSG_SIZE)\n",
    "        ret.ParseFromString(b)\n",
    "        return ret\n",
    "\n",
    "    def _get_return(self) -> pb.ReturnMessage:\n",
    "        return self._receive(self.r_conn, pb.ReturnMessage)\n",
    "\n",
    "    def _get_state(self) -> pb.StateMessage:\n",
    "        return self._receive(self.s_conn, pb.StateMessage)\n",
    "\n",
    "    def _make_coord(self, idx) -> pb.Coord:\n",
    "        return pb.Coord(row=idx // COLS, col=idx % COLS)\n",
    "\n",
    "    def _send_action(self, move) -> None:\n",
    "        action = pb.ActionMessage(move=move)\n",
    "        self.a_conn.send(action.SerializeToString())\n",
    "\n",
    "    def _to_idx(self, coord: pb.Coord) -> int:\n",
    "        return coord.row * COLS + coord.col\n",
    "\n",
    "    def _to_multi_idx(self, move: pb.Move) -> int:\n",
    "        return self._to_idx(move.large) * CELLS + self._to_idx(move.small)\n",
    "\n",
    "    def _process_state(self, state: pb.StateMessage) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        The structure of the state:\n",
    "        (9, 9, 4)\n",
    "        Outer 9 represent board cells\n",
    "        inner 9 represent the cell spaces\n",
    "        each space has 5 objects:\n",
    "            space owner (0, 1, 2) representing if the space is claimed or not\n",
    "            cell owner (0, 1, 2) representing if the cell the space belongs to is claimed or not\n",
    "            curcellornot (0, 1); 1 if the space belongs to the current cell, 0 if not\n",
    "            valid (0, 1); 1 if the space is a valid move, 0 if not\n",
    "        \"\"\"\n",
    "        board_state = np.zeros(self.obs_dim)\n",
    "        for cell_idx in range(len(state.board.cells)):\n",
    "            for space_idx in range(len(state.board.cells[cell_idx].spaces)):\n",
    "                board_state[cell_idx, space_idx, 0] = (\n",
    "                    state.board.cells[cell_idx].spaces[space_idx].val\n",
    "                )\n",
    "                board_state[cell_idx, space_idx, 1] = state.cellowners[cell_idx]\n",
    "                board_state[cell_idx, space_idx, 2] = (\n",
    "                    1 if self._to_idx(state.board.curCell) == cell_idx else 0\n",
    "                )\n",
    "\n",
    "        self.validmoves = list(map(lambda move: self._to_multi_idx(move), state.validmoves))\n",
    "        for idx in self.validmoves:\n",
    "            board_state[idx//CELLS, idx % CELLS, 3] = 1\n",
    "\n",
    "        return board_state\n",
    "\n",
    "    def _get_exploration_reward(self, action: int, msg: pb.ReturnMessage) -> float:\n",
    "        if msg.valid:\n",
    "            return VALID_REWARD\n",
    "        return INVALID_PENALTY\n",
    "\n",
    "    def _get_win_reward(self, msg: pb.ReturnMessage) -> float:\n",
    "        \"\"\"\n",
    "        Get's the reward for winning if the game was won\n",
    "        THIS ONLY APPLIES WHEN it is the player turn;\n",
    "        everything else is passed indirectly\n",
    "        \"\"\"\n",
    "        # the turn sent in the return message should still be the caller's turn\n",
    "        if msg.state.winner == msg.state.turn:\n",
    "            if self.player_turn:\n",
    "                self.won = True\n",
    "                return WIN_REWARD\n",
    "            else:\n",
    "                self.lost = True\n",
    "                return LOSS_PENALTY\n",
    "        elif self.done:\n",
    "            self.tied = True\n",
    "            return TIE_REWARD\n",
    "        return 0\n",
    "\n",
    "    def _get_cell_reward(self, msg: pb.ReturnMessage) -> float:\n",
    "        \"\"\"\n",
    "        Get's the reward for claiming a cell if a cell was claimed\n",
    "        \"\"\"\n",
    "        if self.prev_cellowners == msg.state.cellowners:\n",
    "            return 0\n",
    "        elif list(msg.state.cellowners).count(\n",
    "            msg.state.turn\n",
    "        ) > self.prev_cellowners.count(msg.state.turn):\n",
    "            self.prev_cellowners = list(msg.state.cellowners)\n",
    "            return CELL_REWARD\n",
    "        return 0\n",
    "\n",
    "    def _get_timeout_reward(self):\n",
    "        if self.cur_timestep > self.max_timesteps:\n",
    "            return TIMEOUT_PENALTY\n",
    "        return 0\n",
    "\n",
    "    def _get_reward(self, action: pb.Move, msg: pb.ReturnMessage) -> float:\n",
    "        return (\n",
    "            self._get_exploration_reward(action, msg)\n",
    "            + self._get_cell_reward(msg)\n",
    "            + self._get_win_reward(msg)\n",
    "            + self._get_timeout_reward()\n",
    "        )\n",
    "\n",
    "    def _step(self, action: int) -> Tuple[np.ndarray, float, bool, bool]:\n",
    "        \"\"\"\n",
    "        Updates self.done\n",
    "        \"\"\"\n",
    "        # send action and get response\n",
    "        self._send_action(self.to_move(action))\n",
    "        ret_message = self._get_return()\n",
    "\n",
    "        # return information\n",
    "        self.done = ret_message.state.done\n",
    "        reward = self._get_reward(action, ret_message)\n",
    "        if not self.done:\n",
    "            return self.observe(), reward, self.done, ret_message.valid\n",
    "        else:\n",
    "            return (\n",
    "                self._process_state(ret_message.state),\n",
    "                reward,\n",
    "                self.done,\n",
    "                ret_message.valid,\n",
    "            )\n",
    "\n",
    "    def _take_opponent_turn(self) -> Tuple[np.ndarray, float, bool, bool]:\n",
    "        valid = False\n",
    "        while not valid:\n",
    "            obs, reward, done, valid = self._step(\n",
    "                self.opponent.get_action(self.cur_state)\n",
    "            )\n",
    "        return obs, reward, done, valid\n",
    "\n",
    "    def _reset_vars(self):\n",
    "        self.prev_cellowners = [pb.NONE] * 9\n",
    "        self.cur_state = None  # the current state; used for debugging\n",
    "        self.won = False  # whether or not the player won\n",
    "        self.done = False  # if the game is over\n",
    "        self.lost = False  # whether or not the player lost\n",
    "        self.tied = False  # whether or not the player tied\n",
    "        self.player_turn = True  # whether or not it is the player's turn\n",
    "        self.cur_timestep = 0\n",
    "\n",
    "        self.s_conn = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.a_conn = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.r_conn = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "        self.s_conn.connect((\"\", 8000))\n",
    "        self.a_conn.connect((\"\", 8001))\n",
    "        self.r_conn.connect((\"\", 8002))\n",
    "\n",
    "    # public section\n",
    "    def observe(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Updates self.cur_state and self._turn\n",
    "        \"\"\"\n",
    "        state = self._get_state()\n",
    "        self._turn = state.turn\n",
    "        self.cur_state = self._process_state(state)\n",
    "        return self.cur_state\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool]:\n",
    "        \"\"\"\n",
    "        Updates current timestep\n",
    "\n",
    "        Returns:\n",
    "            - next state\n",
    "            - reward for the action\n",
    "            - done / not done\n",
    "            - valid / invalid\n",
    "        \"\"\"\n",
    "        self.player_turn = True\n",
    "        self.cur_timestep += 1\n",
    "        obs, reward, done, valid = self._step(action)\n",
    "        done = done or self.cur_timestep > self.max_timesteps\n",
    "\n",
    "        # take opponents turn\n",
    "        if valid and not done:\n",
    "            self.player_turn = False\n",
    "            obs, reward2, done, _ = self._take_opponent_turn()\n",
    "\n",
    "            # add the \"lost\" penalty\n",
    "            if self.done:\n",
    "                if self.lost:\n",
    "                    return obs, reward + reward2, done, valid\n",
    "                else:\n",
    "                    # tie\n",
    "                    assert self.tied\n",
    "                    return obs, reward + TIE_REWARD, done, valid\n",
    "            # penalize losing cells\n",
    "            elif reward2 == CELL_REWARD + VALID_REWARD:\n",
    "                return obs, reward - CELL_REWARD * 0.5, done, valid\n",
    "            # nothing special, just return the reward\n",
    "            return obs, reward, done, valid\n",
    "        return obs, reward, done, valid\n",
    "\n",
    "    def turn(self):\n",
    "        return self._turn\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        while 1:\n",
    "            try:\n",
    "                self.cleanup()\n",
    "                # self.pid = os.spawnl(os.P_NOWAIT, \"uttt\", \"uttt\", \"aivai\")\n",
    "                ret = os.system(\"./uttt aivai &\")\n",
    "                time.sleep(SLEEP_TIME)\n",
    "                self._reset_vars()\n",
    "                break\n",
    "            except ConnectionRefusedError:\n",
    "                pass\n",
    "        if self.player1:\n",
    "            return self.observe()\n",
    "        else:\n",
    "            self.observe()\n",
    "            obs, _, _, _ = self._take_opponent_turn()\n",
    "            return obs\n",
    "\n",
    "    def cleanup(self):\n",
    "        os.system(\"killall -q uttt\")\n",
    "        if self.s_conn is not None:\n",
    "            self.s_conn.close()\n",
    "            self.r_conn.close()\n",
    "            self.a_conn.close()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.cleanup()\n",
    "\n",
    "    def to_move(self, idx: int) -> pb.Move:\n",
    "        outer_idx = idx // CELLS\n",
    "        inner_idx = idx % CELLS\n",
    "\n",
    "        return pb.Move(\n",
    "            large=self._make_coord(outer_idx), small=self._make_coord(inner_idx)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UltimateTicTacToeEnv(\n",
    "    max_timesteps=MAX_TIMESTEPS,\n",
    "    opponent=ValidRandomOpponent(UltimateTicTacToeEnv.n_actions),\n",
    "    player1=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer related hyperparameters\n",
    "gamma = config[\"BUFFER\"].getfloat(\"GAMMA\")\n",
    "lam = config[\"BUFFER\"].getfloat(\"LAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "def special_discounted_cumulative_sums(x, discount, invalid_locats):\n",
    "    \"\"\"\n",
    "    invalid_locats: np.ndarray[bool], True if invalid, False if valid\n",
    "    \"\"\"\n",
    "    # discounts, but doesn't stack INVALID_PENALTY\n",
    "    # where it was invalid, use 0, else use the positive x\n",
    "    zeros = np.where(invalid_locats, 0, x)\n",
    "    # filter this\n",
    "    filtered = scipy.signal.lfilter([1], [1, float(-discount)], zeros[::-1], axis=0)[\n",
    "        ::-1\n",
    "    ]\n",
    "    # replace so that you have the invalid penalty where it was invalid\n",
    "    # and the gamma'd reward on valid moves\n",
    "    return np.where(invalid_locats, x, filtered)\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    # Buffer for storing trajectories\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        # Buffer initialization\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, *observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.valid_action_buffer = np.zeros(\n",
    "            (size, UltimateTicTacToeEnv.n_actions), dtype=np.bool8\n",
    "        )\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, action, reward, value, logprobability, valid_actions):\n",
    "        # Append one step of agent-environment interaction\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.logprobability_buffer[self.pointer] = logprobability\n",
    "        self.valid_action_buffer[self.pointer] = valid_actions\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0, use_special: bool = False):\n",
    "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        if use_special:\n",
    "            self.advantage_buffer[path_slice] = special_discounted_cumulative_sums(\n",
    "                deltas, self.gamma * self.lam, rewards[:-1] == INVALID_PENALTY\n",
    "            )\n",
    "            self.return_buffer[path_slice] = special_discounted_cumulative_sums(\n",
    "                rewards, self.gamma, rewards == INVALID_PENALTY\n",
    "            )[:-1]\n",
    "        else:\n",
    "            self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "                deltas, self.gamma * self.lam\n",
    "            )\n",
    "            self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
    "                rewards, self.gamma\n",
    "            )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        # Get all data of the buffer and normalize the advantages\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.logprobability_buffer,\n",
    "            self.valid_action_buffer,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss related hyperparameters\n",
    "clip_ratio = config[\"LOSS\"].getfloat(\"CLIP_RATIO\")\n",
    "target_kl = config[\"LOSS\"].getfloat(\"TARGET_KL\")\n",
    "clip_coef = config[\"LOSS\"].getfloat(\"CLIP_COEF\")\n",
    "v_coef = config[\"LOSS\"].getfloat(\"VALUE_COEFFICIENT\")\n",
    "entropy_coef = config[\"LOSS\"].getfloat(\"ENTROPY_COEFFICIENT\")\n",
    "invalid_coef = config[\"LOSS\"].getfloat(\"INVALID_COEFFICIENT\")\n",
    "reg_coef = config[\"LOSS\"].getfloat(\"REGULARIZER_COEFFICIENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def logprobabilities(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    logprobabilities_all = tf.nn.log_softmax(logits)\n",
    "    logprobability = tf.reduce_sum(\n",
    "        tf.one_hot(a, UltimateTicTacToeEnv.n_actions) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    return logprobability\n",
    "\n",
    "\n",
    "# Sample action from actor\n",
    "@tf.function\n",
    "def sample_action_value(observation, model: keras.Model):\n",
    "    logits, value = model(observation, training=False)\n",
    "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "    return logits, action, value\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def best_action_value(observation, model: keras.Model):\n",
    "    logits, value = model(observation, training=False)\n",
    "    action = tf.argmax(logits, axis=-1)\n",
    "    return logits, action, value\n",
    "\n",
    "\n",
    "@tf.function(reduce_retracing=True)\n",
    "def vector_slice(A: tf.Tensor, B: tf.Tensor):\n",
    "    \"\"\"Returns values of rows i of A at column B[i]\n",
    "\n",
    "    where A is a 2D Tensor with shape [None, D]\n",
    "    and B is a 1D Tensor with shape [None]\n",
    "    with type int32 elements in [0,D)\n",
    "\n",
    "    Example:\n",
    "      A =[[1,2], B = [0,1], vector_slice(A,B) -> [1,4]\n",
    "          [3,4]]\n",
    "\n",
    "    Credit:\n",
    "        https://stackoverflow.com/questions/38492608/tensorflow-indexing-into-2d-tensor-with-1d-tensor\n",
    "    \"\"\"\n",
    "    linear_index = tf.shape(A)[1] * tf.range(0, tf.shape(A)[0])\n",
    "    linear_A = tf.reshape(A, [-1])\n",
    "    return tf.gather(linear_A, B + linear_index)\n",
    "\n",
    "\n",
    "kl_loss = tf.keras.losses.KLDivergence()\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_mod_on_probs(\n",
    "    observation_buffer, desired_probs, return_buffer, model: keras.Model\n",
    "):\n",
    "    \"\"\"\n",
    "    Pretraining the model just training it to imitate\n",
    "    valid moves (mse)\n",
    "    Arguments:\n",
    "        * observation_buffer\n",
    "        * desired_probs\n",
    "        * return_buffer\n",
    "        * model\n",
    "    Returns:\n",
    "        total loss\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, values = model(observation_buffer, training=True)\n",
    "\n",
    "        actor_loss = kl_loss(desired_probs, tf.nn.softmax(logits))\n",
    "\n",
    "        regularizer_loss = tf.reduce_sum(model.losses)\n",
    "        # regularizer_loss = 0\n",
    "\n",
    "        # critic loss = MSE\n",
    "        critic_loss = tf.keras.losses.mse(tf.squeeze(return_buffer), tf.squeeze(values))\n",
    "\n",
    "        loss = actor_loss + critic_loss + regularizer_loss\n",
    "\n",
    "    policy_grads = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(policy_grads, model.trainable_variables))\n",
    "    return actor_loss, critic_loss, regularizer_loss, loss\n",
    "\n",
    "\n",
    "# Train the policy by maxizing the PPO-Clip objective\n",
    "@tf.function\n",
    "def train_mod(\n",
    "    observation_buffer,\n",
    "    action_buffer,\n",
    "    logprobability_buffer,\n",
    "    advantage_buffer,\n",
    "    return_buffer,\n",
    "    valid_moves_buffer,\n",
    "    model: keras.Model,\n",
    "):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, values = model(observation_buffer, training=True)\n",
    "        softmax = tf.nn.softmax(logits)\n",
    "\n",
    "        new_probs = logprobabilities(logits, action_buffer)\n",
    "        # ratio = E(new_probs / old_probs)\n",
    "        # this subtraction method is a way to do this\n",
    "        ratio = tf.exp(new_probs - logprobability_buffer)\n",
    "\n",
    "        # L_clip = E_t * ( min( r_t*A_t, clip(r_t, 1-e, 1+e)*A_t ) )\n",
    "        clip_loss = -tf.reduce_mean(\n",
    "            tf.minimum(\n",
    "                ratio * advantage_buffer,\n",
    "                tf.clip_by_value(ratio, 1 - clip_ratio, 1 + clip_ratio)\n",
    "                * advantage_buffer,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        regularizer_loss = tf.reduce_sum(model.losses)\n",
    "\n",
    "        # critic loss = MSE\n",
    "        critic_loss = tf.keras.losses.mse(return_buffer, tf.squeeze(values))\n",
    "\n",
    "        # entropy loss to encourage exploration\n",
    "        entropy_loss = -tf.reduce_mean(-new_probs)\n",
    "\n",
    "        # penalize large changes\n",
    "        # kl_loss = tf.reduce_mean(logprobability_buffer - new_probs)\n",
    "        # kl_loss = tf.reduce_sum(kl_loss)\n",
    "\n",
    "        # penalize invalids\n",
    "        invalid_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum(\n",
    "                tf.where(\n",
    "                    valid_moves_buffer,\n",
    "                    tf.constant(0, dtype=tf.float32),  # if it was valid, don't penalize\n",
    "                    softmax,  # else, penalize w/ the probability\n",
    "                ),\n",
    "                axis=-1,\n",
    "            )\n",
    "        )\n",
    "        # invalid_loss = tf.reduce_mean(tf.where(return_buffer == INVALID_PENALTY, vector_slice(softmax, action_buffer), tf.constant(0, dtype=tf.float32)))\n",
    "\n",
    "        # full loss\n",
    "        loss = (\n",
    "            clip_loss * clip_coef\n",
    "            + critic_loss * v_coef\n",
    "            + entropy_loss * entropy_coef\n",
    "            + regularizer_loss * reg_coef\n",
    "            + invalid_loss * invalid_coef\n",
    "        )\n",
    "\n",
    "    # apply gradients\n",
    "    policy_grads = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(policy_grads, model.trainable_variables))\n",
    "\n",
    "    # check new kl divergence\n",
    "    logits, _ = model(observation_buffer, training=False)\n",
    "    kl = tf.reduce_mean(logprobability_buffer - logprobabilities(logits, action_buffer))\n",
    "    kl = tf.reduce_sum(kl)\n",
    "\n",
    "    return kl, clip_loss, critic_loss, invalid_loss, entropy_loss, regularizer_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"attenppo8\"\n",
    "load_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 09:31:48.592428: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-07 09:31:48.592686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-07 09:31:48.592854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-07 09:31:48.637711: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-07 09:31:48.637918: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-07 09:31:48.638046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-07 09:31:48.638136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4279 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "def create_shared_layers(input_layer):\n",
    "    # imitation attention 1:\n",
    "    reshaped = layers.Reshape((9, 36))(input_layer)\n",
    "    x1 = layers.Dense(256, activation=\"selu\", use_bias=True)(reshaped)\n",
    "    x2 = layers.Dense(256, activation=\"selu\", use_bias=True)(reshaped)\n",
    "    x3 = layers.Dense(256, activation=\"selu\", use_bias=True)(reshaped)\n",
    "    x = layers.Add()([x1, x2, x3])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "\n",
    "    # feedforward 1:\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(2048, activation=\"selu\", activity_regularizer=\"l2\")(x)\n",
    "    x = layers.AlphaDropout(0.2)(x)\n",
    "    x = layers.Dense(2048, activation=\"selu\", activity_regularizer=\"l2\")(x)\n",
    "\n",
    "    # imitation attention 2\n",
    "    x1 = layers.Dense(2048, activation=\"selu\", use_bias=True, activity_regularizer=\"l2\")(x)\n",
    "    x2 = layers.Dense(2048, activation=\"selu\", use_bias=True, activity_regularizer=\"l2\")(x)\n",
    "    x3 = layers.Dense(2048, activation=\"selu\", use_bias=True, activity_regularizer=\"l2\")(x)\n",
    "    x = layers.Add()([x1, x2, x3])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "\n",
    "    # feedforward 2\n",
    "    x = layers.Dense(2048, activation=\"selu\", activity_regularizer=\"l2\")(x)\n",
    "    x = layers.AlphaDropout(0.3)(x)\n",
    "    x = layers.Dense(2048, activation=\"selu\", activity_regularizer=\"l2\")(x)\n",
    "    x = layers.AlphaDropout(0.2)(x)\n",
    "    x = layers.Dense(1024, activation=\"selu\")(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    # model inputs\n",
    "    inputs = tf.keras.Input(shape=UltimateTicTacToeEnv.obs_dim)\n",
    "    x = create_shared_layers(inputs)\n",
    "    logits = layers.Dense(UltimateTicTacToeEnv.n_actions)(\n",
    "        layers.Dense(512, activation=\"selu\")(x)\n",
    "    )\n",
    "    values = tf.keras.layers.Dense(1)(layers.Dense(512, activation=\"selu\")(x))\n",
    "    return tf.keras.Model(inputs=inputs, outputs=(logits, values))\n",
    "\n",
    "\n",
    "if load_model and os.path.exists(f\"models/{model_name}.keras\"):\n",
    "    print(\"loading model...\")\n",
    "    model = tf.keras.models.load_model(f\"models/{model_name}.keras\")\n",
    "else:\n",
    "    print(\"creating model...\")\n",
    "    model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 9, 9, 4)]            0         []                            \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 9, 36)                0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 9, 256)               9472      ['reshape[0][0]']             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 9, 256)               9472      ['reshape[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 9, 256)               9472      ['reshape[0][0]']             \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 9, 256)               0         ['dense[0][0]',               \n",
      "                                                                     'dense_1[0][0]',             \n",
      "                                                                     'dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 9, 256)               512       ['add[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 2304)                 0         ['layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 2048)                 4720640   ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " alpha_dropout (AlphaDropou  (None, 2048)                 0         ['dense_3[0][0]']             \n",
      " t)                                                                                               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 2048)                 4196352   ['alpha_dropout[0][0]']       \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 2048)                 4196352   ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 2048)                 4196352   ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 2048)                 4196352   ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 2048)                 0         ['dense_5[0][0]',             \n",
      "                                                                     'dense_6[0][0]',             \n",
      "                                                                     'dense_7[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 2048)                 4096      ['add_1[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 2048)                 4196352   ['layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " alpha_dropout_1 (AlphaDrop  (None, 2048)                 0         ['dense_8[0][0]']             \n",
      " out)                                                                                             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 2048)                 4196352   ['alpha_dropout_1[0][0]']     \n",
      "                                                                                                  \n",
      " alpha_dropout_2 (AlphaDrop  (None, 2048)                 0         ['dense_9[0][0]']             \n",
      " out)                                                                                             \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 1024)                 2098176   ['alpha_dropout_2[0][0]']     \n",
      "                                                                                                  \n",
      " dense_12 (Dense)            (None, 512)                  524800    ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      " dense_14 (Dense)            (None, 512)                  524800    ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 81)                   41553     ['dense_12[0][0]']            \n",
      "                                                                                                  \n",
      " dense_13 (Dense)            (None, 1)                    513       ['dense_14[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 33121618 (126.35 MB)\n",
      "Trainable params: 33121618 (126.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate hyperparams\n",
    "learning_rate = config[\"OPTIMIZER\"].getfloat(\"LEARNING_RATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model and os.path.exists(f\"models/{model_name}.keras\"):\n",
    "    optim: tf.keras.optimizers.Adam = model.optimizer\n",
    "    optim.learning_rate = learning_rate\n",
    "else:\n",
    "    optim = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-05>\n"
     ]
    }
   ],
   "source": [
    "print(optim.learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training time hyperparameters\n",
    "train_iterations = config[\"TRAIN\"].getint(\"TRAIN_ITERATIONS\")\n",
    "epochs = config[\"TRAIN\"].getint(\"EPOCHS\")\n",
    "minibatch_size = config[\"TRAIN\"].getint(\"MINIBATCH_SIZE\")\n",
    "steps_per_epoch = config[\"TRAIN\"].getint(\"STEPS_PER_EPOCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = config[\"TRAIN\"].getint(\"EPOCH\")\n",
    "summary_writer = tf.summary.create_file_writer(f\"./logs/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = Buffer(UltimateTicTacToeEnv.obs_dim, steps_per_epoch, gamma=gamma, lam=lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain = False\n",
    "load_timesteps = True\n",
    "pretrain_timesteps = 600000\n",
    "pretrain_batch_size = 10000\n",
    "\n",
    "if pretrain:\n",
    "    if load_timesteps:\n",
    "        pretrain_observations = np.load(\"pretrain_observations.npy\")\n",
    "        pretrain_desired_probs = np.load(\"pretrain_desired_probs.npy\")\n",
    "        pretrain_rewards = np.load(\"pretrain_rewards.npy\")\n",
    "    else:\n",
    "        pretrain_observations = np.zeros((pretrain_timesteps, *env.obs_dim))\n",
    "        pretrain_desired_probs = np.zeros((pretrain_timesteps, env.n_actions))\n",
    "        pretrain_rewards = np.zeros((pretrain_timesteps, 1))\n",
    "else:\n",
    "    pretrain_observations = np.zeros((steps_per_epoch, *env.obs_dim))\n",
    "    pretrain_desired_probs = np.zeros((steps_per_epoch, env.n_actions))\n",
    "    pretrain_rewards = np.zeros((steps_per_epoch, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.utils as utils\n",
    "\n",
    "\n",
    "def collect_pretrain_trajectories():\n",
    "    def preprocess_obs_action_use_map():\n",
    "        global pretrain_observations, pretrain_desired_probs\n",
    "        # initialize obs:probs map and calculate probs\n",
    "        observations_probs_map = {}\n",
    "        for hashable_obs, action_use_map in observations_actions_uses.items():\n",
    "            # initialize the probability matrix\n",
    "            observations_probs_map[hashable_obs] = np.zeros((env.n_actions))\n",
    "\n",
    "            # calculate the probability of each action\n",
    "            total_uses = sum(action_use_map.values())\n",
    "            for action, uses in action_use_map.items():\n",
    "                observations_probs_map[hashable_obs][action] = uses / total_uses\n",
    "\n",
    "            assert round(sum(observations_probs_map[hashable_obs]), 4) == 1.0\n",
    "\n",
    "        # put probs into pretrain_desired_probs\n",
    "        for i in range(pretrain_timesteps):\n",
    "            pretrain_desired_probs[i] = observations_probs_map[\n",
    "                pretrain_observations[i].tobytes()\n",
    "            ]\n",
    "\n",
    "    t = 0\n",
    "    num_valid = 0\n",
    "    start_time = datetime.now()\n",
    "    opponent = WinningRandomOpponent(env.n_actions)\n",
    "\n",
    "    # map observations -> map of actions -> uses\n",
    "    observations_actions_uses = {}\n",
    "    while t < pretrain_timesteps:\n",
    "        # Iterate over the steps of each epoch\n",
    "        observation = env.reset()\n",
    "        start_timestep = t\n",
    "        while not env.done and t < pretrain_timesteps:\n",
    "            # update the map w/ the observation\n",
    "            hashable = observation.tobytes()\n",
    "            if hashable not in observations_actions_uses:\n",
    "                observations_actions_uses[hashable] = {}\n",
    "\n",
    "            # get the action\n",
    "            action = opponent.get_action(observation)\n",
    "\n",
    "            # step\n",
    "            observation_new, reward, done, valid = env.step(action)\n",
    "            if valid:\n",
    "                num_valid += 1\n",
    "            else:\n",
    "                print(\"action was\", action)\n",
    "                print(observation)\n",
    "                raise Exception(\"failed\")\n",
    "\n",
    "            # update the pretrain buffers and the map\n",
    "            pretrain_observations[t] = observation\n",
    "            pretrain_rewards[t] = reward\n",
    "            # update the map w/ the action\n",
    "            if action not in observations_actions_uses[hashable]:\n",
    "                observations_actions_uses[hashable][action] = 1\n",
    "            else:\n",
    "                observations_actions_uses[hashable][action] += 1\n",
    "\n",
    "            # Update the observation\n",
    "            observation = observation_new\n",
    "\n",
    "            # Finish trajectory if reached to a terminal state\n",
    "            t += 1\n",
    "            if done:\n",
    "                observation = env.reset()\n",
    "\n",
    "                # do the discounting\n",
    "                pretrain_rewards[start_timestep:t] = discounted_cumulative_sums(\n",
    "                    pretrain_rewards[start_timestep:t], gamma\n",
    "                )\n",
    "                start_timestep = t\n",
    "\n",
    "            # log\n",
    "            print(\n",
    "                f\"Step {t} / {pretrain_timesteps};\\t\"\n",
    "                + f\"% valid = {num_valid / (t):.4f};\\t\"\n",
    "                + f\"fps: {(t)/(datetime.now()-start_time).total_seconds():.2f};\\t\",\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "    print()\n",
    "\n",
    "    preprocess_obs_action_use_map()\n",
    "\n",
    "    np.save(\"pretrain_observations\", pretrain_observations)\n",
    "    np.save(\"pretrain_desired_probs\", pretrain_desired_probs)\n",
    "    np.save(\"pretrain_rewards\", pretrain_rewards)\n",
    "\n",
    "\n",
    "def pretrain_model(\n",
    "    model: keras.Model, num_epochs=15, collect_trajectories: bool = True\n",
    "):\n",
    "    global pretrain_rewards, pretrain_desired_probs, pretrain_observations\n",
    "    if collect_trajectories:\n",
    "        collect_pretrain_trajectories()\n",
    "\n",
    "    for epoch in range(1, 1 + num_epochs):\n",
    "        pretrain_observations, pretrain_desired_probs, pretrain_rewards = utils.shuffle(\n",
    "            pretrain_observations, pretrain_desired_probs, pretrain_rewards\n",
    "        )\n",
    "        actor_loss, critic_loss, regularizer_loss, loss = 0, 0, 0, 0\n",
    "        # do minibatches\n",
    "        for batch in range(pretrain_timesteps // pretrain_batch_size):\n",
    "            start_idx = batch * pretrain_batch_size\n",
    "            end_idx = (batch + 1) * pretrain_batch_size\n",
    "            _actor_loss, _critic_loss, _regularizer_loss, _loss = train_mod_on_probs(\n",
    "                pretrain_observations[start_idx:end_idx],\n",
    "                pretrain_desired_probs[start_idx:end_idx],\n",
    "                pretrain_rewards[start_idx:end_idx],\n",
    "                model,\n",
    "            )\n",
    "            actor_loss += _actor_loss\n",
    "            critic_loss += _critic_loss\n",
    "            regularizer_loss += _regularizer_loss\n",
    "            loss += _loss\n",
    "\n",
    "        actor_loss /= batch + 1\n",
    "        critic_loss /= batch + 1\n",
    "        regularizer_loss /= batch + 1\n",
    "        loss /= batch + 1\n",
    "        # write summaries\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar(\"pretrain/actor loss\", actor_loss, step=epoch)\n",
    "            tf.summary.scalar(\"pretrain/critic loss\", critic_loss, step=epoch)\n",
    "            tf.summary.scalar(\"pretrain/regularizer loss\", regularizer_loss, step=epoch)\n",
    "            tf.summary.scalar(\"pretrain/loss\", loss, step=epoch)\n",
    "\n",
    "        print(\n",
    "            f\"epoch {epoch}: actor loss: {actor_loss}; critic loss: {critic_loss}; regularizer_loss: {regularizer_loss}, total loss {loss}\"\n",
    "        )\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 1., 1.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on real observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training function\n",
    "def train_model(\n",
    "    model: keras.Model,\n",
    "    epoch_start: int,\n",
    "    use_kl: bool = True,\n",
    "    use_adaptive_kl: bool = True,\n",
    "    shuffle: bool = True,\n",
    "    use_special: bool = True,\n",
    "    just_train_critic: bool = False,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Train the model for `epochs` epochs\n",
    "    Arguments:\n",
    "        * model: keras.Model - the model to train\n",
    "        * epoch_start: int - the epoch to start logging at\n",
    "        * use_kl: bool - whether or not to stop early if kl divergence > target_kl\n",
    "        * use_adaptive_kl: bool - whether or not to adapt kl (increase it if divergence > target_kl, decrease it otherwise)\n",
    "        * shuffle: bool - whether or not to shuffle the observations\n",
    "        * use_special: bool -whether or not to use special version of discounted sums\n",
    "        * just_train_critic: bool - whether or not to just train the critic; defaults to False\n",
    "    \"\"\"\n",
    "    global target_kl, pretrain_desired_probs, WIN_REWARD\n",
    "    # Iterate over the number of epochs\n",
    "    for epoch in range(epoch_start, epoch_start + epochs):\n",
    "        # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "        sum_return = 0\n",
    "        num_episodes = 0\n",
    "        episode_return = 0\n",
    "        episode_length = 0\n",
    "        lengths = []\n",
    "\n",
    "        # logging variables\n",
    "        num_valid = 0\n",
    "        num_wins = 0\n",
    "        num_losses = 0\n",
    "        num_ties = 0\n",
    "\n",
    "        # Iterate over the steps of each epoch\n",
    "        observation = env.reset()\n",
    "        start_time = datetime.now()\n",
    "        for t in range(steps_per_epoch):\n",
    "            valid_actions = env.validmoves\n",
    "            valid_actions = np.array(\n",
    "                [True if i in valid_actions else False for i in range(env.n_actions)]\n",
    "            )\n",
    "\n",
    "            # Get the logits, action, and take one step in the environment\n",
    "            observation = observation.reshape(1, *env.obs_dim)\n",
    "\n",
    "            logits, action, value_t = sample_action_value(observation, model)\n",
    "            # logits, action, value_t = best_action_value(observation, model)\n",
    "\n",
    "            observation_new, reward, done, valid = env.step(action[0].numpy())\n",
    "            episode_return += reward\n",
    "            episode_length += 1\n",
    "\n",
    "            # logging variables\n",
    "            num_valid += 1 if valid else 0\n",
    "            num_wins += 1 if env.won else 0\n",
    "            num_losses += 1 if env.lost else 0\n",
    "            num_ties += 1 if env.tied else 0\n",
    "\n",
    "            # Get the value and log-probability of the action\n",
    "            logprobability_t = logprobabilities(logits, action)\n",
    "\n",
    "            if just_train_critic:\n",
    "                pretrain_desired_probs[t] = tf.nn.softmax(logits)\n",
    "\n",
    "            # Store obs, act, rew, v_t, logp_pi_t\n",
    "            buffer.store(\n",
    "                observation, action, reward, value_t, logprobability_t, valid_actions\n",
    "            )\n",
    "\n",
    "            # Update the observation\n",
    "            observation = observation_new\n",
    "\n",
    "            # Finish trajectory if reached to a terminal state\n",
    "            terminal = done\n",
    "            if terminal or t == steps_per_epoch - 1:\n",
    "                last_value = (\n",
    "                    0\n",
    "                    if done\n",
    "                    else model(observation.reshape(1, *env.obs_dim), training=False)[1]\n",
    "                )\n",
    "                buffer.finish_trajectory(last_value, use_special=use_special)\n",
    "                sum_return += episode_return\n",
    "                lengths.append(episode_length)\n",
    "                num_episodes += 1\n",
    "                observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "            # log\n",
    "            print(\n",
    "                f\"Step {t+1} / {steps_per_epoch};\\t\"\n",
    "                + f\"% valid = {num_valid / (t+1):.4f};\\t\"\n",
    "                + f\"fps: {(t+1)/(datetime.now()-start_time).total_seconds():.2f};\\t\"\n",
    "                + f\"win rate: {(num_wins/num_episodes if num_episodes > 0 else 0):.2f}\\t\"\n",
    "                + f\"tie rate: {(num_ties/num_episodes if num_episodes > 0 else 0):.2f}\\t\"\n",
    "                + f\"loss rate: {(num_losses/num_episodes if num_episodes > 0 else 0):.2f}\",\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "        print()\n",
    "\n",
    "        (\n",
    "            observation_buffer,\n",
    "            action_buffer,\n",
    "            advantage_buffer,\n",
    "            return_buffer,\n",
    "            logprobability_buffer,\n",
    "            valid_action_buffer,\n",
    "        ) = buffer.get()\n",
    "\n",
    "        if just_train_critic:\n",
    "            for it in range(train_iterations):\n",
    "                train_mod_on_probs(\n",
    "                    observation_buffer,\n",
    "                    pretrain_desired_probs[:steps_per_epoch],\n",
    "                    return_buffer,\n",
    "                    model,\n",
    "                )\n",
    "            continue\n",
    "\n",
    "        # Update the policy and implement early stopping using KL divergence\n",
    "        kl = 0\n",
    "        clip_loss = 0\n",
    "        critic_loss = 0\n",
    "        invalid_loss = 0\n",
    "        entropy_loss = 0\n",
    "        regularizer_loss = 0\n",
    "        for it in range(train_iterations):\n",
    "            if shuffle:\n",
    "                (\n",
    "                    _observation_buffer,\n",
    "                    _action_buffer,\n",
    "                    _advantage_buffer,\n",
    "                    _return_buffer,\n",
    "                    _logprobability_buffer,\n",
    "                    _valid_action_buffer,\n",
    "                ) = utils.shuffle(\n",
    "                    observation_buffer,\n",
    "                    action_buffer,\n",
    "                    advantage_buffer,\n",
    "                    return_buffer,\n",
    "                    logprobability_buffer,\n",
    "                    valid_action_buffer,\n",
    "                )\n",
    "            else:\n",
    "                (\n",
    "                    _observation_buffer,\n",
    "                    _action_buffer,\n",
    "                    _advantage_buffer,\n",
    "                    _return_buffer,\n",
    "                    _logprobability_buffer,\n",
    "                    _valid_action_buffer,\n",
    "                ) = (\n",
    "                    observation_buffer,\n",
    "                    action_buffer,\n",
    "                    advantage_buffer,\n",
    "                    return_buffer,\n",
    "                    logprobability_buffer,\n",
    "                    valid_action_buffer,\n",
    "                )\n",
    "            temp_clip_loss = 0\n",
    "            temp_critic_loss = 0\n",
    "            temp_invalid_loss = 0\n",
    "            temp_entropy_loss = 0\n",
    "            temp_regularizer_loss = 0\n",
    "            stopped_early = False\n",
    "            # do minibatches\n",
    "            for i in range(steps_per_epoch // minibatch_size):\n",
    "                # get the parts of the kl and losses\n",
    "                (\n",
    "                    kl,\n",
    "                    clip_loss_part,\n",
    "                    critic_loss_part,\n",
    "                    invalid_loss_part,\n",
    "                    entropy_loss_part,\n",
    "                    regularizer_loss_part,\n",
    "                ) = train_mod(\n",
    "                    tf.constant(\n",
    "                        _observation_buffer[\n",
    "                            i * minibatch_size : (i + 1) * minibatch_size\n",
    "                        ]\n",
    "                    ),  # obs\n",
    "                    tf.constant(\n",
    "                        _action_buffer[i * minibatch_size : (i + 1) * minibatch_size]\n",
    "                    ),  # act\n",
    "                    tf.constant(\n",
    "                        _logprobability_buffer[\n",
    "                            i * minibatch_size : (i + 1) * minibatch_size\n",
    "                        ]\n",
    "                    ),  # logprobs\n",
    "                    tf.constant(\n",
    "                        _advantage_buffer[i * minibatch_size : (i + 1) * minibatch_size]\n",
    "                    ),  # advantages\n",
    "                    tf.constant(\n",
    "                        _return_buffer[i * minibatch_size : (i + 1) * minibatch_size]\n",
    "                    ),  # returns\n",
    "                    tf.constant(\n",
    "                        _valid_action_buffer[\n",
    "                            i * minibatch_size : (i + 1) * minibatch_size\n",
    "                        ]\n",
    "                    ),  # valid actions\n",
    "                    model,\n",
    "                )\n",
    "                # update the temps\n",
    "                temp_clip_loss += clip_loss_part\n",
    "                temp_critic_loss += critic_loss_part\n",
    "                temp_invalid_loss += invalid_loss_part\n",
    "                temp_entropy_loss += entropy_loss_part\n",
    "                temp_regularizer_loss += regularizer_loss_part\n",
    "                if use_kl and kl > 1.5 * target_kl:\n",
    "                    stopped_early = True\n",
    "                    if use_adaptive_kl:\n",
    "                        target_kl *= 1.5\n",
    "                    break\n",
    "\n",
    "            # average the temps to get the amount per that pass\n",
    "            temp_clip_loss /= i + 1\n",
    "            temp_critic_loss /= i + 1\n",
    "            temp_invalid_loss /= i + 1\n",
    "            temp_entropy_loss /= i + 1\n",
    "            temp_regularizer_loss /= i + 1\n",
    "\n",
    "            # update the main counts\n",
    "            clip_loss += temp_clip_loss\n",
    "            critic_loss += temp_critic_loss\n",
    "            invalid_loss += temp_invalid_loss\n",
    "            entropy_loss += temp_entropy_loss\n",
    "            regularizer_loss += temp_regularizer_loss\n",
    "\n",
    "            if stopped_early:\n",
    "                # Early Stopping\n",
    "                break\n",
    "        else:\n",
    "            if use_kl and use_adaptive_kl:\n",
    "                target_kl /= 1.2\n",
    "\n",
    "        clip_loss /= it + 1\n",
    "        critic_loss /= it + 1\n",
    "        invalid_loss /= it + 1\n",
    "        entropy_loss /= it + 1\n",
    "        regularizer_loss /= it + 1\n",
    "\n",
    "        # Print mean return and length for each epoch\n",
    "        print(\n",
    "            f\"Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum(lengths) / num_episodes}. STD Length: {np.std(lengths)}\"\n",
    "        )\n",
    "        print(\"=\" * 64)\n",
    "\n",
    "        # log scalars\n",
    "        with summary_writer.as_default():\n",
    "            # episode info\n",
    "            tf.summary.scalar(\"episode/win rate\", num_wins / num_episodes, step=epoch)\n",
    "            tf.summary.scalar(\"episode/tie rate\", num_ties / num_episodes, step=epoch)\n",
    "            tf.summary.scalar(\n",
    "                \"episode/loss rate\", num_losses / num_episodes, step=epoch\n",
    "            )\n",
    "            tf.summary.scalar(\"episode/valid percentage\", num_valid / t, step=epoch)\n",
    "            tf.summary.scalar(\n",
    "                \"episode/mean reward\", sum_return / num_episodes, step=epoch\n",
    "            )\n",
    "            tf.summary.scalar(\n",
    "                \"episode/mean length\", sum(lengths) / num_episodes, step=epoch\n",
    "            )\n",
    "            tf.summary.scalar(\"episode/std length\", np.std(lengths), step=epoch)\n",
    "\n",
    "            # training info\n",
    "            tf.summary.scalar(\"train/clip_loss\", clip_loss, step=epoch)\n",
    "            tf.summary.scalar(\"train/critic_loss\", critic_loss, step=epoch)\n",
    "            tf.summary.scalar(\"train/invalid_loss\", invalid_loss, step=epoch)\n",
    "            tf.summary.scalar(\"train/entropy_loss\", entropy_loss, step=epoch)\n",
    "            tf.summary.scalar(\"train/regularizer_loss\", regularizer_loss, step=epoch)\n",
    "            tf.summary.scalar(\"train/kl\", kl, step=epoch)\n",
    "            tf.summary.scalar(\n",
    "                \"train/total_loss\",\n",
    "                clip_loss\n",
    "                + critic_loss * v_coef\n",
    "                + invalid_loss * invalid_coef\n",
    "                + entropy_loss * entropy_coef\n",
    "                + regularizer_loss * reg_coef,\n",
    "                step=epoch,\n",
    "            )\n",
    "            tf.summary.scalar(\"train/num_iterations\", it + 1, step=epoch)\n",
    "\n",
    "        # save every 5\n",
    "        if epoch % 5 == 0:\n",
    "            model.save(\"model_temp.keras\")\n",
    "\n",
    "        WIN_REWARD += 0.1 / 240\n",
    "\n",
    "    return epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 09:31:49.144410: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-08-07 09:31:49.180762: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16000 / 16000;\t% valid = 0.0871;\tfps: 488.37;\twin rate: 0.00\ttie rate: 0.00\tloss rate: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 09:32:23.348161: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8c65c24400 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-07 09:32:23.348181: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2023-08-07 09:32:23.351115: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-08-07 09:32:23.435026: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2331. Mean Return: -33.042853932584094. Mean Length: 35.95505617977528. STD Length: 0.9470250564362013\n",
      "================================================================\n",
      "Step 16000 / 16000;\t% valid = 0.1345;\tfps: 474.63;\twin rate: 0.00\ttie rate: 0.00\tloss rate: 0.00\n",
      "Epoch: 2332. Mean Return: -31.32152808988758. Mean Length: 35.95505617977528. STD Length: 0.9470250564362013\n",
      "================================================================\n",
      "Step 16000 / 16000;\t% valid = 0.1932;\tfps: 494.03;\twin rate: 0.00\ttie rate: 0.00\tloss rate: 0.00\n",
      "Epoch: 2333. Mean Return: -29.189471910112356. Mean Length: 35.95505617977528. STD Length: 0.9470250564362013\n",
      "================================================================\n",
      "Step 16000 / 16000;\t% valid = 0.2399;\tfps: 490.00;\twin rate: 0.00\ttie rate: 0.00\tloss rate: 0.00\n",
      "Epoch: 2334. Mean Return: -27.495640449438188. Mean Length: 35.95505617977528. STD Length: 0.9470250564362013\n",
      "================================================================\n",
      "Step 9867 / 16000;\t% valid = 0.2490;\tfps: 486.90;\twin rate: 0.00\ttie rate: 0.00\tloss rate: 0.00\r"
     ]
    }
   ],
   "source": [
    "if not pretrain:\n",
    "    epoch = train_model(\n",
    "        model,\n",
    "        epoch,\n",
    "        use_kl=True,\n",
    "        use_adaptive_kl=False,\n",
    "        shuffle=True,\n",
    "        use_special=True,\n",
    "        just_train_critic=False,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove previous\n",
    "os.system(f\"rm models/{model_name}.keras\")\n",
    "time.sleep(1)\n",
    "\n",
    "# save\n",
    "model.save(f\"models/{model_name}.keras\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: keras.Model, num_episodes: int = 10):\n",
    "    mean_reward = 0\n",
    "    mean_len = 0\n",
    "    num_won = 0\n",
    "    num_lost = 0\n",
    "    for _ in range(num_episodes):\n",
    "        # initialize vars\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        i = 0\n",
    "\n",
    "        # complete a round\n",
    "        while not done:\n",
    "            # step process\n",
    "            _, action, _ = best_action_value(obs.reshape(1, *obs.shape), model)\n",
    "            obs, reward, done, _ = env.step(action[0].numpy())\n",
    "\n",
    "            # update counts\n",
    "            mean_reward += reward\n",
    "            i += 1\n",
    "        # update mean length\n",
    "        mean_len += i\n",
    "\n",
    "        if env.won:\n",
    "            num_won += 1\n",
    "        if env.lost:\n",
    "            num_lost += 1\n",
    "\n",
    "    # average them\n",
    "    mean_reward /= num_episodes\n",
    "    mean_len /= num_episodes\n",
    "    print(\"Episode mean reward:\", mean_reward)\n",
    "    print(\"Episode mean length:\", mean_len)\n",
    "    print(f\"win rate: {num_won / num_episodes}, loss rate: {num_lost / num_episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode mean reward: -1.7062000000000626\n",
      "Episode mean length: 21.0\n",
      "win rate: 0.0, loss rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, num_episodes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
